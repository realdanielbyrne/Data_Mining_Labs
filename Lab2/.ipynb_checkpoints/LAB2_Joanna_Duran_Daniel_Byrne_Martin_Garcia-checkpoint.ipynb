{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Two: Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build upon the predictive analysis (classification) that we completed in the\n",
    "previous mini-project, adding additional modeling from new classification algorithms as well as\n",
    "more explanations that are inline with the CRISP-DM framework.\n",
    "\n",
    "We chose to continue to use the CIFAR-10 dataset. We identified the two tasks from the dataset to classify. \n",
    "Task 1: Cats and dogs\n",
    "Task 2: Birds and airplanes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By Martin Garcia, Joanna Duran, Daniel Byrne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "import sklearn.naive_bayes as b\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.ndimage import convolve\n",
    "from sklearn.utils import compute_class_weight\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Minilab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Understanding\n",
    "Our objective was to create a logistic regression model and a support vector machine model for the classification of each image. We were to determine which model is best suited for this standard classification task based on a comparison on their prediction accuracy, training times, and computational efficiency.\n",
    "\n",
    "The CIFAR-10 dataset (Canadian Institute For Advanced Research) is a collection of images that are commonly used to train machine learning and computer vision algorithms. It is one of the most widely used datasets for machine learning research. It is a subset of the 80 million tiny images dataset and consists of 60,000 32x32 color images containing one of 10 object classes, with 6,000 images per class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Meaning Type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The CIFAR-10 dataset\n",
    "We are using the [CIFAR-10](http://www.cs.toronto.edu/~kriz/cifar.html) dataset which consists of 60,000 32x32 colour images in 10 classes, with 6,000 images per class. The dataset includes are 50,000(80%) training images and 10,000(20%) test images broken in to 5 pre-randomized training batches and 1 test batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each training batch contains 10,000 observations with a row vector of length 3,072 representative of color image of 32x32 pixels. The first 1,024 columns consist of red values, followed by green, and blue. The data also incorporates labels ranging from 0 to 9 and are listed below.\n",
    "\n",
    "* airplane : 0\n",
    "* automobile : 1\n",
    "* bird : 2\n",
    "* cat : 3\n",
    "* deer : 4\n",
    "* dog : 5\n",
    "* frog : 6\n",
    "* horse : 7\n",
    "* ship : 8\n",
    "* truck : 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test batch contains 1,000 randomly-selected images from each class. The 5 training batches are randomized and contain a variable number of images from each class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classes are completely mutually exclusive. There is no overlap between automobiles and trucks. \"Automobile\" includes sedans, SUVs, things of that sort. \"Truck\" includes only big trucks. Neither includes pickup trucks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data, reshape to 32x32 matrix per color, transpose matrices\n",
    "def load_cfar10_batch(path, batch_id = None, reshape = True):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    path -- path the datasets\n",
    "    batch_id -- id of the batch (1 to 5) to load\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing:\n",
    "                    X -- features\n",
    "                    Y -- labels\n",
    "    \"\"\"\n",
    "    if batch_id is not None:\n",
    "        filepath = path + 'data_batch_' + str(batch_id)\n",
    "    else:\n",
    "        filepath = path\n",
    "        \n",
    "    with open(filepath, mode='rb') as file:\n",
    "        # note the encoding type is 'latin1'\n",
    "        batch = pickle.load(file, encoding='latin1')\n",
    "    \n",
    "    if reshape:    \n",
    "        X = batch['data'].reshape((len(batch['data']), 3, 32, 32)).transpose(0, 2, 3, 1)\n",
    "        Y = np.array(batch['labels'])\n",
    "    else:\n",
    "        X = batch['data']\n",
    "        Y = np.array(batch['labels'])\n",
    "        \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cfar10_dataset():\n",
    "    \"\"\"\n",
    "    Loads the cfar10 dataset\n",
    "    \n",
    "    Arguments: \n",
    "        None\n",
    "    \n",
    "    Returns:\n",
    "        X - Training dataset\n",
    "        Y - Training labels\n",
    "         - Test datset\n",
    "        y_test - test labelss\n",
    "    \"\"\"\n",
    "    x_test,y_test = load_cfar10_batch(\"data/test_batch\",None,False)\n",
    "    X,Y = load_cfar10_batch(\"data/\",1,False)\n",
    "\n",
    "    for n in range(2,6):\n",
    "        x,y = load_cfar10_batch(\"data/\",n,False)    \n",
    "        X = np.concatenate((X,x),axis=0)\n",
    "        Y = np.concatenate((Y,y),axis=0)\n",
    "\n",
    "\n",
    "    return (X,Y,x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load labels for our label\n",
    "def load_label_names():\n",
    "    return ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch #4:\n",
      "# of Samples: 10000\n",
      "\n",
      "Label Counts of [0](AIRPLANE) : 1003\n",
      "Label Counts of [1](AUTOMOBILE) : 963\n",
      "Label Counts of [2](BIRD) : 1041\n",
      "Label Counts of [3](CAT) : 976\n",
      "Label Counts of [4](DEER) : 1004\n",
      "Label Counts of [5](DOG) : 1021\n",
      "Label Counts of [6](FROG) : 1004\n",
      "Label Counts of [7](HORSE) : 981\n",
      "Label Counts of [8](SHIP) : 1024\n",
      "Label Counts of [9](TRUCK) : 983\n",
      "\n",
      "Example of Image 8762:\n",
      "Image - Min Value: 4 Max Value: 246\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 4 Name: deer\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#display images\n",
    "def display_stats(data, batch_id, sample_id):\n",
    "    features, labels = load_cfar10_batch(data, batch_id)\n",
    "\n",
    "    if not (0 <= sample_id < len(features)):\n",
    "        print('{} samples in batch {}.  {} is out of range.'.format(len(features), batch_id, sample_id))\n",
    "        return None\n",
    "\n",
    "    print('\\nStats of batch #{}:'.format(batch_id))\n",
    "    print('# of Samples: {}\\n'.format(len(features)))\n",
    "\n",
    "    label_names = load_label_names()\n",
    "    label_counts = dict(zip(*np.unique(labels, return_counts=True)))\n",
    "    for key, value in label_counts.items():\n",
    "        print('Label Counts of [{}]({}) : {}'.format(key, label_names[key].upper(), value))\n",
    "\n",
    "    sample_image = features[sample_id]\n",
    "    sample_label = labels[sample_id]\n",
    "\n",
    "    print('\\nExample of Image {}:'.format(sample_id))\n",
    "    print('Image - Min Value: {} Max Value: {}'.format(sample_image.min(), sample_image.max()))\n",
    "    print('Image - Shape: {}'.format(sample_image.shape))\n",
    "    print('Label - Label Id: {} Name: {}'.format(sample_label, label_names[sample_label]))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(sample_image)\n",
    "\n",
    "batch_id = random.randint(1,5)\n",
    "sample_id = random.randint(1,10000)\n",
    "display_stats( \"data/\", batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we define and prepare our class variables. \n",
    "\n",
    "Each of the batch files contains a dictionary with the following elements: \n",
    "\n",
    "- data -- a 10000x3072 numpy array of uint8s. Each row of the array stores a 32x32 8bit color image. The first 1024 entries contain the red channel values, the next 1024 the green, and the final 1024 the blue. The image is stored in row-major order, so that the first 32 entries of the array are the red channel values of the first row of the image. \n",
    "\n",
    "- labels -- a list of 10000 numbers in the range 0-9. The number at index i indicates the label of the ith image in the array data.\n",
    "\n",
    "The dataset contains another file, called batches.meta. It too contains a Python dictionary object. It has the following entries: \n",
    "\n",
    "- label_names -- a 10-element list which gives meaningful names to the numeric labels in the labels array described above. For example, label_names[0] == \"airplane\", label_names[1] == \"automobile\", etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the dataset\n",
    "X,Y,x_test,y_test = load_cfar10_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artificial Data Augmentation\n",
    "\n",
    "In order to learn good latent representations from a small dataset, \n",
    "we artificially generate more labeled data by perturbing the training\n",
    "data with linear shifts of 1 pixel in each direction.\n",
    "\n",
    "Other augmentations such as color shifting, rotation and cropping are useful and often used as well in preparing image data for deeplearning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nudge_dataset(X, Y):\n",
    "    \"\"\"\n",
    "    This method produces a dataset 5 times bigger than the original one,\n",
    "    by moving the 32x32 images in X around by 1px to left, right, down, up.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- training data\n",
    "    Y -- label data\n",
    "    \n",
    "    Returns:\n",
    "    Augmented dataset\n",
    "    \"\"\"\n",
    "    direction_vectors = np.array([\n",
    "        [[[0, 1, 0],\n",
    "          [0, 0, 0],\n",
    "          [0, 0, 0]]],\n",
    "\n",
    "        [[[0, 0, 0],\n",
    "          [1, 0, 0],\n",
    "          [0, 0, 0]]],\n",
    "\n",
    "        [[[0, 0, 0],\n",
    "         [0, 0, 1],\n",
    "         [0, 0, 0]]],\n",
    "\n",
    "        [[[0, 0, 0],\n",
    "         [0, 0, 0],\n",
    "         [0, 1, 0]]]\n",
    "    ])\n",
    "    \n",
    "    def shift(x, w):\n",
    "        c = convolve(x.reshape((3,32,32)), mode='constant', weights=w).ravel()\n",
    "        return c\n",
    "\n",
    "    X = np.concatenate([X] +\n",
    "                       [np.apply_along_axis(shift, 1, X, vector)\n",
    "                        for vector in direction_vectors])\n",
    "    Y = np.concatenate([Y for _ in range(5)], axis=0)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to learn good latent representations from a small dataset, \n",
    "# we artificially generate more labeled data by perturbing the training\n",
    "# data with linear shifts of 1 pixel in each direction.\n",
    "X, Y = nudge_dataset(X, Y)\n",
    "x_test, y_test = nudge_dataset(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plti(im, h=2, **kwargs):\n",
    "    \"\"\"\n",
    "    Helper function to plot an image.\n",
    "    \"\"\"\n",
    "    y = im.shape[0]\n",
    "    x = im.shape[1]\n",
    "    w = (y/x) * h\n",
    "    plt.figure(figsize=(w,h))\n",
    "    plt.imshow(im, interpolation=\"none\", **kwargs)\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot augmented images\n",
    "\n",
    "plti(X[0].reshape((3,32,32)).transpose(1,2,0))\n",
    "plti(X[50000].reshape((3,32,32)).transpose(1,2,0))\n",
    "plti(X[100000].reshape((3,32,32)).transpose(1,2,0))\n",
    "plti(X[150000].reshape((3,32,32)).transpose(1,2,0))\n",
    "plti(X[200000].reshape((3,32,32)).transpose(1,2,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After augmentation the dataset is artificially increased by 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Reduction\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_labels(df,indexes,val = 10):\n",
    "    \"\"\"\n",
    "    Returns a modified label dataset filtered ot use only the classes specified\n",
    "\n",
    "    Arguments:\n",
    "    indees: indexes to filter\n",
    "    df : label dataset\n",
    "    val: new label value\n",
    "\n",
    "    Returns:\n",
    "    fdf - filtered df\n",
    "      \n",
    "    \"\"\"\n",
    "\n",
    "    def f(n):\n",
    "        if n in indexes:\n",
    "            return n\n",
    "        else:\n",
    "            return val\n",
    "    \n",
    "    return np.array(list(map(f,df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplify label datasets to only the classes we care about for task 1\n",
    "Ycatdog = simplify_labels(Y,[3,5],10)\n",
    "y_test_catdog = simplify_labels(y_test,[3,5],10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplify label datasets to only the classes we care about for task 2\n",
    "Ybirdplane = simplify_labels(Y,[0,2],10)\n",
    "y_test_birdplane = simplify_labels(y_test,[0,2],10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Scaling\n",
    "\n",
    "Multi-layer Perceptrons are sensitive to feature scaling, so sklearn highly recommendeds scaling the data.  The same scaling must be applied to the test set for meaningful results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "### Data Scaling\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "scaler = StandardScaler()  \n",
    "scaler.fit(X)  \n",
    "Xmlp = scaler.transform(X)  \n",
    "x_testmlp = scaler.transform(x_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation Part 2\n",
    "\n",
    "The final training dataset is 5 times larger than the original due to data augmentation. Also, we've created 4 new label datasets targeting the labels we want our models to classify\n",
    "\n",
    "- Ycatdog, y_test_catdog  : Cat and dog labels are retained, all others forced to next unused label\n",
    "- Ybirdplane,y_test_birdplane : Bird and plane  labels are retained, all others forced to next unused label\n",
    "- Xmlp,x_testmlp : Scaled training and test datasets\n",
    "\n",
    "\n",
    "We also scaled the dataset by subtracting the mean and dividing by the standard deviation using the sklearn builtin [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented Data shape:  (1250000, 3072)\n",
      "Augmented Dog/Cat Labels shape:  (1250000,)\n",
      "Augmented Bird/Plane Labels shape:  (1250000,)\n",
      "Augmented Test Data shape:  (250000, 3072)\n",
      "Augmented Dog/Cat Test Labels shape:  (250000,)\n",
      "Augmented Bird/Plane Test Labels shape:  (250000,)\n",
      "The original dataset range is 0 255\n",
      "The scaled dataset range is -2.2025231485140195 2.6196548440090894\n"
     ]
    }
   ],
   "source": [
    "print(\"Augmented Data shape: \",X.shape)\n",
    "print(\"Augmented Dog/Cat Labels shape: \",Ycatdog.shape)\n",
    "print(\"Augmented Bird/Plane Labels shape: \",Ybirdplane.shape)\n",
    "\n",
    "print(\"Augmented Test Data shape: \",x_test.shape)\n",
    "print(\"Augmented Dog/Cat Test Labels shape: \",y_test_catdog.shape)\n",
    "print(\"Augmented Bird/Plane Test Labels shape: \",y_test_birdplane.shape)\n",
    "\n",
    "print(\"The original dataset range is\",np.min(X),np.max(X))\n",
    "print(\"The scaled dataset range is\",np.min(Xmlp),np.max(x_testmlp))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using machine learning to detect and classify images is becoming a very common occurrence. It is being used for the detection of oil spills in satellite radar images as well as gender classification on real-world face images. \n",
    "\n",
    "For our project we will classify images into 2 sets of 2 classes. Overall accuracy is a definite concern. We want to ensure that the number of false positives are low. Another factor that influences the number of false positives/ negatives is if the classes are not balanced. For our data set this is not a concern because our ten classes each have the same number of samples.\n",
    "\n",
    "We use class model score() methods to gauge the accuracy of our models.  The dataset we chose had previously split the dataset into training and test sets.  The dataset was divided into five training batches and one test batch, each with 10000 images. \n",
    "\n",
    "The test batch contained exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order. The original training dataset contain exactly 25000 images from each class.  The augmented datasets are 5 times the original datasets sizes, but the same ratio between train and test datasets exist in our augmented training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We didn't modify the train/test CFAR10 dataset training and test data split as it represents an industry standard baseline test for models as is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We identified the two sets of classes we wish to build models to classify from the dataset.\n",
    "\n",
    "Task 1: Classify Cats and dogs\n",
    "\n",
    "    Model A: Naive Bayes\n",
    "    Model B: Multi-layer Perceptron classifier (MLP)\n",
    "    Model C: Support Vector Machine (SVM)\n",
    "    \n",
    "Task 2: Classify Birds and airplanes\n",
    "\n",
    "    Model A: Naive Bayes\n",
    "    Model B: Multi-layer Perceptron classifier (MLP)\n",
    "    Model C: Support Vector Machine (SVM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildmodel(nmodel,X,Y,x_test,y_test,**modelargs):\n",
    "    \"\"\"\n",
    "    Builds one of a subset of sklearn models and returns a score\n",
    "    \n",
    "    Arguments:\n",
    "    modeltype -- One of either \"multi\", \"gauss\", \"bernoulli\", \"c\", \"nn\" \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    if nmodel == 'multi':\n",
    "        m = b.MultinomialNB(**modelargs)\n",
    "    elif nmodel == 'gauss':\n",
    "        m = b.GaussianNB(**modelargs)\n",
    "    elif nmodel == 'bernoulli':\n",
    "        m = b.BernoulliNB(**modelargs)\n",
    "    elif nmodel == \"mlp\":\n",
    "        m = MLPClassifier(**modelargs)\n",
    "    elif nmodel == \"svm\":\n",
    "        m = SGDClassifier(**modelargs)\n",
    "    \n",
    "    m.fit(X, Y)\n",
    "    score = m.score(x_test,y_test) * 100\n",
    "\n",
    "    return m,score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "berboulliNB, bernoulliNBScore = buildmodel('bernoulli',X,Ycatdog,x_test,y_test_catdog,alpha = .01)\n",
    "print(\"BernoulliNB score :%\",round(bernoulliNBScore,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "berboulliNBbp, bernoulliNBScorebp = buildmodel('bernoulli',X,Ybirdplane,x_test,y_test_birdplane,alpha = .01)\n",
    "print(\"BernoulliNB score :%\",round(bernoulliNBScorebp,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.55540982\n",
      "Iteration 2, loss = 0.48965374\n",
      "Iteration 3, loss = 0.47411030\n",
      "Iteration 4, loss = 0.46777918\n",
      "Iteration 5, loss = 0.46319362\n",
      "Iteration 6, loss = 0.45959056\n",
      "Iteration 7, loss = 0.45735656\n",
      "Iteration 8, loss = 0.45467369\n",
      "Iteration 9, loss = 0.45267992\n",
      "Iteration 10, loss = 0.45062179\n",
      "Iteration 11, loss = 0.44867539\n",
      "Iteration 12, loss = 0.44809624\n",
      "Iteration 13, loss = 0.44678363\n",
      "Iteration 14, loss = 0.44484739\n",
      "Iteration 15, loss = 0.44373537\n",
      "Iteration 16, loss = 0.44330410\n",
      "Iteration 17, loss = 0.44198703\n",
      "Iteration 18, loss = 0.44084836\n",
      "Iteration 19, loss = 0.44063666\n",
      "Iteration 20, loss = 0.43969698\n",
      "Iteration 21, loss = 0.43916001\n",
      "Iteration 22, loss = 0.43794461\n",
      "Iteration 23, loss = 0.43733903\n",
      "Iteration 24, loss = 0.43702245\n",
      "Iteration 25, loss = 0.43688401\n",
      "Iteration 26, loss = 0.43543648\n",
      "Iteration 27, loss = 0.43493762\n",
      "Iteration 28, loss = 0.43503960\n",
      "Iteration 29, loss = 0.43450510\n",
      "Iteration 30, loss = 0.43435107\n",
      "Iteration 31, loss = 0.43348169\n",
      "Iteration 32, loss = 0.43308199\n",
      "Training loss did not improve more than tol=0.001000 for 5 consecutive epochs. Stopping.\n",
      "Multi-layer Perceptron score :% 81.69\n"
     ]
    }
   ],
   "source": [
    "mlp, mlpScore = buildmodel('mlp',Xmlp,Ycatdog,x_testmlp,y_test_catdog,\n",
    "                           activation = 'relu',hidden_layer_sizes=(10,),learning_rate ='adaptive',verbose = True, tol = .001,\n",
    "                           max_iter = 50, n_iter_no_change =5)\n",
    "print(\"Multi-layer Perceptron score :%\",round(mlpScore,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1-- Epoch 1\n",
      "-- Epoch 1\n",
      "\n",
      "Norm: 27.07, NNZs: 3072, Bias: 130.060532, T: 250000, Avg. loss: 34.974994\n",
      "Total training time: 2.21 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 43.89, NNZs: 3072, Bias: -470.710591, T: 250000, Avg. loss: 38.039267\n",
      "Total training time: 2.66 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 47.16, NNZs: 3072, Bias: -466.576966, T: 250000, Avg. loss: 40.663918\n",
      "Total training time: 2.76 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 15.90, NNZs: 3072, Bias: 53.172496, T: 500000, Avg. loss: 10.383192\n",
      "Total training time: 4.28 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 40.62, NNZs: 3072, Bias: -427.050795, T: 500000, Avg. loss: 27.027816\n",
      "Total training time: 5.36 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 44.73, NNZs: 3072, Bias: -418.679253, T: 500000, Avg. loss: 28.896995\n",
      "Total training time: 5.41 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 7.61, NNZs: 3072, Bias: 12.249395, T: 750000, Avg. loss: 3.756211\n",
      "Total training time: 6.32 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 43.08, NNZs: 3072, Bias: -390.768627, T: 750000, Avg. loss: 26.380927\n",
      "Total training time: 8.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 38.62, NNZs: 3072, Bias: -401.551005, T: 750000, Avg. loss: 24.919920\n",
      "Total training time: 8.02 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.72, NNZs: 3072, Bias: 2.498356, T: 1000000, Avg. loss: 0.825645\n",
      "Total training time: 8.33 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.00, NNZs: 3072, Bias: 2.279655, T: 1250000, Avg. loss: 0.560440\n",
      "Total training time: 10.39 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 41.74, NNZs: 3072, Bias: -370.998661, T: 1000000, Avg. loss: 24.775194\n",
      "Total training time: 10.54 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 37.78, NNZs: 3072, Bias: -383.438061, T: 1000000, Avg. loss: 23.563265\n",
      "Total training time: 10.68 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.75, NNZs: 3072, Bias: 2.139812, T: 1500000, Avg. loss: 0.525527\n",
      "Total training time: 12.46 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 40.54, NNZs: 3072, Bias: -355.711080, T: 1250000, Avg. loss: 23.590169\n",
      "Total training time: 12.98 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 36.70, NNZs: 3072, Bias: -369.445881, T: 1250000, Avg. loss: 22.557884\n",
      "Total training time: 13.27 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.60, NNZs: 3072, Bias: 2.073141, T: 1750000, Avg. loss: 0.504486\n",
      "Total training time: 14.49 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 39.32, NNZs: 3072, Bias: -343.262625, T: 1500000, Avg. loss: 22.660837\n",
      "Total training time: 15.32 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 35.99, NNZs: 3072, Bias: -358.000464, T: 1500000, Avg. loss: 21.771124\n",
      "Total training time: 15.83 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.51, NNZs: 3072, Bias: 2.029167, T: 2000000, Avg. loss: 0.489813\n",
      "Total training time: 16.57 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 38.47, NNZs: 3072, Bias: -332.732931, T: 1750000, Avg. loss: 21.897238\n",
      "Total training time: 17.64 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 35.20, NNZs: 3072, Bias: -348.353894, T: 1750000, Avg. loss: 21.120670\n",
      "Total training time: 18.31 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 1.46, NNZs: 3072, Bias: 2.003864, T: 2250000, Avg. loss: 0.479955\n",
      "Total training time: 18.62 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 37.88, NNZs: 3072, Bias: -323.605946, T: 2000000, Avg. loss: 21.234028\n",
      "Total training time: 19.93 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 1.42, NNZs: 3072, Bias: 1.958703, T: 2500000, Avg. loss: 0.471499\n",
      "Total training time: 20.62 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 34.47, NNZs: 3072, Bias: -340.007101, T: 2000000, Avg. loss: 20.568000\n",
      "Total training time: 20.81 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 37.33, NNZs: 3072, Bias: -315.564064, T: 2250000, Avg. loss: 20.667030\n",
      "Total training time: 22.13 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 1.39, NNZs: 3072, Bias: 1.925938, T: 2750000, Avg. loss: 0.465746\n",
      "Total training time: 22.67 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 34.23, NNZs: 3072, Bias: -332.607915, T: 2250000, Avg. loss: 20.089317\n",
      "Total training time: 23.24 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 36.66, NNZs: 3072, Bias: -308.399519, T: 2500000, Avg. loss: 20.159497\n",
      "Total training time: 24.31 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 1.36, NNZs: 3072, Bias: 1.920763, T: 3000000, Avg. loss: 0.461063\n",
      "Total training time: 24.73 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 33.76, NNZs: 3072, Bias: -326.022483, T: 2500000, Avg. loss: 19.659317\n",
      "Total training time: 25.57 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 36.20, NNZs: 3072, Bias: -301.907792, T: 2750000, Avg. loss: 19.710018\n",
      "Total training time: 26.50 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 1.33, NNZs: 3072, Bias: 1.925656, T: 3250000, Avg. loss: 0.457087\n",
      "Total training time: 26.81 seconds.\n",
      "Convergence after 13 epochs took 26.81 seconds\n",
      "Norm: 33.37, NNZs: 3072, Bias: -320.063959, T: 2750000, Avg. loss: 19.274928\n",
      "Total training time: 27.84 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 35.74, NNZs: 3072, Bias: -295.988723, T: 3000000, Avg. loss: 19.297398\n",
      "Total training time: 28.58 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 32.97, NNZs: 3072, Bias: -314.632152, T: 3000000, Avg. loss: 18.934277\n",
      "Total training time: 30.01 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 35.21, NNZs: 3072, Bias: -290.561029, T: 3250000, Avg. loss: 18.916485\n",
      "Total training time: 30.59 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 32.55, NNZs: 3072, Bias: -309.643748, T: 3250000, Avg. loss: 18.609491\n",
      "Total training time: 32.16 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 34.85, NNZs: 3072, Bias: -285.522066, T: 3500000, Avg. loss: 18.573379\n",
      "Total training time: 32.64 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 32.29, NNZs: 3072, Bias: -305.012195, T: 3500000, Avg. loss: 18.314820\n",
      "Total training time: 34.31 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 34.46, NNZs: 3072, Bias: -280.841120, T: 3750000, Avg. loss: 18.254703\n",
      "Total training time: 34.69 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 32.02, NNZs: 3072, Bias: -300.706612, T: 3750000, Avg. loss: 18.044480\n",
      "Total training time: 36.47 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 34.10, NNZs: 3072, Bias: -276.465048, T: 4000000, Avg. loss: 17.957619\n",
      "Total training time: 36.77 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 31.65, NNZs: 3072, Bias: -296.693289, T: 4000000, Avg. loss: 17.785456\n",
      "Total training time: 38.70 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 33.76, NNZs: 3072, Bias: -272.358085, T: 4250000, Avg. loss: 17.668154\n",
      "Total training time: 38.91 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 31.43, NNZs: 3072, Bias: -292.911284, T: 4250000, Avg. loss: 17.550020\n",
      "Total training time: 40.84 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 33.50, NNZs: 3072, Bias: -268.480668, T: 4500000, Avg. loss: 17.411517\n",
      "Total training time: 40.94 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 31.16, NNZs: 3072, Bias: -289.353867, T: 4500000, Avg. loss: 17.326427\n",
      "Total training time: 42.91 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 33.17, NNZs: 3072, Bias: -264.825424, T: 4750000, Avg. loss: 17.161998\n",
      "Total training time: 42.94 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 32.95, NNZs: 3072, Bias: -261.347206, T: 5000000, Avg. loss: 16.926004\n",
      "Total training time: 45.00 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 30.88, NNZs: 3072, Bias: -285.992539, T: 4750000, Avg. loss: 17.114777\n",
      "Total training time: 45.03 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 32.60, NNZs: 3072, Bias: -258.058454, T: 5250000, Avg. loss: 16.708187\n",
      "Total training time: 47.01 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 30.74, NNZs: 3072, Bias: -282.791876, T: 5000000, Avg. loss: 16.913901\n",
      "Total training time: 47.06 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 32.37, NNZs: 3072, Bias: -254.911440, T: 5500000, Avg. loss: 16.492455\n",
      "Total training time: 49.03 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 30.53, NNZs: 3072, Bias: -279.757410, T: 5250000, Avg. loss: 16.728081\n",
      "Total training time: 49.12 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 32.16, NNZs: 3072, Bias: -251.905046, T: 5750000, Avg. loss: 16.293516\n",
      "Total training time: 51.05 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 30.38, NNZs: 3072, Bias: -276.859537, T: 5500000, Avg. loss: 16.545978\n",
      "Total training time: 51.15 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 31.85, NNZs: 3072, Bias: -249.040869, T: 6000000, Avg. loss: 16.095510\n",
      "Total training time: 53.06 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 30.12, NNZs: 3072, Bias: -274.103275, T: 5750000, Avg. loss: 16.373704\n",
      "Total training time: 53.18 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 31.62, NNZs: 3072, Bias: -246.286442, T: 6250000, Avg. loss: 15.914711\n",
      "Total training time: 55.07 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 29.90, NNZs: 3072, Bias: -271.462602, T: 6000000, Avg. loss: 16.211283\n",
      "Total training time: 55.23 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 31.42, NNZs: 3072, Bias: -243.638740, T: 6500000, Avg. loss: 15.736200\n",
      "Total training time: 57.07 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 29.75, NNZs: 3072, Bias: -268.923875, T: 6250000, Avg. loss: 16.053186\n",
      "Total training time: 57.27 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 31.15, NNZs: 3072, Bias: -241.101196, T: 6750000, Avg. loss: 15.567653\n",
      "Total training time: 59.01 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 29.60, NNZs: 3072, Bias: -266.486015, T: 6500000, Avg. loss: 15.900878\n",
      "Total training time: 59.28 seconds.\n",
      "-- Epoch 27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 30.93, NNZs: 3072, Bias: -238.652882, T: 7000000, Avg. loss: 15.402875\n",
      "Total training time: 61.00 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 29.37, NNZs: 3072, Bias: -264.149772, T: 6750000, Avg. loss: 15.757735\n",
      "Total training time: 61.26 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 30.74, NNZs: 3072, Bias: -236.288833, T: 7250000, Avg. loss: 15.246083\n",
      "Total training time: 62.95 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 29.30, NNZs: 3072, Bias: -261.883726, T: 7000000, Avg. loss: 15.618915\n",
      "Total training time: 63.24 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 30.55, NNZs: 3072, Bias: -234.006215, T: 7500000, Avg. loss: 15.092793\n",
      "Total training time: 64.94 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 29.08, NNZs: 3072, Bias: -259.714015, T: 7250000, Avg. loss: 15.481117\n",
      "Total training time: 65.22 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 30.34, NNZs: 3072, Bias: -231.802151, T: 7750000, Avg. loss: 14.944569\n",
      "Total training time: 66.87 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 28.88, NNZs: 3072, Bias: -257.616454, T: 7500000, Avg. loss: 15.353955\n",
      "Total training time: 67.20 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 30.15, NNZs: 3072, Bias: -229.667601, T: 8000000, Avg. loss: 14.806106\n",
      "Total training time: 68.83 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 28.77, NNZs: 3072, Bias: -255.579497, T: 7750000, Avg. loss: 15.229824\n",
      "Total training time: 69.19 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 30.00, NNZs: 3072, Bias: -227.595507, T: 8250000, Avg. loss: 14.668124\n",
      "Total training time: 70.82 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 28.62, NNZs: 3072, Bias: -253.613290, T: 8000000, Avg. loss: 15.110052\n",
      "Total training time: 71.18 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 29.79, NNZs: 3072, Bias: -225.594368, T: 8500000, Avg. loss: 14.533944\n",
      "Total training time: 72.78 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 28.50, NNZs: 3072, Bias: -251.704102, T: 8250000, Avg. loss: 14.990080\n",
      "Total training time: 73.13 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 29.63, NNZs: 3072, Bias: -223.644727, T: 8750000, Avg. loss: 14.405642\n",
      "Total training time: 74.77 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 28.36, NNZs: 3072, Bias: -249.855330, T: 8500000, Avg. loss: 14.878396\n",
      "Total training time: 75.12 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 29.44, NNZs: 3072, Bias: -221.756390, T: 9000000, Avg. loss: 14.280085\n",
      "Total training time: 76.74 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 28.23, NNZs: 3072, Bias: -248.060198, T: 8750000, Avg. loss: 14.767451\n",
      "Total training time: 77.08 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 29.29, NNZs: 3072, Bias: -219.916710, T: 9250000, Avg. loss: 14.158524\n",
      "Total training time: 78.67 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 28.11, NNZs: 3072, Bias: -246.315721, T: 9000000, Avg. loss: 14.661192\n",
      "Total training time: 79.07 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 29.15, NNZs: 3072, Bias: -218.124959, T: 9500000, Avg. loss: 14.040330\n",
      "Total training time: 80.67 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 27.95, NNZs: 3072, Bias: -244.623363, T: 9250000, Avg. loss: 14.556095\n",
      "Total training time: 81.06 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 28.94, NNZs: 3072, Bias: -216.390166, T: 9750000, Avg. loss: 13.922659\n",
      "Total training time: 82.63 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 27.86, NNZs: 3072, Bias: -242.969381, T: 9500000, Avg. loss: 14.455250\n",
      "Total training time: 83.04 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 28.84, NNZs: 3072, Bias: -214.685770, T: 10000000, Avg. loss: 13.809491\n",
      "Total training time: 84.56 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 27.77, NNZs: 3072, Bias: -241.359180, T: 9750000, Avg. loss: 14.355665\n",
      "Total training time: 85.01 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 28.65, NNZs: 3072, Bias: -213.036229, T: 10250000, Avg. loss: 13.700537\n",
      "Total training time: 86.57 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 27.62, NNZs: 3072, Bias: -239.797468, T: 10000000, Avg. loss: 14.260136\n",
      "Total training time: 87.02 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 28.50, NNZs: 3072, Bias: -211.421733, T: 10500000, Avg. loss: 13.596068\n",
      "Total training time: 88.52 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 27.51, NNZs: 3072, Bias: -238.269558, T: 10250000, Avg. loss: 14.168171\n",
      "Total training time: 89.05 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 28.38, NNZs: 3072, Bias: -209.842234, T: 10750000, Avg. loss: 13.490900\n",
      "Total training time: 90.49 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 27.39, NNZs: 3072, Bias: -236.781744, T: 10500000, Avg. loss: 14.077216\n",
      "Total training time: 90.99 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 28.24, NNZs: 3072, Bias: -208.302627, T: 11000000, Avg. loss: 13.390065\n",
      "Total training time: 92.49 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 27.29, NNZs: 3072, Bias: -235.326034, T: 10750000, Avg. loss: 13.987714\n",
      "Total training time: 92.99 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 28.13, NNZs: 3072, Bias: -206.796169, T: 11250000, Avg. loss: 13.290519\n",
      "Total training time: 94.46 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 27.14, NNZs: 3072, Bias: -233.910293, T: 11000000, Avg. loss: 13.900093\n",
      "Total training time: 94.95 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 27.97, NNZs: 3072, Bias: -205.328189, T: 11500000, Avg. loss: 13.193040\n",
      "Total training time: 96.42 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 27.05, NNZs: 3072, Bias: -232.520643, T: 11250000, Avg. loss: 13.814960\n",
      "Total training time: 96.93 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 27.83, NNZs: 3072, Bias: -203.890799, T: 11750000, Avg. loss: 13.098875\n",
      "Total training time: 98.35 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 26.96, NNZs: 3072, Bias: -231.161977, T: 11500000, Avg. loss: 13.734381\n",
      "Total training time: 98.88 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 27.71, NNZs: 3072, Bias: -202.482594, T: 12000000, Avg. loss: 13.002826\n",
      "Total training time: 100.28 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 26.86, NNZs: 3072, Bias: -229.834026, T: 11750000, Avg. loss: 13.652239\n",
      "Total training time: 100.84 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 27.56, NNZs: 3072, Bias: -201.106787, T: 12250000, Avg. loss: 12.914359\n",
      "Total training time: 102.24 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 26.74, NNZs: 3072, Bias: -228.536531, T: 12000000, Avg. loss: 13.572493\n",
      "Total training time: 102.80 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 27.44, NNZs: 3072, Bias: -199.756512, T: 12500000, Avg. loss: 12.826605\n",
      "Total training time: 104.21 seconds.\n",
      "-- Epoch 51\n",
      "Norm: 26.66, NNZs: 3072, Bias: -227.261607, T: 12250000, Avg. loss: 13.495368\n",
      "Total training time: 104.79 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 27.31, NNZs: 3072, Bias: -198.434043, T: 12750000, Avg. loss: 12.738245\n",
      "Total training time: 106.22 seconds.\n",
      "-- Epoch 52\n",
      "Norm: 26.56, NNZs: 3072, Bias: -226.015859, T: 12500000, Avg. loss: 13.419367\n",
      "Total training time: 106.77 seconds.\n",
      "-- Epoch 51\n",
      "Norm: 27.22, NNZs: 3072, Bias: -197.133038, T: 13000000, Avg. loss: 12.654153\n",
      "Total training time: 108.20 seconds.\n",
      "-- Epoch 53\n",
      "Norm: 26.51, NNZs: 3072, Bias: -224.789134, T: 12750000, Avg. loss: 13.344410\n",
      "Total training time: 108.76 seconds.\n",
      "-- Epoch 52\n",
      "Norm: 27.08, NNZs: 3072, Bias: -195.863082, T: 13250000, Avg. loss: 12.569559\n",
      "Total training time: 110.17 seconds.\n",
      "-- Epoch 54\n",
      "Norm: 26.39, NNZs: 3072, Bias: -223.594056, T: 13000000, Avg. loss: 13.272218\n",
      "Total training time: 110.70 seconds.\n",
      "-- Epoch 53\n",
      "Norm: 26.97, NNZs: 3072, Bias: -194.614580, T: 13500000, Avg. loss: 12.488292\n",
      "Total training time: 112.15 seconds.\n",
      "-- Epoch 55\n",
      "Norm: 26.32, NNZs: 3072, Bias: -222.417300, T: 13250000, Avg. loss: 13.200253\n",
      "Total training time: 112.73 seconds.\n",
      "-- Epoch 54\n",
      "Norm: 26.88, NNZs: 3072, Bias: -193.387306, T: 13750000, Avg. loss: 12.406990\n",
      "Total training time: 114.13 seconds.\n",
      "-- Epoch 56\n",
      "Norm: 26.22, NNZs: 3072, Bias: -221.265789, T: 13500000, Avg. loss: 13.130347\n",
      "Total training time: 114.71 seconds.\n",
      "-- Epoch 55\n",
      "Norm: 26.75, NNZs: 3072, Bias: -192.187005, T: 14000000, Avg. loss: 12.328409\n",
      "Total training time: 116.14 seconds.\n",
      "-- Epoch 57\n",
      "Norm: 26.13, NNZs: 3072, Bias: -220.134971, T: 13750000, Avg. loss: 13.061537\n",
      "Total training time: 116.75 seconds.\n",
      "-- Epoch 56\n",
      "Norm: 26.65, NNZs: 3072, Bias: -191.005580, T: 14250000, Avg. loss: 12.249173\n",
      "Total training time: 118.12 seconds.\n",
      "-- Epoch 58\n",
      "Norm: 26.04, NNZs: 3072, Bias: -219.024632, T: 14000000, Avg. loss: 12.992744\n",
      "Total training time: 118.75 seconds.\n",
      "-- Epoch 57\n",
      "Norm: 26.54, NNZs: 3072, Bias: -189.845278, T: 14500000, Avg. loss: 12.174527\n",
      "Total training time: 120.10 seconds.\n",
      "-- Epoch 59\n",
      "Norm: 25.97, NNZs: 3072, Bias: -217.932164, T: 14250000, Avg. loss: 12.927911\n",
      "Total training time: 120.75 seconds.\n",
      "-- Epoch 58\n",
      "Norm: 26.44, NNZs: 3072, Bias: -188.705411, T: 14750000, Avg. loss: 12.099013\n",
      "Total training time: 122.03 seconds.\n",
      "-- Epoch 60\n",
      "Norm: 25.87, NNZs: 3072, Bias: -216.861834, T: 14500000, Avg. loss: 12.861754\n",
      "Total training time: 122.72 seconds.\n",
      "-- Epoch 59\n",
      "Norm: 26.32, NNZs: 3072, Bias: -187.586703, T: 15000000, Avg. loss: 12.024895\n",
      "Total training time: 124.01 seconds.\n",
      "-- Epoch 61\n",
      "Norm: 25.79, NNZs: 3072, Bias: -215.808307, T: 14750000, Avg. loss: 12.799103\n",
      "Total training time: 124.68 seconds.\n",
      "-- Epoch 60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 26.21, NNZs: 3072, Bias: -186.486059, T: 15250000, Avg. loss: 11.953276\n",
      "Total training time: 125.94 seconds.\n",
      "-- Epoch 62\n",
      "Norm: 25.73, NNZs: 3072, Bias: -214.770904, T: 15000000, Avg. loss: 12.734595\n",
      "Total training time: 126.69 seconds.\n",
      "-- Epoch 61\n",
      "Norm: 26.13, NNZs: 3072, Bias: -185.399902, T: 15500000, Avg. loss: 11.884403\n",
      "Total training time: 127.92 seconds.\n",
      "-- Epoch 63\n",
      "Norm: 25.66, NNZs: 3072, Bias: -213.751353, T: 15250000, Avg. loss: 12.673111\n",
      "Total training time: 128.69 seconds.\n",
      "-- Epoch 62\n",
      "Norm: 26.00, NNZs: 3072, Bias: -184.337607, T: 15750000, Avg. loss: 11.810502\n",
      "Total training time: 129.90 seconds.\n",
      "-- Epoch 64\n",
      "Norm: 25.59, NNZs: 3072, Bias: -212.749401, T: 15500000, Avg. loss: 12.612642\n",
      "Total training time: 130.67 seconds.\n",
      "-- Epoch 63\n",
      "Norm: 25.93, NNZs: 3072, Bias: -183.284931, T: 16000000, Avg. loss: 11.744735\n",
      "Total training time: 131.89 seconds.\n",
      "-- Epoch 65\n",
      "Norm: 25.49, NNZs: 3072, Bias: -211.766848, T: 15750000, Avg. loss: 12.552382\n",
      "Total training time: 132.69 seconds.\n",
      "-- Epoch 64\n",
      "Norm: 25.81, NNZs: 3072, Bias: -182.255817, T: 16250000, Avg. loss: 11.676031\n",
      "Total training time: 133.87 seconds.\n",
      "-- Epoch 66\n",
      "Norm: 25.42, NNZs: 3072, Bias: -210.797463, T: 16000000, Avg. loss: 12.494280\n",
      "Total training time: 134.63 seconds.\n",
      "-- Epoch 65\n",
      "Norm: 25.73, NNZs: 3072, Bias: -181.237506, T: 16500000, Avg. loss: 11.610452\n",
      "Total training time: 135.79 seconds.\n",
      "-- Epoch 67\n",
      "Norm: 25.34, NNZs: 3072, Bias: -209.843503, T: 16250000, Avg. loss: 12.436225\n",
      "Total training time: 136.54 seconds.\n",
      "-- Epoch 66\n",
      "Norm: 25.63, NNZs: 3072, Bias: -180.236981, T: 16750000, Avg. loss: 11.543812\n",
      "Total training time: 137.70 seconds.\n",
      "-- Epoch 68\n",
      "Norm: 25.28, NNZs: 3072, Bias: -208.901942, T: 16500000, Avg. loss: 12.379326\n",
      "Total training time: 138.49 seconds.\n",
      "-- Epoch 67\n",
      "Norm: 25.53, NNZs: 3072, Bias: -179.252329, T: 17000000, Avg. loss: 11.479690\n",
      "Total training time: 139.66 seconds.\n",
      "-- Epoch 69\n",
      "Norm: 25.20, NNZs: 3072, Bias: -207.977809, T: 16750000, Avg. loss: 12.322720\n",
      "Total training time: 140.43 seconds.\n",
      "-- Epoch 68\n",
      "Norm: 25.44, NNZs: 3072, Bias: -178.282171, T: 17250000, Avg. loss: 11.415128\n",
      "Total training time: 141.58 seconds.\n",
      "-- Epoch 70\n",
      "Norm: 25.12, NNZs: 3072, Bias: -207.067927, T: 17000000, Avg. loss: 12.267585\n",
      "Total training time: 142.45 seconds.\n",
      "-- Epoch 69\n",
      "Norm: 25.34, NNZs: 3072, Bias: -177.325365, T: 17500000, Avg. loss: 11.354519\n",
      "Total training time: 143.55 seconds.\n",
      "-- Epoch 71\n",
      "Norm: 25.05, NNZs: 3072, Bias: -206.169410, T: 17250000, Avg. loss: 12.214219\n",
      "Total training time: 144.41 seconds.\n",
      "-- Epoch 70\n",
      "Norm: 25.26, NNZs: 3072, Bias: -176.381063, T: 17750000, Avg. loss: 11.293338\n",
      "Total training time: 145.50 seconds.\n",
      "-- Epoch 72\n",
      "Norm: 24.99, NNZs: 3072, Bias: -205.283452, T: 17500000, Avg. loss: 12.159156\n",
      "Total training time: 146.33 seconds.\n",
      "-- Epoch 71\n",
      "Norm: 25.17, NNZs: 3072, Bias: -175.451974, T: 18000000, Avg. loss: 11.231879\n",
      "Total training time: 147.46 seconds.\n",
      "-- Epoch 73\n",
      "Norm: 24.92, NNZs: 3072, Bias: -204.411467, T: 17750000, Avg. loss: 12.107006\n",
      "Total training time: 148.30 seconds.\n",
      "-- Epoch 72\n",
      "Norm: 25.08, NNZs: 3072, Bias: -174.535837, T: 18250000, Avg. loss: 11.171872\n",
      "Total training time: 149.40 seconds.\n",
      "-- Epoch 74\n",
      "Norm: 24.86, NNZs: 3072, Bias: -203.549872, T: 18000000, Avg. loss: 12.055264\n",
      "Total training time: 150.27 seconds.\n",
      "-- Epoch 73\n",
      "Norm: 24.99, NNZs: 3072, Bias: -173.631765, T: 18500000, Avg. loss: 11.112682\n",
      "Total training time: 151.41 seconds.\n",
      "-- Epoch 75\n",
      "Norm: 24.79, NNZs: 3072, Bias: -202.703375, T: 18250000, Avg. loss: 12.003261\n",
      "Total training time: 152.24 seconds.\n",
      "-- Epoch 74\n",
      "Norm: 24.91, NNZs: 3072, Bias: -172.739449, T: 18750000, Avg. loss: 11.055124\n",
      "Total training time: 153.35 seconds.\n",
      "-- Epoch 76\n",
      "Norm: 24.72, NNZs: 3072, Bias: -201.867629, T: 18500000, Avg. loss: 11.952170\n",
      "Total training time: 154.26 seconds.\n",
      "-- Epoch 75\n",
      "Norm: 24.81, NNZs: 3072, Bias: -171.861731, T: 19000000, Avg. loss: 10.996350\n",
      "Total training time: 155.36 seconds.\n",
      "-- Epoch 77\n",
      "Norm: 24.66, NNZs: 3072, Bias: -201.041022, T: 18750000, Avg. loss: 11.903496\n",
      "Total training time: 156.22 seconds.\n",
      "-- Epoch 76\n",
      "Norm: 24.74, NNZs: 3072, Bias: -170.991981, T: 19250000, Avg. loss: 10.940601\n",
      "Total training time: 157.34 seconds.\n",
      "-- Epoch 78\n",
      "Norm: 24.61, NNZs: 3072, Bias: -200.225928, T: 19000000, Avg. loss: 11.852802\n",
      "Total training time: 158.19 seconds.\n",
      "-- Epoch 77\n",
      "Norm: 24.67, NNZs: 3072, Bias: -170.133822, T: 19500000, Avg. loss: 10.883752\n",
      "Total training time: 159.28 seconds.\n",
      "-- Epoch 79\n",
      "Norm: 24.53, NNZs: 3072, Bias: -199.424511, T: 19250000, Avg. loss: 11.804845\n",
      "Total training time: 160.18 seconds.\n",
      "-- Epoch 78\n",
      "Norm: 24.57, NNZs: 3072, Bias: -169.290383, T: 19750000, Avg. loss: 10.828867\n",
      "Total training time: 161.26 seconds.\n",
      "-- Epoch 80\n",
      "Norm: 24.46, NNZs: 3072, Bias: -198.632490, T: 19500000, Avg. loss: 11.755800\n",
      "Total training time: 162.08 seconds.\n",
      "-- Epoch 79\n",
      "Norm: 24.51, NNZs: 3072, Bias: -168.452375, T: 20000000, Avg. loss: 10.773603\n",
      "Total training time: 163.20 seconds.\n",
      "-- Epoch 81\n",
      "Norm: 24.40, NNZs: 3072, Bias: -197.850491, T: 19750000, Avg. loss: 11.709413\n",
      "Total training time: 164.02 seconds.\n",
      "-- Epoch 80\n",
      "Norm: 24.42, NNZs: 3072, Bias: -167.630733, T: 20250000, Avg. loss: 10.720904\n",
      "Total training time: 165.18 seconds.\n",
      "-- Epoch 82\n",
      "Norm: 24.33, NNZs: 3072, Bias: -197.078560, T: 20000000, Avg. loss: 11.662581\n",
      "Total training time: 165.96 seconds.\n",
      "-- Epoch 81\n",
      "Norm: 24.35, NNZs: 3072, Bias: -166.815542, T: 20500000, Avg. loss: 10.667681\n",
      "Total training time: 167.12 seconds.\n",
      "-- Epoch 83\n",
      "Norm: 24.28, NNZs: 3072, Bias: -196.314570, T: 20250000, Avg. loss: 11.617115\n",
      "Total training time: 167.86 seconds.\n",
      "-- Epoch 82\n",
      "Norm: 24.26, NNZs: 3072, Bias: -166.012903, T: 20750000, Avg. loss: 10.615374\n",
      "Total training time: 169.09 seconds.\n",
      "-- Epoch 84\n",
      "Norm: 24.23, NNZs: 3072, Bias: -195.560084, T: 20500000, Avg. loss: 11.570767\n",
      "Total training time: 169.80 seconds.\n",
      "-- Epoch 83\n",
      "Norm: 24.19, NNZs: 3072, Bias: -165.217046, T: 21000000, Avg. loss: 10.562643\n",
      "Total training time: 171.03 seconds.\n",
      "-- Epoch 85\n",
      "Norm: 24.16, NNZs: 3072, Bias: -194.816751, T: 20750000, Avg. loss: 11.526743\n",
      "Total training time: 171.74 seconds.\n",
      "-- Epoch 84\n",
      "Norm: 24.10, NNZs: 3072, Bias: -164.434436, T: 21250000, Avg. loss: 10.512726\n",
      "Total training time: 172.98 seconds.\n",
      "-- Epoch 86\n",
      "Norm: 24.13, NNZs: 3072, Bias: -194.079078, T: 21000000, Avg. loss: 11.481380\n",
      "Total training time: 173.69 seconds.\n",
      "-- Epoch 85\n",
      "Norm: 24.03, NNZs: 3072, Bias: -163.659432, T: 21500000, Avg. loss: 10.461317\n",
      "Total training time: 174.91 seconds.\n",
      "-- Epoch 87\n",
      "Norm: 24.07, NNZs: 3072, Bias: -193.353228, T: 21250000, Avg. loss: 11.437201\n",
      "Total training time: 175.70 seconds.\n",
      "-- Epoch 86\n",
      "Norm: 23.95, NNZs: 3072, Bias: -162.893631, T: 21750000, Avg. loss: 10.411701\n",
      "Total training time: 176.84 seconds.\n",
      "-- Epoch 88\n",
      "Norm: 24.00, NNZs: 3072, Bias: -192.636087, T: 21500000, Avg. loss: 11.393813\n",
      "Total training time: 177.63 seconds.\n",
      "-- Epoch 87\n",
      "Norm: 23.86, NNZs: 3072, Bias: -162.138308, T: 22000000, Avg. loss: 10.362395\n",
      "Total training time: 178.76 seconds.\n",
      "-- Epoch 89\n",
      "Norm: 23.94, NNZs: 3072, Bias: -191.927986, T: 21750000, Avg. loss: 11.351618\n",
      "Total training time: 179.60 seconds.\n",
      "-- Epoch 88\n",
      "Norm: 23.82, NNZs: 3072, Bias: -161.385370, T: 22250000, Avg. loss: 10.313822\n",
      "Total training time: 180.72 seconds.\n",
      "-- Epoch 90\n",
      "Norm: 23.88, NNZs: 3072, Bias: -191.226500, T: 22000000, Avg. loss: 11.308759\n",
      "Total training time: 181.55 seconds.\n",
      "-- Epoch 89\n",
      "Norm: 23.74, NNZs: 3072, Bias: -160.646542, T: 22500000, Avg. loss: 10.264783\n",
      "Total training time: 182.71 seconds.\n",
      "-- Epoch 91\n",
      "Norm: 23.82, NNZs: 3072, Bias: -190.534336, T: 22250000, Avg. loss: 11.267551\n",
      "Total training time: 183.54 seconds.\n",
      "-- Epoch 90\n",
      "Norm: 23.67, NNZs: 3072, Bias: -159.915025, T: 22750000, Avg. loss: 10.217300\n",
      "Total training time: 184.70 seconds.\n",
      "-- Epoch 92\n",
      "Norm: 23.77, NNZs: 3072, Bias: -189.848413, T: 22500000, Avg. loss: 11.225966\n",
      "Total training time: 185.52 seconds.\n",
      "-- Epoch 91\n",
      "Norm: 23.58, NNZs: 3072, Bias: -159.192996, T: 23000000, Avg. loss: 10.170393\n",
      "Total training time: 186.67 seconds.\n",
      "-- Epoch 93\n",
      "Norm: 23.72, NNZs: 3072, Bias: -189.171013, T: 22750000, Avg. loss: 11.184477\n",
      "Total training time: 187.42 seconds.\n",
      "-- Epoch 92\n",
      "Norm: 23.53, NNZs: 3072, Bias: -158.475777, T: 23250000, Avg. loss: 10.124164\n",
      "Total training time: 188.60 seconds.\n",
      "-- Epoch 94\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 23.67, NNZs: 3072, Bias: -188.500237, T: 23000000, Avg. loss: 11.144931\n",
      "Total training time: 189.45 seconds.\n",
      "-- Epoch 93\n",
      "Norm: 23.45, NNZs: 3072, Bias: -157.768240, T: 23500000, Avg. loss: 10.077491\n",
      "Total training time: 190.57 seconds.\n",
      "-- Epoch 95\n",
      "Norm: 23.62, NNZs: 3072, Bias: -187.837003, T: 23250000, Avg. loss: 11.104460\n",
      "Total training time: 191.42 seconds.\n",
      "-- Epoch 94\n",
      "Norm: 23.38, NNZs: 3072, Bias: -157.068190, T: 23750000, Avg. loss: 10.032574\n",
      "Total training time: 192.52 seconds.\n",
      "-- Epoch 96\n",
      "Norm: 23.58, NNZs: 3072, Bias: -187.179764, T: 23500000, Avg. loss: 11.064919\n",
      "Total training time: 193.40 seconds.\n",
      "-- Epoch 95\n",
      "Norm: 23.32, NNZs: 3072, Bias: -156.374294, T: 24000000, Avg. loss: 9.985727\n",
      "Total training time: 194.56 seconds.\n",
      "-- Epoch 97\n",
      "Norm: 23.51, NNZs: 3072, Bias: -186.532550, T: 23750000, Avg. loss: 11.026457\n",
      "Total training time: 195.45 seconds.\n",
      "-- Epoch 96\n",
      "Norm: 23.25, NNZs: 3072, Bias: -155.690253, T: 24250000, Avg. loss: 9.943172\n",
      "Total training time: 196.58 seconds.\n",
      "-- Epoch 98\n",
      "Norm: 23.46, NNZs: 3072, Bias: -185.891066, T: 24000000, Avg. loss: 10.986868\n",
      "Total training time: 197.49 seconds.\n",
      "-- Epoch 97\n",
      "Norm: 23.18, NNZs: 3072, Bias: -155.011513, T: 24500000, Avg. loss: 9.898577\n",
      "Total training time: 198.61 seconds.\n",
      "-- Epoch 99\n",
      "Norm: 23.41, NNZs: 3072, Bias: -185.255609, T: 24250000, Avg. loss: 10.948519\n",
      "Total training time: 199.61 seconds.\n",
      "-- Epoch 98\n",
      "Norm: 23.12, NNZs: 3072, Bias: -154.339865, T: 24750000, Avg. loss: 9.854120\n",
      "Total training time: 200.67 seconds.\n",
      "-- Epoch 100\n",
      "Norm: 23.36, NNZs: 3072, Bias: -184.627537, T: 24500000, Avg. loss: 10.911794\n",
      "Total training time: 201.65 seconds.\n",
      "-- Epoch 99\n",
      "Norm: 23.05, NNZs: 3072, Bias: -153.676045, T: 25000000, Avg. loss: 9.811971\n",
      "Total training time: 202.71 seconds.\n",
      "-- Epoch 101\n",
      "Norm: 23.31, NNZs: 3072, Bias: -184.005661, T: 24750000, Avg. loss: 10.873963\n",
      "Total training time: 203.69 seconds.\n",
      "-- Epoch 100\n",
      "Norm: 22.98, NNZs: 3072, Bias: -153.018680, T: 25250000, Avg. loss: 9.768820\n",
      "Total training time: 204.75 seconds.\n",
      "-- Epoch 102\n",
      "Norm: 23.25, NNZs: 3072, Bias: -183.391377, T: 25000000, Avg. loss: 10.836086\n",
      "Total training time: 205.69 seconds.\n",
      "-- Epoch 101\n",
      "Norm: 22.92, NNZs: 3072, Bias: -152.366082, T: 25500000, Avg. loss: 9.726285\n",
      "Total training time: 206.74 seconds.\n",
      "-- Epoch 103\n",
      "Norm: 23.21, NNZs: 3072, Bias: -182.780215, T: 25250000, Avg. loss: 10.799071\n",
      "Total training time: 207.75 seconds.\n",
      "-- Epoch 102\n",
      "Norm: 22.85, NNZs: 3072, Bias: -151.723427, T: 25750000, Avg. loss: 9.684533\n",
      "Total training time: 208.78 seconds.\n",
      "-- Epoch 104\n",
      "Norm: 23.16, NNZs: 3072, Bias: -182.177758, T: 25500000, Avg. loss: 10.763817\n",
      "Total training time: 209.83 seconds.\n",
      "-- Epoch 103\n",
      "Norm: 22.79, NNZs: 3072, Bias: -151.083953, T: 26000000, Avg. loss: 9.643388\n",
      "Total training time: 210.81 seconds.\n",
      "-- Epoch 105\n",
      "Norm: 23.10, NNZs: 3072, Bias: -181.581676, T: 25750000, Avg. loss: 10.727381\n",
      "Total training time: 211.78 seconds.\n",
      "-- Epoch 104\n",
      "Norm: 22.72, NNZs: 3072, Bias: -150.452872, T: 26250000, Avg. loss: 9.600648\n",
      "Total training time: 212.76 seconds.\n",
      "-- Epoch 106\n",
      "Norm: 23.05, NNZs: 3072, Bias: -180.990076, T: 26000000, Avg. loss: 10.691894\n",
      "Total training time: 213.77 seconds.\n",
      "-- Epoch 105\n",
      "Norm: 22.66, NNZs: 3072, Bias: -149.826476, T: 26500000, Avg. loss: 9.561734\n",
      "Total training time: 214.75 seconds.\n",
      "-- Epoch 107\n",
      "Norm: 23.00, NNZs: 3072, Bias: -180.404001, T: 26250000, Avg. loss: 10.656863\n",
      "Total training time: 215.74 seconds.\n",
      "-- Epoch 106\n",
      "Norm: 22.60, NNZs: 3072, Bias: -149.206985, T: 26750000, Avg. loss: 9.519506\n",
      "Total training time: 216.74 seconds.\n",
      "-- Epoch 108\n",
      "Norm: 22.96, NNZs: 3072, Bias: -179.822670, T: 26500000, Avg. loss: 10.621999\n",
      "Total training time: 217.77 seconds.\n",
      "-- Epoch 107\n",
      "Norm: 22.55, NNZs: 3072, Bias: -148.590472, T: 27000000, Avg. loss: 9.481311\n",
      "Total training time: 218.74 seconds.\n",
      "-- Epoch 109\n",
      "Norm: 22.92, NNZs: 3072, Bias: -179.247378, T: 26750000, Avg. loss: 10.586361\n",
      "Total training time: 219.76 seconds.\n",
      "-- Epoch 108\n",
      "Norm: 22.48, NNZs: 3072, Bias: -147.983077, T: 27250000, Avg. loss: 9.441292\n",
      "Total training time: 220.75 seconds.\n",
      "-- Epoch 110\n",
      "Norm: 22.88, NNZs: 3072, Bias: -178.677652, T: 27000000, Avg. loss: 10.552342\n",
      "Total training time: 221.76 seconds.\n",
      "-- Epoch 109\n",
      "Norm: 22.41, NNZs: 3072, Bias: -147.382128, T: 27500000, Avg. loss: 9.402146\n",
      "Total training time: 222.73 seconds.\n",
      "-- Epoch 111\n",
      "Norm: 22.83, NNZs: 3072, Bias: -178.113839, T: 27250000, Avg. loss: 10.518823\n",
      "Total training time: 223.72 seconds.\n",
      "-- Epoch 110\n",
      "Norm: 22.36, NNZs: 3072, Bias: -146.783011, T: 27750000, Avg. loss: 9.363425\n",
      "Total training time: 224.68 seconds.\n",
      "-- Epoch 112\n",
      "Norm: 22.79, NNZs: 3072, Bias: -177.554441, T: 27500000, Avg. loss: 10.485127\n",
      "Total training time: 225.68 seconds.\n",
      "-- Epoch 111\n",
      "Norm: 22.31, NNZs: 3072, Bias: -146.190985, T: 28000000, Avg. loss: 9.325415\n",
      "Total training time: 226.62 seconds.\n",
      "-- Epoch 113\n",
      "Norm: 22.74, NNZs: 3072, Bias: -177.001798, T: 27750000, Avg. loss: 10.452151\n",
      "Total training time: 227.64 seconds.\n",
      "-- Epoch 112\n",
      "Norm: 22.25, NNZs: 3072, Bias: -145.603788, T: 28250000, Avg. loss: 9.286807\n",
      "Total training time: 228.58 seconds.\n",
      "-- Epoch 114\n",
      "Norm: 22.69, NNZs: 3072, Bias: -176.452808, T: 28000000, Avg. loss: 10.418895\n",
      "Total training time: 229.64 seconds.\n",
      "-- Epoch 113\n",
      "Norm: 22.19, NNZs: 3072, Bias: -145.023522, T: 28500000, Avg. loss: 9.249697\n",
      "Total training time: 230.54 seconds.\n",
      "-- Epoch 115\n",
      "Norm: 22.64, NNZs: 3072, Bias: -175.910430, T: 28250000, Avg. loss: 10.385699\n",
      "Total training time: 231.61 seconds.\n",
      "-- Epoch 114\n",
      "Norm: 22.14, NNZs: 3072, Bias: -144.446469, T: 28750000, Avg. loss: 9.212364\n",
      "Total training time: 232.58 seconds.\n",
      "-- Epoch 116\n",
      "Norm: 22.60, NNZs: 3072, Bias: -175.370720, T: 28500000, Avg. loss: 10.353466\n",
      "Total training time: 233.64 seconds.\n",
      "-- Epoch 115\n",
      "Norm: 22.07, NNZs: 3072, Bias: -143.877106, T: 29000000, Avg. loss: 9.174820\n",
      "Total training time: 234.55 seconds.\n",
      "-- Epoch 117\n",
      "Norm: 22.56, NNZs: 3072, Bias: -174.836332, T: 28750000, Avg. loss: 10.321621\n",
      "Total training time: 235.67 seconds.\n",
      "-- Epoch 116\n",
      "Norm: 22.02, NNZs: 3072, Bias: -143.310138, T: 29250000, Avg. loss: 9.137990\n",
      "Total training time: 236.55 seconds.\n",
      "-- Epoch 118\n",
      "Norm: 22.51, NNZs: 3072, Bias: -174.307082, T: 29000000, Avg. loss: 10.288954\n",
      "Total training time: 237.67 seconds.\n",
      "-- Epoch 117\n",
      "Norm: 21.95, NNZs: 3072, Bias: -142.750286, T: 29500000, Avg. loss: 9.102140\n",
      "Total training time: 238.53 seconds.\n",
      "-- Epoch 119\n",
      "Norm: 22.46, NNZs: 3072, Bias: -173.783884, T: 29250000, Avg. loss: 10.257627\n",
      "Total training time: 239.64 seconds.\n",
      "-- Epoch 118\n",
      "Norm: 21.90, NNZs: 3072, Bias: -142.193720, T: 29750000, Avg. loss: 9.065226\n",
      "Total training time: 240.49 seconds.\n",
      "-- Epoch 120\n",
      "Norm: 22.42, NNZs: 3072, Bias: -173.262789, T: 29500000, Avg. loss: 10.226916\n",
      "Total training time: 241.65 seconds.\n",
      "-- Epoch 119\n",
      "Norm: 21.85, NNZs: 3072, Bias: -141.641873, T: 30000000, Avg. loss: 9.029699\n",
      "Total training time: 242.43 seconds.\n",
      "-- Epoch 121\n",
      "Norm: 22.37, NNZs: 3072, Bias: -172.747992, T: 29750000, Avg. loss: 10.195138\n",
      "Total training time: 243.63 seconds.\n",
      "-- Epoch 120\n",
      "Norm: 21.79, NNZs: 3072, Bias: -141.095564, T: 30250000, Avg. loss: 8.993918\n",
      "Total training time: 244.40 seconds.\n",
      "-- Epoch 122\n",
      "Norm: 22.34, NNZs: 3072, Bias: -172.235494, T: 30000000, Avg. loss: 10.165436\n",
      "Total training time: 245.64 seconds.\n",
      "-- Epoch 121\n",
      "Norm: 21.74, NNZs: 3072, Bias: -140.553007, T: 30500000, Avg. loss: 8.959073\n",
      "Total training time: 246.38 seconds.\n",
      "-- Epoch 123\n",
      "Norm: 22.30, NNZs: 3072, Bias: -171.727393, T: 30250000, Avg. loss: 10.134507\n",
      "Total training time: 247.65 seconds.\n",
      "-- Epoch 122\n",
      "Norm: 21.69, NNZs: 3072, Bias: -140.014580, T: 30750000, Avg. loss: 8.923744\n",
      "Total training time: 248.38 seconds.\n",
      "-- Epoch 124\n",
      "Norm: 22.26, NNZs: 3072, Bias: -171.223960, T: 30500000, Avg. loss: 10.104333\n",
      "Total training time: 249.64 seconds.\n",
      "-- Epoch 123\n",
      "Norm: 21.63, NNZs: 3072, Bias: -139.482209, T: 31000000, Avg. loss: 8.889664\n",
      "Total training time: 250.36 seconds.\n",
      "-- Epoch 125\n",
      "Norm: 22.22, NNZs: 3072, Bias: -170.725399, T: 30750000, Avg. loss: 10.073842\n",
      "Total training time: 251.67 seconds.\n",
      "-- Epoch 124\n",
      "Norm: 21.58, NNZs: 3072, Bias: -138.951976, T: 31250000, Avg. loss: 8.854900\n",
      "Total training time: 252.35 seconds.\n",
      "-- Epoch 126\n",
      "Norm: 22.17, NNZs: 3072, Bias: -170.232344, T: 31000000, Avg. loss: 10.044103\n",
      "Total training time: 253.65 seconds.\n",
      "-- Epoch 125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 21.52, NNZs: 3072, Bias: -138.427934, T: 31500000, Avg. loss: 8.821280\n",
      "Total training time: 254.31 seconds.\n",
      "-- Epoch 127\n",
      "Norm: 22.14, NNZs: 3072, Bias: -169.739757, T: 31250000, Avg. loss: 10.015371\n",
      "Total training time: 255.60 seconds.\n",
      "-- Epoch 126\n",
      "Norm: 21.46, NNZs: 3072, Bias: -137.908366, T: 31750000, Avg. loss: 8.787205\n",
      "Total training time: 256.24 seconds.\n",
      "-- Epoch 128\n",
      "Norm: 22.09, NNZs: 3072, Bias: -169.253663, T: 31500000, Avg. loss: 9.985869\n",
      "Total training time: 257.60 seconds.\n",
      "-- Epoch 127\n",
      "Norm: 21.41, NNZs: 3072, Bias: -137.392207, T: 32000000, Avg. loss: 8.754508\n",
      "Total training time: 258.20 seconds.\n",
      "-- Epoch 129\n",
      "Norm: 22.06, NNZs: 3072, Bias: -168.770076, T: 31750000, Avg. loss: 9.956840\n",
      "Total training time: 259.64 seconds.\n",
      "-- Epoch 128\n",
      "Norm: 21.37, NNZs: 3072, Bias: -136.879058, T: 32250000, Avg. loss: 8.720914\n",
      "Total training time: 260.25 seconds.\n",
      "-- Epoch 130\n",
      "Norm: 22.01, NNZs: 3072, Bias: -168.291673, T: 32000000, Avg. loss: 9.927394\n",
      "Total training time: 261.69 seconds.\n",
      "-- Epoch 129\n",
      "Norm: 21.31, NNZs: 3072, Bias: -136.371609, T: 32500000, Avg. loss: 8.688231\n",
      "Total training time: 262.26 seconds.\n",
      "-- Epoch 131\n",
      "Norm: 21.97, NNZs: 3072, Bias: -167.816663, T: 32250000, Avg. loss: 9.899562\n",
      "Total training time: 263.71 seconds.\n",
      "-- Epoch 130\n",
      "Norm: 21.26, NNZs: 3072, Bias: -135.867358, T: 32750000, Avg. loss: 8.655877\n",
      "Total training time: 264.31 seconds.\n",
      "-- Epoch 132\n",
      "Norm: 21.94, NNZs: 3072, Bias: -167.344964, T: 32500000, Avg. loss: 9.871232\n",
      "Total training time: 265.73 seconds.\n",
      "-- Epoch 131\n",
      "Norm: 21.20, NNZs: 3072, Bias: -135.368010, T: 33000000, Avg. loss: 8.622761\n",
      "Total training time: 266.27 seconds.\n",
      "-- Epoch 133\n",
      "Norm: 21.90, NNZs: 3072, Bias: -166.876246, T: 32750000, Avg. loss: 9.843180\n",
      "Total training time: 267.72 seconds.\n",
      "-- Epoch 132\n",
      "Norm: 21.16, NNZs: 3072, Bias: -134.869718, T: 33250000, Avg. loss: 8.590459\n",
      "Total training time: 268.23 seconds.\n",
      "-- Epoch 134\n",
      "Norm: 21.87, NNZs: 3072, Bias: -166.411157, T: 33000000, Avg. loss: 9.814913\n",
      "Total training time: 269.69 seconds.\n",
      "-- Epoch 133\n",
      "Norm: 21.11, NNZs: 3072, Bias: -134.377916, T: 33500000, Avg. loss: 8.558612\n",
      "Total training time: 270.19 seconds.\n",
      "-- Epoch 135\n",
      "Norm: 21.83, NNZs: 3072, Bias: -165.950932, T: 33250000, Avg. loss: 9.787705\n",
      "Total training time: 271.66 seconds.\n",
      "-- Epoch 134\n",
      "Norm: 21.06, NNZs: 3072, Bias: -133.888676, T: 33750000, Avg. loss: 8.526891\n",
      "Total training time: 272.15 seconds.\n",
      "-- Epoch 136\n",
      "Norm: 21.79, NNZs: 3072, Bias: -165.494152, T: 33500000, Avg. loss: 9.759592\n",
      "Total training time: 273.63 seconds.\n",
      "-- Epoch 135\n",
      "Norm: 21.01, NNZs: 3072, Bias: -133.404255, T: 34000000, Avg. loss: 8.495222\n",
      "Total training time: 274.15 seconds.\n",
      "-- Epoch 137\n",
      "Norm: 21.75, NNZs: 3072, Bias: -165.039540, T: 33750000, Avg. loss: 9.732674\n",
      "Total training time: 275.57 seconds.\n",
      "-- Epoch 136\n",
      "Norm: 20.95, NNZs: 3072, Bias: -132.922858, T: 34250000, Avg. loss: 8.463777\n",
      "Total training time: 276.09 seconds.\n",
      "-- Epoch 138\n",
      "Norm: 21.71, NNZs: 3072, Bias: -164.590581, T: 34000000, Avg. loss: 9.705835\n",
      "Total training time: 277.57 seconds.\n",
      "-- Epoch 137\n",
      "Norm: 20.91, NNZs: 3072, Bias: -132.443916, T: 34500000, Avg. loss: 8.434168\n",
      "Total training time: 278.05 seconds.\n",
      "-- Epoch 139\n",
      "Norm: 21.68, NNZs: 3072, Bias: -164.142196, T: 34250000, Avg. loss: 9.678579\n",
      "Total training time: 279.55 seconds.\n",
      "-- Epoch 138\n",
      "Norm: 20.87, NNZs: 3072, Bias: -131.968742, T: 34750000, Avg. loss: 8.402097\n",
      "Total training time: 280.02 seconds.\n",
      "-- Epoch 140\n",
      "Norm: 21.64, NNZs: 3072, Bias: -163.698220, T: 34500000, Avg. loss: 9.651714\n",
      "Total training time: 281.51 seconds.\n",
      "-- Epoch 139\n",
      "Norm: 20.82, NNZs: 3072, Bias: -131.498031, T: 35000000, Avg. loss: 8.372680\n",
      "Total training time: 281.99 seconds.\n",
      "-- Epoch 141\n",
      "Norm: 21.60, NNZs: 3072, Bias: -163.258488, T: 34750000, Avg. loss: 9.625990\n",
      "Total training time: 283.50 seconds.\n",
      "-- Epoch 140\n",
      "Norm: 20.76, NNZs: 3072, Bias: -131.031862, T: 35250000, Avg. loss: 8.342123\n",
      "Total training time: 283.94 seconds.\n",
      "-- Epoch 142\n",
      "Norm: 21.56, NNZs: 3072, Bias: -162.821802, T: 35000000, Avg. loss: 9.599702\n",
      "Total training time: 285.51 seconds.\n",
      "-- Epoch 141\n",
      "Norm: 20.72, NNZs: 3072, Bias: -130.566221, T: 35500000, Avg. loss: 8.312435\n",
      "Total training time: 285.94 seconds.\n",
      "-- Epoch 143\n",
      "Norm: 21.52, NNZs: 3072, Bias: -162.387883, T: 35250000, Avg. loss: 9.573325\n",
      "Total training time: 287.57 seconds.\n",
      "-- Epoch 142\n",
      "Norm: 20.68, NNZs: 3072, Bias: -130.104739, T: 35750000, Avg. loss: 8.282097\n",
      "Total training time: 287.95 seconds.\n",
      "-- Epoch 144\n",
      "Norm: 21.50, NNZs: 3072, Bias: -161.954799, T: 35500000, Avg. loss: 9.547602\n",
      "Total training time: 289.57 seconds.\n",
      "-- Epoch 143\n",
      "Norm: 20.63, NNZs: 3072, Bias: -129.646662, T: 36000000, Avg. loss: 8.251920\n",
      "Total training time: 289.91 seconds.\n",
      "-- Epoch 145\n",
      "Norm: 21.46, NNZs: 3072, Bias: -161.527823, T: 35750000, Avg. loss: 9.521556\n",
      "Total training time: 291.54 seconds.\n",
      "-- Epoch 144\n",
      "Norm: 20.58, NNZs: 3072, Bias: -129.193603, T: 36250000, Avg. loss: 8.223096\n",
      "Total training time: 291.91 seconds.\n",
      "-- Epoch 146\n",
      "Norm: 21.43, NNZs: 3072, Bias: -161.102221, T: 36000000, Avg. loss: 9.496470\n",
      "Total training time: 293.58 seconds.\n",
      "-- Epoch 145\n",
      "Norm: 20.54, NNZs: 3072, Bias: -128.741300, T: 36500000, Avg. loss: 8.193782\n",
      "Total training time: 293.92 seconds.\n",
      "-- Epoch 147\n",
      "Norm: 21.39, NNZs: 3072, Bias: -160.680429, T: 36250000, Avg. loss: 9.470873\n",
      "Total training time: 295.59 seconds.\n",
      "-- Epoch 146\n",
      "Norm: 20.48, NNZs: 3072, Bias: -128.294010, T: 36750000, Avg. loss: 8.165305\n",
      "Total training time: 295.90 seconds.\n",
      "-- Epoch 148\n",
      "Norm: 21.36, NNZs: 3072, Bias: -160.261017, T: 36500000, Avg. loss: 9.445871\n",
      "Total training time: 297.62 seconds.\n",
      "-- Epoch 147\n",
      "Norm: 20.43, NNZs: 3072, Bias: -127.849581, T: 37000000, Avg. loss: 8.136287\n",
      "Total training time: 297.95 seconds.\n",
      "-- Epoch 149\n",
      "Norm: 21.33, NNZs: 3072, Bias: -159.843263, T: 36750000, Avg. loss: 9.420488\n",
      "Total training time: 299.60 seconds.\n",
      "-- Epoch 148\n",
      "Norm: 20.39, NNZs: 3072, Bias: -127.407296, T: 37250000, Avg. loss: 8.107893\n",
      "Total training time: 299.95 seconds.\n",
      "-- Epoch 150\n",
      "Norm: 21.29, NNZs: 3072, Bias: -159.431238, T: 37000000, Avg. loss: 9.395180\n",
      "Total training time: 301.64 seconds.\n",
      "-- Epoch 149\n",
      "Norm: 20.35, NNZs: 3072, Bias: -126.967895, T: 37500000, Avg. loss: 8.079662\n",
      "Total training time: 301.96 seconds.\n",
      "-- Epoch 151\n",
      "Norm: 21.26, NNZs: 3072, Bias: -159.020140, T: 37250000, Avg. loss: 9.371655\n",
      "Total training time: 303.68 seconds.\n",
      "-- Epoch 150\n",
      "Norm: 20.30, NNZs: 3072, Bias: -126.531773, T: 37750000, Avg. loss: 8.051444\n",
      "Total training time: 303.97 seconds.\n",
      "-- Epoch 152\n",
      "Norm: 21.22, NNZs: 3072, Bias: -158.613072, T: 37500000, Avg. loss: 9.347236\n",
      "Total training time: 305.67 seconds.\n",
      "-- Epoch 151\n",
      "Norm: 20.25, NNZs: 3072, Bias: -126.099638, T: 38000000, Avg. loss: 8.023127\n",
      "Total training time: 305.92 seconds.\n",
      "-- Epoch 153\n",
      "Norm: 21.19, NNZs: 3072, Bias: -158.207912, T: 37750000, Avg. loss: 9.322131\n",
      "Total training time: 307.65 seconds.\n",
      "-- Epoch 152\n",
      "Norm: 20.21, NNZs: 3072, Bias: -125.668764, T: 38250000, Avg. loss: 7.995562\n",
      "Total training time: 307.90 seconds.\n",
      "-- Epoch 154\n",
      "Norm: 21.14, NNZs: 3072, Bias: -157.807770, T: 38000000, Avg. loss: 9.298696\n",
      "Total training time: 309.62 seconds.\n",
      "-- Epoch 153\n",
      "Norm: 20.17, NNZs: 3072, Bias: -125.241835, T: 38500000, Avg. loss: 7.967743\n",
      "Total training time: 309.88 seconds.\n",
      "-- Epoch 155\n",
      "Norm: 21.12, NNZs: 3072, Bias: -157.406352, T: 38250000, Avg. loss: 9.274782\n",
      "Total training time: 311.64 seconds.\n",
      "-- Epoch 154\n",
      "Norm: 20.12, NNZs: 3072, Bias: -124.817082, T: 38750000, Avg. loss: 7.940002\n",
      "Total training time: 311.88 seconds.\n",
      "-- Epoch 156\n",
      "Norm: 21.07, NNZs: 3072, Bias: -157.010452, T: 38500000, Avg. loss: 9.250882\n",
      "Total training time: 313.61 seconds.\n",
      "-- Epoch 155\n",
      "Norm: 20.08, NNZs: 3072, Bias: -124.395098, T: 39000000, Avg. loss: 7.913001\n",
      "Total training time: 313.84 seconds.\n",
      "-- Epoch 157\n",
      "Norm: 21.03, NNZs: 3072, Bias: -156.616998, T: 38750000, Avg. loss: 9.227010\n",
      "Total training time: 315.57 seconds.\n",
      "-- Epoch 156\n",
      "Norm: 20.04, NNZs: 3072, Bias: -123.976103, T: 39250000, Avg. loss: 7.886085\n",
      "Total training time: 315.74 seconds.\n",
      "-- Epoch 158\n",
      "Norm: 21.02, NNZs: 3072, Bias: -156.223324, T: 39000000, Avg. loss: 9.204079\n",
      "Total training time: 317.51 seconds.\n",
      "-- Epoch 157\n",
      "Norm: 19.99, NNZs: 3072, Bias: -123.561117, T: 39500000, Avg. loss: 7.858866\n",
      "Total training time: 317.68 seconds.\n",
      "-- Epoch 159\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 20.97, NNZs: 3072, Bias: -155.836185, T: 39250000, Avg. loss: 9.179733\n",
      "Total training time: 319.47 seconds.\n",
      "-- Epoch 158\n",
      "Norm: 19.95, NNZs: 3072, Bias: -123.146567, T: 39750000, Avg. loss: 7.832482\n",
      "Total training time: 319.64 seconds.\n",
      "-- Epoch 160\n",
      "Norm: 20.94, NNZs: 3072, Bias: -155.448434, T: 39500000, Avg. loss: 9.157327\n",
      "Total training time: 321.55 seconds.\n",
      "-- Epoch 159\n",
      "Norm: 19.90, NNZs: 3072, Bias: -122.736878, T: 40000000, Avg. loss: 7.806032\n",
      "Total training time: 321.73 seconds.\n",
      "-- Epoch 161\n",
      "Norm: 20.91, NNZs: 3072, Bias: -155.064610, T: 39750000, Avg. loss: 9.133805\n",
      "Total training time: 323.60 seconds.\n",
      "-- Epoch 160\n",
      "Norm: 19.86, NNZs: 3072, Bias: -122.328797, T: 40250000, Avg. loss: 7.779494\n",
      "Total training time: 323.82 seconds.\n",
      "-- Epoch 162\n",
      "Norm: 20.88, NNZs: 3072, Bias: -154.682201, T: 40000000, Avg. loss: 9.111251\n",
      "Total training time: 325.78 seconds.\n",
      "-- Epoch 161\n",
      "Norm: 19.82, NNZs: 3072, Bias: -121.922868, T: 40500000, Avg. loss: 7.753394\n",
      "Total training time: 325.94 seconds.\n",
      "-- Epoch 163\n",
      "Norm: 20.84, NNZs: 3072, Bias: -154.303527, T: 40250000, Avg. loss: 9.088638\n",
      "Total training time: 327.77 seconds.\n",
      "-- Epoch 162\n",
      "Norm: 19.78, NNZs: 3072, Bias: -121.519236, T: 40750000, Avg. loss: 7.727501\n",
      "Total training time: 327.89 seconds.\n",
      "-- Epoch 164\n",
      "Norm: 20.81, NNZs: 3072, Bias: -153.925662, T: 40500000, Avg. loss: 9.066150\n",
      "Total training time: 329.74 seconds.\n",
      "-- Epoch 163\n",
      "Norm: 19.74, NNZs: 3072, Bias: -121.117925, T: 41000000, Avg. loss: 7.701080\n",
      "Total training time: 329.86 seconds.\n",
      "-- Epoch 165\n",
      "Norm: 20.78, NNZs: 3072, Bias: -153.550877, T: 40750000, Avg. loss: 9.043427\n",
      "Total training time: 331.72 seconds.\n",
      "-- Epoch 164\n",
      "Norm: 19.70, NNZs: 3072, Bias: -120.719623, T: 41250000, Avg. loss: 7.675473\n",
      "Total training time: 331.85 seconds.\n",
      "-- Epoch 166\n",
      "Norm: 20.75, NNZs: 3072, Bias: -153.178689, T: 41000000, Avg. loss: 9.021294\n",
      "Total training time: 333.75 seconds.\n",
      "-- Epoch 165\n",
      "Norm: 19.65, NNZs: 3072, Bias: -120.325559, T: 41500000, Avg. loss: 7.649843\n",
      "Total training time: 333.91 seconds.\n",
      "-- Epoch 167\n",
      "Norm: 20.71, NNZs: 3072, Bias: -152.809545, T: 41250000, Avg. loss: 8.999052\n",
      "Total training time: 335.73 seconds.\n",
      "-- Epoch 166\n",
      "Norm: 19.60, NNZs: 3072, Bias: -119.932740, T: 41750000, Avg. loss: 7.624603\n",
      "Total training time: 335.88 seconds.\n",
      "-- Epoch 168\n",
      "Norm: 20.69, NNZs: 3072, Bias: -152.441055, T: 41500000, Avg. loss: 8.976785\n",
      "Total training time: 337.71 seconds.\n",
      "-- Epoch 167\n",
      "Norm: 19.57, NNZs: 3072, Bias: -119.541077, T: 42000000, Avg. loss: 7.599531\n",
      "Total training time: 337.82 seconds.\n",
      "-- Epoch 169\n",
      "Norm: 20.65, NNZs: 3072, Bias: -152.075919, T: 41750000, Avg. loss: 8.955380\n",
      "Total training time: 339.68 seconds.\n",
      "-- Epoch 168\n",
      "Norm: 19.53, NNZs: 3072, Bias: -119.152429, T: 42250000, Avg. loss: 7.574711\n",
      "Total training time: 339.77 seconds.\n",
      "-- Epoch 170\n",
      "Norm: 20.62, NNZs: 3072, Bias: -151.712508, T: 42000000, Avg. loss: 8.933001\n",
      "Total training time: 341.69 seconds.\n",
      "-- Epoch 169\n",
      "Norm: 19.48, NNZs: 3072, Bias: -118.766417, T: 42500000, Avg. loss: 7.549403\n",
      "Total training time: 341.78 seconds.\n",
      "-- Epoch 171\n",
      "Norm: 20.59, NNZs: 3072, Bias: -151.351281, T: 42250000, Avg. loss: 8.911946\n",
      "Total training time: 343.70 seconds.\n",
      "-- Epoch 170\n",
      "Norm: 19.44, NNZs: 3072, Bias: -118.382850, T: 42750000, Avg. loss: 7.524982\n",
      "Total training time: 343.78 seconds.\n",
      "-- Epoch 172\n",
      "Norm: 20.56, NNZs: 3072, Bias: -150.991999, T: 42500000, Avg. loss: 8.890359\n",
      "Total training time: 345.66 seconds.\n",
      "-- Epoch 171\n",
      "Norm: 19.40, NNZs: 3072, Bias: -118.001340, T: 43000000, Avg. loss: 7.500085\n",
      "Total training time: 345.73 seconds.\n",
      "-- Epoch 173\n",
      "Norm: 20.54, NNZs: 3072, Bias: -150.634445, T: 42750000, Avg. loss: 8.868885\n",
      "Total training time: 347.65 seconds.\n",
      "-- Epoch 172\n",
      "Norm: 19.37, NNZs: 3072, Bias: -117.621034, T: 43250000, Avg. loss: 7.475181\n",
      "Total training time: 347.72 seconds.\n",
      "-- Epoch 174\n",
      "Norm: 20.50, NNZs: 3072, Bias: -150.280443, T: 43000000, Avg. loss: 8.847550\n",
      "Total training time: 349.58 seconds.\n",
      "-- Epoch 173\n",
      "Norm: 19.33, NNZs: 3072, Bias: -117.243804, T: 43500000, Avg. loss: 7.451104\n",
      "Total training time: 349.68 seconds.\n",
      "-- Epoch 175\n",
      "Norm: 20.47, NNZs: 3072, Bias: -149.927631, T: 43250000, Avg. loss: 8.826439\n",
      "Total training time: 351.53 seconds.\n",
      "-- Epoch 174\n",
      "Norm: 19.28, NNZs: 3072, Bias: -116.870452, T: 43750000, Avg. loss: 7.427051\n",
      "Total training time: 351.62 seconds.\n",
      "-- Epoch 176\n",
      "Norm: 20.45, NNZs: 3072, Bias: -149.576557, T: 43500000, Avg. loss: 8.805214\n",
      "Total training time: 353.51 seconds.\n",
      "-- Epoch 175\n",
      "Norm: 19.24, NNZs: 3072, Bias: -116.497484, T: 44000000, Avg. loss: 7.402912\n",
      "Total training time: 353.61 seconds.\n",
      "-- Epoch 177\n",
      "Norm: 20.42, NNZs: 3072, Bias: -149.228036, T: 43750000, Avg. loss: 8.784905\n",
      "Total training time: 355.59 seconds.\n",
      "-- Epoch 176\n",
      "Norm: 19.21, NNZs: 3072, Bias: -116.125821, T: 44250000, Avg. loss: 7.378613\n",
      "Total training time: 355.67 seconds.\n",
      "-- Epoch 178\n",
      "Norm: 20.38, NNZs: 3072, Bias: -148.882600, T: 44000000, Avg. loss: 8.763802\n",
      "Total training time: 357.57 seconds.\n",
      "-- Epoch 177\n",
      "Norm: 19.17, NNZs: 3072, Bias: -115.758199, T: 44500000, Avg. loss: 7.355061\n",
      "Total training time: 357.66 seconds.\n",
      "-- Epoch 179\n",
      "Norm: 20.34, NNZs: 3072, Bias: -148.538725, T: 44250000, Avg. loss: 8.743338\n",
      "Total training time: 359.63 seconds.\n",
      "-- Epoch 178\n",
      "Norm: 19.13, NNZs: 3072, Bias: -115.391516, T: 44750000, Avg. loss: 7.331292\n",
      "Total training time: 359.70 seconds.\n",
      "-- Epoch 180\n",
      "Norm: 20.33, NNZs: 3072, Bias: -148.194056, T: 44500000, Avg. loss: 8.722724\n",
      "Total training time: 361.61 seconds.\n",
      "-- Epoch 179\n",
      "Norm: 19.10, NNZs: 3072, Bias: -115.026726, T: 45000000, Avg. loss: 7.307929\n",
      "Total training time: 361.68 seconds.\n",
      "-- Epoch 181\n",
      "Norm: 20.28, NNZs: 3072, Bias: -147.855463, T: 44750000, Avg. loss: 8.702137\n",
      "Total training time: 363.57 seconds.\n",
      "-- Epoch 180\n",
      "Norm: 19.05, NNZs: 3072, Bias: -114.665930, T: 45250000, Avg. loss: 7.284960\n",
      "Total training time: 363.63 seconds.\n",
      "-- Epoch 182\n",
      "Norm: 20.27, NNZs: 3072, Bias: -147.515365, T: 45000000, Avg. loss: 8.682235\n",
      "Total training time: 365.54 seconds.\n",
      "-- Epoch 181\n",
      "Norm: 19.01, NNZs: 3072, Bias: -114.305595, T: 45500000, Avg. loss: 7.261569\n",
      "Total training time: 365.61 seconds.\n",
      "-- Epoch 183\n",
      "Norm: 20.24, NNZs: 3072, Bias: -147.177917, T: 45250000, Avg. loss: 8.661959\n",
      "Total training time: 367.52 seconds.\n",
      "-- Epoch 182\n",
      "Norm: 18.98, NNZs: 3072, Bias: -113.946708, T: 45750000, Avg. loss: 7.238944\n",
      "Total training time: 367.60 seconds.\n",
      "-- Epoch 184\n",
      "Norm: 20.20, NNZs: 3072, Bias: -146.844552, T: 45500000, Avg. loss: 8.641988\n",
      "Total training time: 369.51 seconds.\n",
      "-- Epoch 183\n",
      "Norm: 18.94, NNZs: 3072, Bias: -113.590243, T: 46000000, Avg. loss: 7.214816\n",
      "Total training time: 369.57 seconds.\n",
      "-- Epoch 185\n",
      "Norm: 20.18, NNZs: 3072, Bias: -146.510720, T: 45750000, Avg. loss: 8.622203\n",
      "Total training time: 371.45 seconds.\n",
      "-- Epoch 184\n",
      "Norm: 18.90, NNZs: 3072, Bias: -113.237382, T: 46250000, Avg. loss: 7.192882\n",
      "Total training time: 371.52 seconds.\n",
      "-- Epoch 186\n",
      "Norm: 20.14, NNZs: 3072, Bias: -146.179998, T: 46000000, Avg. loss: 8.602414\n",
      "Total training time: 373.39 seconds.\n",
      "-- Epoch 185\n",
      "Norm: 18.86, NNZs: 3072, Bias: -112.885702, T: 46500000, Avg. loss: 7.170130\n",
      "Total training time: 373.46 seconds.\n",
      "-- Epoch 187\n",
      "Norm: 20.11, NNZs: 3072, Bias: -145.851199, T: 46250000, Avg. loss: 8.582415\n",
      "Total training time: 375.37 seconds.\n",
      "-- Epoch 186\n",
      "Norm: 18.82, NNZs: 3072, Bias: -112.534769, T: 46750000, Avg. loss: 7.146116\n",
      "Total training time: 375.40 seconds.\n",
      "-- Epoch 188\n",
      "Norm: 20.09, NNZs: 3072, Bias: -145.522395, T: 46500000, Avg. loss: 8.563094\n",
      "Total training time: 377.35 seconds.\n",
      "-- Epoch 187\n",
      "Norm: 18.79, NNZs: 3072, Bias: -112.185570, T: 47000000, Avg. loss: 7.124924\n",
      "Total training time: 377.38 seconds.\n",
      "-- Epoch 189\n",
      "Norm: 20.06, NNZs: 3072, Bias: -145.197275, T: 46750000, Avg. loss: 8.542829\n",
      "Total training time: 379.36 seconds.\n",
      "-- Epoch 188\n",
      "Norm: 18.75, NNZs: 3072, Bias: -111.840362, T: 47250000, Avg. loss: 7.102520\n",
      "Total training time: 379.38 seconds.\n",
      "-- Epoch 190\n",
      "Norm: 20.04, NNZs: 3072, Bias: -144.872320, T: 47000000, Avg. loss: 8.524290\n",
      "Total training time: 381.34 seconds.\n",
      "-- Epoch 189\n",
      "Norm: 18.71, NNZs: 3072, Bias: -111.495404, T: 47500000, Avg. loss: 7.080183\n",
      "Total training time: 381.36 seconds.\n",
      "-- Epoch 191\n",
      "Norm: 20.01, NNZs: 3072, Bias: -144.549495, T: 47250000, Avg. loss: 8.504747\n",
      "Total training time: 383.30 seconds.\n",
      "-- Epoch 190\n",
      "Norm: 18.67, NNZs: 3072, Bias: -111.153987, T: 47750000, Avg. loss: 7.058275\n",
      "Total training time: 383.35 seconds.\n",
      "-- Epoch 192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 19.98, NNZs: 3072, Bias: -144.229625, T: 47500000, Avg. loss: 8.485773\n",
      "Total training time: 385.28 seconds.\n",
      "-- Epoch 191\n",
      "Norm: 18.64, NNZs: 3072, Bias: -110.811705, T: 48000000, Avg. loss: 7.036258\n",
      "Total training time: 385.32 seconds.\n",
      "-- Epoch 193\n",
      "Norm: 19.95, NNZs: 3072, Bias: -143.911374, T: 47750000, Avg. loss: 8.466621\n",
      "Total training time: 387.25 seconds.\n",
      "-- Epoch 192\n",
      "Norm: 18.60, NNZs: 3072, Bias: -110.474151, T: 48250000, Avg. loss: 7.014400\n",
      "Total training time: 387.31 seconds.\n",
      "-- Epoch 194\n",
      "Norm: 19.92, NNZs: 3072, Bias: -143.594082, T: 48000000, Avg. loss: 8.447329\n",
      "Total training time: 389.16 seconds.\n",
      "-- Epoch 193\n",
      "Norm: 18.57, NNZs: 3072, Bias: -110.135815, T: 48500000, Avg. loss: 6.992822\n",
      "Total training time: 389.25 seconds.\n",
      "-- Epoch 195\n",
      "Norm: 19.89, NNZs: 3072, Bias: -143.278513, T: 48250000, Avg. loss: 8.428638\n",
      "Total training time: 391.10 seconds.\n",
      "-- Epoch 194\n",
      "Norm: 18.53, NNZs: 3072, Bias: -109.800518, T: 48750000, Avg. loss: 6.970836\n",
      "Total training time: 391.21 seconds.\n",
      "-- Epoch 196\n",
      "Norm: 19.87, NNZs: 3072, Bias: -142.964669, T: 48500000, Avg. loss: 8.409561\n",
      "Total training time: 393.11 seconds.\n",
      "-- Epoch 195\n",
      "Norm: 18.49, NNZs: 3072, Bias: -109.467032, T: 49000000, Avg. loss: 6.949686\n",
      "Total training time: 393.20 seconds.\n",
      "-- Epoch 197\n",
      "Norm: 19.84, NNZs: 3072, Bias: -142.652892, T: 48750000, Avg. loss: 8.391482\n",
      "Total training time: 395.10 seconds.\n",
      "-- Epoch 196\n",
      "Norm: 18.46, NNZs: 3072, Bias: -109.134160, T: 49250000, Avg. loss: 6.928328\n",
      "Total training time: 395.18 seconds.\n",
      "-- Epoch 198\n",
      "Norm: 19.80, NNZs: 3072, Bias: -142.343052, T: 49000000, Avg. loss: 8.372420\n",
      "Total training time: 397.08 seconds.\n",
      "-- Epoch 197\n",
      "Norm: 18.42, NNZs: 3072, Bias: -108.804066, T: 49500000, Avg. loss: 6.906983\n",
      "Total training time: 397.13 seconds.\n",
      "-- Epoch 199\n",
      "Norm: 19.78, NNZs: 3072, Bias: -142.033741, T: 49250000, Avg. loss: 8.354279\n",
      "Total training time: 399.05 seconds.\n",
      "-- Epoch 198\n",
      "Norm: 18.39, NNZs: 3072, Bias: -108.475433, T: 49750000, Avg. loss: 6.885802\n",
      "Total training time: 399.08 seconds.\n",
      "-- Epoch 200\n",
      "Norm: 19.76, NNZs: 3072, Bias: -141.725162, T: 49500000, Avg. loss: 8.335979\n",
      "Total training time: 401.00 seconds.\n",
      "-- Epoch 199\n",
      "Norm: 18.35, NNZs: 3072, Bias: -108.149000, T: 50000000, Avg. loss: 6.864854\n",
      "Total training time: 401.06 seconds.\n",
      "-- Epoch 201\n",
      "Norm: 18.32, NNZs: 3072, Bias: -107.823324, T: 50250000, Avg. loss: 6.844014\n",
      "Total training time: 402.97 seconds.\n",
      "-- Epoch 202\n",
      "Norm: 19.73, NNZs: 3072, Bias: -141.420299, T: 49750000, Avg. loss: 8.317576\n",
      "Total training time: 402.97 seconds.\n",
      "-- Epoch 200\n",
      "Norm: 18.28, NNZs: 3072, Bias: -107.500359, T: 50500000, Avg. loss: 6.823134\n",
      "Total training time: 404.95 seconds.\n",
      "-- Epoch 203\n",
      "Norm: 19.70, NNZs: 3072, Bias: -141.115689, T: 50000000, Avg. loss: 8.299369\n",
      "Total training time: 404.97 seconds.\n",
      "-- Epoch 201\n",
      "Norm: 18.25, NNZs: 3072, Bias: -107.178165, T: 50750000, Avg. loss: 6.802401\n",
      "Total training time: 406.93 seconds.\n",
      "-- Epoch 204\n",
      "Norm: 19.67, NNZs: 3072, Bias: -140.813379, T: 50250000, Avg. loss: 8.281002\n",
      "Total training time: 406.96 seconds.\n",
      "-- Epoch 202\n",
      "Norm: 18.21, NNZs: 3072, Bias: -106.857372, T: 51000000, Avg. loss: 6.781815\n",
      "Total training time: 408.93 seconds.\n",
      "-- Epoch 205\n",
      "Norm: 19.65, NNZs: 3072, Bias: -140.511514, T: 50500000, Avg. loss: 8.263264\n",
      "Total training time: 408.97 seconds.\n",
      "-- Epoch 203\n",
      "Norm: 18.17, NNZs: 3072, Bias: -106.539465, T: 51250000, Avg. loss: 6.761238\n",
      "Total training time: 410.90 seconds.\n",
      "-- Epoch 206\n",
      "Norm: 19.62, NNZs: 3072, Bias: -140.212127, T: 50750000, Avg. loss: 8.245504\n",
      "Total training time: 410.93 seconds.\n",
      "-- Epoch 204\n",
      "Norm: 18.14, NNZs: 3072, Bias: -106.221648, T: 51500000, Avg. loss: 6.740793\n",
      "Total training time: 412.90 seconds.\n",
      "-- Epoch 207\n",
      "Norm: 19.60, NNZs: 3072, Bias: -139.913162, T: 51000000, Avg. loss: 8.227428\n",
      "Total training time: 412.95 seconds.\n",
      "-- Epoch 205\n",
      "Norm: 18.11, NNZs: 3072, Bias: -105.906634, T: 51750000, Avg. loss: 6.720442\n",
      "Total training time: 414.89 seconds.\n",
      "-- Epoch 208\n",
      "Norm: 19.57, NNZs: 3072, Bias: -139.616731, T: 51250000, Avg. loss: 8.209732\n",
      "Total training time: 414.94 seconds.\n",
      "-- Epoch 206\n",
      "Norm: 18.08, NNZs: 3072, Bias: -105.591634, T: 52000000, Avg. loss: 6.700090\n",
      "Total training time: 416.87 seconds.\n",
      "-- Epoch 209\n",
      "Norm: 19.54, NNZs: 3072, Bias: -139.322150, T: 51500000, Avg. loss: 8.192297\n",
      "Total training time: 416.93 seconds.\n",
      "-- Epoch 207\n",
      "Norm: 18.04, NNZs: 3072, Bias: -105.278967, T: 52250000, Avg. loss: 6.680458\n",
      "Total training time: 418.84 seconds.\n",
      "-- Epoch 210\n",
      "Norm: 19.52, NNZs: 3072, Bias: -139.027514, T: 51750000, Avg. loss: 8.174833\n",
      "Total training time: 418.91 seconds.\n",
      "-- Epoch 208\n",
      "Norm: 18.00, NNZs: 3072, Bias: -104.969196, T: 52500000, Avg. loss: 6.659892\n",
      "Total training time: 420.85 seconds.\n",
      "-- Epoch 211\n",
      "Norm: 19.49, NNZs: 3072, Bias: -138.735461, T: 52000000, Avg. loss: 8.157480\n",
      "Total training time: 420.92 seconds.\n",
      "-- Epoch 209\n",
      "Norm: 17.97, NNZs: 3072, Bias: -104.660400, T: 52750000, Avg. loss: 6.640065\n",
      "Total training time: 422.81 seconds.\n",
      "-- Epoch 212\n",
      "Norm: 19.47, NNZs: 3072, Bias: -138.444455, T: 52250000, Avg. loss: 8.140052\n",
      "Total training time: 422.90 seconds.\n",
      "-- Epoch 210\n",
      "Norm: 17.94, NNZs: 3072, Bias: -104.350696, T: 53000000, Avg. loss: 6.620272\n",
      "Total training time: 424.77 seconds.\n",
      "-- Epoch 213\n",
      "Norm: 19.44, NNZs: 3072, Bias: -138.155296, T: 52500000, Avg. loss: 8.122290\n",
      "Total training time: 424.88 seconds.\n",
      "-- Epoch 211\n",
      "Norm: 17.90, NNZs: 3072, Bias: -104.045024, T: 53250000, Avg. loss: 6.600778\n",
      "Total training time: 426.76 seconds.\n",
      "-- Epoch 214\n",
      "Norm: 19.42, NNZs: 3072, Bias: -137.866455, T: 52750000, Avg. loss: 8.105526\n",
      "Total training time: 426.89 seconds.\n",
      "-- Epoch 212\n",
      "Norm: 17.87, NNZs: 3072, Bias: -103.740016, T: 53500000, Avg. loss: 6.580793\n",
      "Total training time: 428.77 seconds.\n",
      "-- Epoch 215\n",
      "Norm: 19.39, NNZs: 3072, Bias: -137.579734, T: 53000000, Avg. loss: 8.088259\n",
      "Total training time: 428.91 seconds.\n",
      "-- Epoch 213\n",
      "Norm: 17.84, NNZs: 3072, Bias: -103.436296, T: 53750000, Avg. loss: 6.561237\n",
      "Total training time: 430.74 seconds.\n",
      "-- Epoch 216\n",
      "Norm: 19.37, NNZs: 3072, Bias: -137.293474, T: 53250000, Avg. loss: 8.071013\n",
      "Total training time: 430.89 seconds.\n",
      "-- Epoch 214\n",
      "Norm: 17.81, NNZs: 3072, Bias: -103.133702, T: 54000000, Avg. loss: 6.541818\n",
      "Total training time: 432.67 seconds.\n",
      "-- Epoch 217\n",
      "Norm: 19.34, NNZs: 3072, Bias: -137.010952, T: 53500000, Avg. loss: 8.054404\n",
      "Total training time: 432.81 seconds.\n",
      "-- Epoch 215\n",
      "Norm: 17.78, NNZs: 3072, Bias: -102.833349, T: 54250000, Avg. loss: 6.522861\n",
      "Total training time: 434.61 seconds.\n",
      "-- Epoch 218\n",
      "Norm: 19.32, NNZs: 3072, Bias: -136.727033, T: 53750000, Avg. loss: 8.037454\n",
      "Total training time: 434.79 seconds.\n",
      "-- Epoch 216\n",
      "Norm: 17.74, NNZs: 3072, Bias: -102.534163, T: 54500000, Avg. loss: 6.503334\n",
      "Total training time: 436.53 seconds.\n",
      "-- Epoch 219\n",
      "Norm: 19.29, NNZs: 3072, Bias: -136.447229, T: 54000000, Avg. loss: 8.020506\n",
      "Total training time: 436.72 seconds.\n",
      "-- Epoch 217\n",
      "Norm: 17.71, NNZs: 3072, Bias: -102.236428, T: 54750000, Avg. loss: 6.484192\n",
      "Total training time: 438.46 seconds.\n",
      "-- Epoch 220\n",
      "Norm: 19.26, NNZs: 3072, Bias: -136.166969, T: 54250000, Avg. loss: 8.004021\n",
      "Total training time: 438.67 seconds.\n",
      "-- Epoch 218\n",
      "Norm: 17.68, NNZs: 3072, Bias: -101.939372, T: 55000000, Avg. loss: 6.465439\n",
      "Total training time: 440.46 seconds.\n",
      "-- Epoch 221\n",
      "Norm: 19.24, NNZs: 3072, Bias: -135.887623, T: 54500000, Avg. loss: 7.987356\n",
      "Total training time: 440.68 seconds.\n",
      "-- Epoch 219\n",
      "Norm: 17.64, NNZs: 3072, Bias: -101.644728, T: 55250000, Avg. loss: 6.445855\n",
      "Total training time: 442.41 seconds.\n",
      "-- Epoch 222\n",
      "Norm: 19.22, NNZs: 3072, Bias: -135.609835, T: 54750000, Avg. loss: 7.970916\n",
      "Total training time: 442.66 seconds.\n",
      "-- Epoch 220\n",
      "Norm: 17.61, NNZs: 3072, Bias: -101.350712, T: 55500000, Avg. loss: 6.427326\n",
      "Total training time: 444.40 seconds.\n",
      "-- Epoch 223\n",
      "Norm: 19.19, NNZs: 3072, Bias: -135.334379, T: 55000000, Avg. loss: 7.953842\n",
      "Total training time: 444.62 seconds.\n",
      "-- Epoch 221\n",
      "Norm: 17.58, NNZs: 3072, Bias: -101.058561, T: 55750000, Avg. loss: 6.408499\n",
      "Total training time: 446.37 seconds.\n",
      "-- Epoch 224\n",
      "Norm: 19.17, NNZs: 3072, Bias: -135.058984, T: 55250000, Avg. loss: 7.937876\n",
      "Total training time: 446.56 seconds.\n",
      "-- Epoch 222\n",
      "Norm: 17.55, NNZs: 3072, Bias: -100.767336, T: 56000000, Avg. loss: 6.389717\n",
      "Total training time: 448.32 seconds.\n",
      "-- Epoch 225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 19.14, NNZs: 3072, Bias: -134.786434, T: 55500000, Avg. loss: 7.921223\n",
      "Total training time: 448.52 seconds.\n",
      "-- Epoch 223\n",
      "Norm: 17.52, NNZs: 3072, Bias: -100.477658, T: 56250000, Avg. loss: 6.371024\n",
      "Total training time: 450.26 seconds.\n",
      "-- Epoch 226\n",
      "Norm: 19.12, NNZs: 3072, Bias: -134.513626, T: 55750000, Avg. loss: 7.905293\n",
      "Total training time: 450.47 seconds.\n",
      "-- Epoch 224\n",
      "Norm: 17.49, NNZs: 3072, Bias: -100.188888, T: 56500000, Avg. loss: 6.352596\n",
      "Total training time: 452.22 seconds.\n",
      "-- Epoch 227\n",
      "Norm: 19.09, NNZs: 3072, Bias: -134.242371, T: 56000000, Avg. loss: 7.888846\n",
      "Total training time: 452.41 seconds.\n",
      "-- Epoch 225\n",
      "Norm: 17.46, NNZs: 3072, Bias: -99.902067, T: 56750000, Avg. loss: 6.334008\n",
      "Total training time: 454.21 seconds.\n",
      "-- Epoch 228\n",
      "Norm: 19.07, NNZs: 3072, Bias: -133.972409, T: 56250000, Avg. loss: 7.872701\n",
      "Total training time: 454.40 seconds.\n",
      "-- Epoch 226\n",
      "Norm: 17.42, NNZs: 3072, Bias: -99.616765, T: 57000000, Avg. loss: 6.316025\n",
      "Total training time: 456.17 seconds.\n",
      "-- Epoch 229\n",
      "Norm: 19.05, NNZs: 3072, Bias: -133.703354, T: 56500000, Avg. loss: 7.857055\n",
      "Total training time: 456.40 seconds.\n",
      "-- Epoch 227\n",
      "Norm: 17.39, NNZs: 3072, Bias: -99.331980, T: 57250000, Avg. loss: 6.296914\n",
      "Total training time: 458.15 seconds.\n",
      "-- Epoch 230\n",
      "Norm: 19.02, NNZs: 3072, Bias: -133.436597, T: 56750000, Avg. loss: 7.841116\n",
      "Total training time: 458.39 seconds.\n",
      "-- Epoch 228\n",
      "Norm: 17.36, NNZs: 3072, Bias: -99.048566, T: 57500000, Avg. loss: 6.279475\n",
      "Total training time: 460.13 seconds.\n",
      "-- Epoch 231\n",
      "Norm: 18.99, NNZs: 3072, Bias: -133.170363, T: 57000000, Avg. loss: 7.825237\n",
      "Total training time: 460.35 seconds.\n",
      "-- Epoch 229\n",
      "Norm: 17.33, NNZs: 3072, Bias: -98.766573, T: 57750000, Avg. loss: 6.261075\n",
      "Total training time: 462.07 seconds.\n",
      "-- Epoch 232\n",
      "Norm: 18.97, NNZs: 3072, Bias: -132.905388, T: 57250000, Avg. loss: 7.809211\n",
      "Total training time: 462.33 seconds.\n",
      "-- Epoch 230\n",
      "Norm: 17.29, NNZs: 3072, Bias: -98.486585, T: 58000000, Avg. loss: 6.243107\n",
      "Total training time: 464.14 seconds.\n",
      "-- Epoch 233\n",
      "Norm: 18.94, NNZs: 3072, Bias: -132.642072, T: 57500000, Avg. loss: 7.793387\n",
      "Total training time: 464.40 seconds.\n",
      "-- Epoch 231\n",
      "Norm: 17.27, NNZs: 3072, Bias: -98.206868, T: 58250000, Avg. loss: 6.224986\n",
      "Total training time: 466.17 seconds.\n",
      "-- Epoch 234\n",
      "Norm: 18.92, NNZs: 3072, Bias: -132.378667, T: 57750000, Avg. loss: 7.777857\n",
      "Total training time: 466.46 seconds.\n",
      "-- Epoch 232\n",
      "Norm: 17.24, NNZs: 3072, Bias: -97.927882, T: 58500000, Avg. loss: 6.207518\n",
      "Total training time: 468.24 seconds.\n",
      "-- Epoch 235\n",
      "Norm: 18.90, NNZs: 3072, Bias: -132.117041, T: 58000000, Avg. loss: 7.762430\n",
      "Total training time: 468.50 seconds.\n",
      "-- Epoch 233\n",
      "Norm: 17.21, NNZs: 3072, Bias: -97.650609, T: 58750000, Avg. loss: 6.189357\n",
      "Total training time: 470.23 seconds.\n",
      "-- Epoch 236\n",
      "Norm: 18.89, NNZs: 3072, Bias: -131.854570, T: 58250000, Avg. loss: 7.746382\n",
      "Total training time: 470.46 seconds.\n",
      "-- Epoch 234\n",
      "Norm: 17.17, NNZs: 3072, Bias: -97.375518, T: 59000000, Avg. loss: 6.172019\n",
      "Total training time: 472.18 seconds.\n",
      "-- Epoch 237\n",
      "Norm: 18.86, NNZs: 3072, Bias: -131.596689, T: 58500000, Avg. loss: 7.731175\n",
      "Total training time: 472.42 seconds.\n",
      "-- Epoch 235\n",
      "Norm: 17.15, NNZs: 3072, Bias: -97.100550, T: 59250000, Avg. loss: 6.154046\n",
      "Total training time: 474.16 seconds.\n",
      "-- Epoch 238\n",
      "Norm: 18.84, NNZs: 3072, Bias: -131.338100, T: 58750000, Avg. loss: 7.715663\n",
      "Total training time: 474.41 seconds.\n",
      "-- Epoch 236\n",
      "Norm: 17.12, NNZs: 3072, Bias: -96.826629, T: 59500000, Avg. loss: 6.136241\n",
      "Total training time: 476.15 seconds.\n",
      "-- Epoch 239\n",
      "Norm: 18.81, NNZs: 3072, Bias: -131.081935, T: 59000000, Avg. loss: 7.700805\n",
      "Total training time: 476.41 seconds.\n",
      "-- Epoch 237\n",
      "Norm: 17.09, NNZs: 3072, Bias: -96.554777, T: 59750000, Avg. loss: 6.119292\n",
      "Total training time: 478.11 seconds.\n",
      "-- Epoch 240\n",
      "Norm: 18.79, NNZs: 3072, Bias: -130.825289, T: 59250000, Avg. loss: 7.685397\n",
      "Total training time: 478.39 seconds.\n",
      "-- Epoch 238\n",
      "Norm: 17.05, NNZs: 3072, Bias: -96.284045, T: 60000000, Avg. loss: 6.101503\n",
      "Total training time: 480.09 seconds.\n",
      "-- Epoch 241\n",
      "Norm: 18.76, NNZs: 3072, Bias: -130.570916, T: 59500000, Avg. loss: 7.670112\n",
      "Total training time: 480.40 seconds.\n",
      "-- Epoch 239\n",
      "Norm: 17.02, NNZs: 3072, Bias: -96.013837, T: 60250000, Avg. loss: 6.084409\n",
      "Total training time: 482.10 seconds.\n",
      "-- Epoch 242\n",
      "Norm: 18.73, NNZs: 3072, Bias: -130.317682, T: 59750000, Avg. loss: 7.655025\n",
      "Total training time: 482.36 seconds.\n",
      "-- Epoch 240\n",
      "Norm: 16.99, NNZs: 3072, Bias: -95.745181, T: 60500000, Avg. loss: 6.067173\n",
      "Total training time: 484.07 seconds.\n",
      "-- Epoch 243\n",
      "Norm: 18.71, NNZs: 3072, Bias: -130.064295, T: 60000000, Avg. loss: 7.639987\n",
      "Total training time: 484.35 seconds.\n",
      "-- Epoch 241\n",
      "Norm: 16.96, NNZs: 3072, Bias: -95.477315, T: 60750000, Avg. loss: 6.049937\n",
      "Total training time: 486.05 seconds.\n",
      "-- Epoch 244\n",
      "Norm: 18.69, NNZs: 3072, Bias: -129.812726, T: 60250000, Avg. loss: 7.624869\n",
      "Total training time: 486.30 seconds.\n",
      "-- Epoch 242\n",
      "Norm: 16.93, NNZs: 3072, Bias: -95.212159, T: 61000000, Avg. loss: 6.032798\n",
      "Total training time: 488.04 seconds.\n",
      "-- Epoch 245\n",
      "Norm: 18.66, NNZs: 3072, Bias: -129.562724, T: 60500000, Avg. loss: 7.610011\n",
      "Total training time: 488.30 seconds.\n",
      "-- Epoch 243\n",
      "Norm: 16.91, NNZs: 3072, Bias: -94.945340, T: 61250000, Avg. loss: 6.015448\n",
      "Total training time: 490.11 seconds.\n",
      "-- Epoch 246\n",
      "Norm: 18.65, NNZs: 3072, Bias: -129.312567, T: 60750000, Avg. loss: 7.594987\n",
      "Total training time: 490.39 seconds.\n",
      "-- Epoch 244\n",
      "Norm: 16.87, NNZs: 3072, Bias: -94.681655, T: 61500000, Avg. loss: 5.998687\n",
      "Total training time: 492.08 seconds.\n",
      "-- Epoch 247\n",
      "Norm: 18.62, NNZs: 3072, Bias: -129.064048, T: 61000000, Avg. loss: 7.580117\n",
      "Total training time: 492.38 seconds.\n",
      "-- Epoch 245\n",
      "Norm: 16.84, NNZs: 3072, Bias: -94.418111, T: 61750000, Avg. loss: 5.981411\n",
      "Total training time: 494.07 seconds.\n",
      "-- Epoch 248\n",
      "Norm: 18.60, NNZs: 3072, Bias: -128.816998, T: 61250000, Avg. loss: 7.565473\n",
      "Total training time: 494.40 seconds.\n",
      "-- Epoch 246\n",
      "Norm: 16.81, NNZs: 3072, Bias: -94.156872, T: 62000000, Avg. loss: 5.964607\n",
      "Total training time: 496.02 seconds.\n",
      "-- Epoch 249\n",
      "Norm: 18.58, NNZs: 3072, Bias: -128.570020, T: 61500000, Avg. loss: 7.550768\n",
      "Total training time: 496.33 seconds.\n",
      "-- Epoch 247\n",
      "Norm: 16.78, NNZs: 3072, Bias: -93.895687, T: 62250000, Avg. loss: 5.948415\n",
      "Total training time: 498.01 seconds.\n",
      "-- Epoch 250\n",
      "Norm: 18.55, NNZs: 3072, Bias: -128.326050, T: 61750000, Avg. loss: 7.535771\n",
      "Total training time: 498.33 seconds.\n",
      "-- Epoch 248\n",
      "Norm: 16.76, NNZs: 3072, Bias: -93.635120, T: 62500000, Avg. loss: 5.931536\n",
      "Total training time: 500.04 seconds.\n",
      "-- Epoch 251\n",
      "Norm: 18.53, NNZs: 3072, Bias: -128.080679, T: 62000000, Avg. loss: 7.521755\n",
      "Total training time: 500.34 seconds.\n",
      "-- Epoch 249\n",
      "Norm: 16.73, NNZs: 3072, Bias: -93.375629, T: 62750000, Avg. loss: 5.914930\n",
      "Total training time: 502.03 seconds.\n",
      "-- Epoch 252\n",
      "Norm: 18.51, NNZs: 3072, Bias: -127.836722, T: 62250000, Avg. loss: 7.507252\n",
      "Total training time: 502.34 seconds.\n",
      "-- Epoch 250\n",
      "Norm: 16.70, NNZs: 3072, Bias: -93.118111, T: 63000000, Avg. loss: 5.898441\n",
      "Total training time: 503.98 seconds.\n",
      "-- Epoch 253\n",
      "Norm: 18.48, NNZs: 3072, Bias: -127.595110, T: 62500000, Avg. loss: 7.492623\n",
      "Total training time: 504.34 seconds.\n",
      "-- Epoch 251\n",
      "Norm: 16.67, NNZs: 3072, Bias: -92.861192, T: 63250000, Avg. loss: 5.882074\n",
      "Total training time: 505.98 seconds.\n",
      "-- Epoch 254\n",
      "Norm: 18.47, NNZs: 3072, Bias: -127.353030, T: 62750000, Avg. loss: 7.478382\n",
      "Total training time: 506.34 seconds.\n",
      "-- Epoch 252\n",
      "Norm: 16.64, NNZs: 3072, Bias: -92.605892, T: 63500000, Avg. loss: 5.865463\n",
      "Total training time: 507.99 seconds.\n",
      "-- Epoch 255\n",
      "Norm: 18.44, NNZs: 3072, Bias: -127.113415, T: 63000000, Avg. loss: 7.463834\n",
      "Total training time: 508.32 seconds.\n",
      "-- Epoch 253\n",
      "Norm: 16.62, NNZs: 3072, Bias: -92.350337, T: 63750000, Avg. loss: 5.849164\n",
      "Total training time: 509.94 seconds.\n",
      "-- Epoch 256\n",
      "Norm: 18.42, NNZs: 3072, Bias: -126.873449, T: 63250000, Avg. loss: 7.449759\n",
      "Total training time: 510.27 seconds.\n",
      "-- Epoch 254\n",
      "Norm: 16.59, NNZs: 3072, Bias: -92.096881, T: 64000000, Avg. loss: 5.832905\n",
      "Total training time: 511.89 seconds.\n",
      "-- Epoch 257\n",
      "Norm: 18.39, NNZs: 3072, Bias: -126.635756, T: 63500000, Avg. loss: 7.435370\n",
      "Total training time: 512.20 seconds.\n",
      "-- Epoch 255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 16.56, NNZs: 3072, Bias: -91.844555, T: 64250000, Avg. loss: 5.816639\n",
      "Total training time: 513.86 seconds.\n",
      "-- Epoch 258\n",
      "Norm: 18.38, NNZs: 3072, Bias: -126.397471, T: 63750000, Avg. loss: 7.421468\n",
      "Total training time: 514.17 seconds.\n",
      "-- Epoch 256\n",
      "Norm: 16.53, NNZs: 3072, Bias: -91.593151, T: 64500000, Avg. loss: 5.800555\n",
      "Total training time: 515.84 seconds.\n",
      "-- Epoch 259\n",
      "Norm: 18.36, NNZs: 3072, Bias: -126.160267, T: 64000000, Avg. loss: 7.407402\n",
      "Total training time: 516.11 seconds.\n",
      "-- Epoch 257\n",
      "Norm: 16.50, NNZs: 3072, Bias: -91.342079, T: 64750000, Avg. loss: 5.784277\n",
      "Total training time: 517.76 seconds.\n",
      "-- Epoch 260\n",
      "Norm: 18.33, NNZs: 3072, Bias: -125.924855, T: 64250000, Avg. loss: 7.393117\n",
      "Total training time: 518.06 seconds.\n",
      "-- Epoch 258\n",
      "Norm: 16.47, NNZs: 3072, Bias: -91.092218, T: 65000000, Avg. loss: 5.768465\n",
      "Total training time: 519.73 seconds.\n",
      "-- Epoch 261\n",
      "Norm: 18.30, NNZs: 3072, Bias: -125.691367, T: 64500000, Avg. loss: 7.378705\n",
      "Total training time: 520.00 seconds.\n",
      "-- Epoch 259\n",
      "Norm: 16.45, NNZs: 3072, Bias: -90.843058, T: 65250000, Avg. loss: 5.752582\n",
      "Total training time: 521.64 seconds.\n",
      "-- Epoch 262\n",
      "Norm: 18.29, NNZs: 3072, Bias: -125.455670, T: 64750000, Avg. loss: 7.365315\n",
      "Total training time: 521.94 seconds.\n",
      "-- Epoch 260\n",
      "Norm: 16.41, NNZs: 3072, Bias: -90.595928, T: 65500000, Avg. loss: 5.736539\n",
      "Total training time: 523.66 seconds.\n",
      "-- Epoch 263\n",
      "Norm: 18.27, NNZs: 3072, Bias: -125.222654, T: 65000000, Avg. loss: 7.351526\n",
      "Total training time: 523.96 seconds.\n",
      "-- Epoch 261\n",
      "Norm: 16.39, NNZs: 3072, Bias: -90.348196, T: 65750000, Avg. loss: 5.720757\n",
      "Total training time: 525.64 seconds.\n",
      "-- Epoch 264\n",
      "Norm: 18.25, NNZs: 3072, Bias: -124.991479, T: 65250000, Avg. loss: 7.337730\n",
      "Total training time: 525.94 seconds.\n",
      "-- Epoch 262\n",
      "Norm: 16.36, NNZs: 3072, Bias: -90.102874, T: 66000000, Avg. loss: 5.705211\n",
      "Total training time: 527.60 seconds.\n",
      "-- Epoch 265\n",
      "Norm: 18.22, NNZs: 3072, Bias: -124.760644, T: 65500000, Avg. loss: 7.323935\n",
      "Total training time: 527.92 seconds.\n",
      "-- Epoch 263\n",
      "Norm: 16.33, NNZs: 3072, Bias: -89.858142, T: 66250000, Avg. loss: 5.689521\n",
      "Total training time: 529.54 seconds.\n",
      "-- Epoch 266\n",
      "Norm: 18.20, NNZs: 3072, Bias: -124.530187, T: 65750000, Avg. loss: 7.310369\n",
      "Total training time: 529.94 seconds.\n",
      "-- Epoch 264\n",
      "Norm: 16.30, NNZs: 3072, Bias: -89.614490, T: 66500000, Avg. loss: 5.673733\n",
      "Total training time: 531.58 seconds.\n",
      "-- Epoch 267\n",
      "Norm: 18.18, NNZs: 3072, Bias: -124.300575, T: 66000000, Avg. loss: 7.296666\n",
      "Total training time: 531.97 seconds.\n",
      "-- Epoch 265\n",
      "Norm: 16.28, NNZs: 3072, Bias: -89.371272, T: 66750000, Avg. loss: 5.657929\n",
      "Total training time: 533.62 seconds.\n",
      "-- Epoch 268\n",
      "Norm: 18.16, NNZs: 3072, Bias: -124.072556, T: 66250000, Avg. loss: 7.282974\n",
      "Total training time: 533.97 seconds.\n",
      "-- Epoch 266\n",
      "Norm: 16.25, NNZs: 3072, Bias: -89.129102, T: 67000000, Avg. loss: 5.642531\n",
      "Total training time: 535.58 seconds.\n",
      "-- Epoch 269\n",
      "Norm: 18.15, NNZs: 3072, Bias: -123.843808, T: 66500000, Avg. loss: 7.269219\n",
      "Total training time: 536.00 seconds.\n",
      "-- Epoch 267\n",
      "Norm: 16.22, NNZs: 3072, Bias: -88.888388, T: 67250000, Avg. loss: 5.626822\n",
      "Total training time: 537.62 seconds.\n",
      "-- Epoch 270\n",
      "Norm: 18.12, NNZs: 3072, Bias: -123.617965, T: 66750000, Avg. loss: 7.256094\n",
      "Total training time: 538.02 seconds.\n",
      "-- Epoch 268\n",
      "Norm: 16.20, NNZs: 3072, Bias: -88.647451, T: 67500000, Avg. loss: 5.611561\n",
      "Total training time: 539.63 seconds.\n",
      "-- Epoch 271\n",
      "Norm: 18.10, NNZs: 3072, Bias: -123.391942, T: 67000000, Avg. loss: 7.242619\n",
      "Total training time: 540.06 seconds.\n",
      "-- Epoch 269\n",
      "Norm: 16.17, NNZs: 3072, Bias: -88.408438, T: 67750000, Avg. loss: 5.596259\n",
      "Total training time: 541.72 seconds.\n",
      "-- Epoch 272\n",
      "Norm: 18.08, NNZs: 3072, Bias: -123.167123, T: 67250000, Avg. loss: 7.228949\n",
      "Total training time: 542.12 seconds.\n",
      "-- Epoch 270\n",
      "Norm: 16.14, NNZs: 3072, Bias: -88.170875, T: 68000000, Avg. loss: 5.581106\n",
      "Total training time: 543.80 seconds.\n",
      "-- Epoch 273\n",
      "Norm: 18.06, NNZs: 3072, Bias: -122.942639, T: 67500000, Avg. loss: 7.215639\n",
      "Total training time: 544.19 seconds.\n",
      "-- Epoch 271\n",
      "Norm: 16.12, NNZs: 3072, Bias: -87.932709, T: 68250000, Avg. loss: 5.565743\n",
      "Total training time: 545.79 seconds.\n",
      "-- Epoch 274\n",
      "Norm: 18.03, NNZs: 3072, Bias: -122.719979, T: 67750000, Avg. loss: 7.202577\n",
      "Total training time: 546.23 seconds.\n",
      "-- Epoch 272\n",
      "Norm: 16.08, NNZs: 3072, Bias: -87.696956, T: 68500000, Avg. loss: 5.550180\n",
      "Total training time: 547.79 seconds.\n",
      "-- Epoch 275\n",
      "Norm: 18.01, NNZs: 3072, Bias: -122.497765, T: 68000000, Avg. loss: 7.189183\n",
      "Total training time: 548.22 seconds.\n",
      "-- Epoch 273\n",
      "Norm: 16.06, NNZs: 3072, Bias: -87.461080, T: 68750000, Avg. loss: 5.535666\n",
      "Total training time: 549.83 seconds.\n",
      "-- Epoch 276\n",
      "Norm: 17.99, NNZs: 3072, Bias: -122.275728, T: 68250000, Avg. loss: 7.176135\n",
      "Total training time: 550.28 seconds.\n",
      "-- Epoch 274\n",
      "Norm: 16.03, NNZs: 3072, Bias: -87.226397, T: 69000000, Avg. loss: 5.520528\n",
      "Total training time: 551.86 seconds.\n",
      "-- Epoch 277\n",
      "Norm: 17.98, NNZs: 3072, Bias: -122.054497, T: 68500000, Avg. loss: 7.162542\n",
      "Total training time: 552.33 seconds.\n",
      "-- Epoch 275\n",
      "Norm: 16.00, NNZs: 3072, Bias: -86.992303, T: 69250000, Avg. loss: 5.505193\n",
      "Total training time: 553.89 seconds.\n",
      "-- Epoch 278\n",
      "Norm: 17.95, NNZs: 3072, Bias: -121.834735, T: 68750000, Avg. loss: 7.149812\n",
      "Total training time: 554.40 seconds.\n",
      "-- Epoch 276\n",
      "Norm: 15.98, NNZs: 3072, Bias: -86.758576, T: 69500000, Avg. loss: 5.490668\n",
      "Total training time: 555.97 seconds.\n",
      "-- Epoch 279\n",
      "Norm: 17.93, NNZs: 3072, Bias: -121.615700, T: 69000000, Avg. loss: 7.136760\n",
      "Total training time: 556.46 seconds.\n",
      "-- Epoch 277\n",
      "Norm: 15.95, NNZs: 3072, Bias: -86.527632, T: 69750000, Avg. loss: 5.475449\n",
      "Total training time: 557.95 seconds.\n",
      "-- Epoch 280\n",
      "Norm: 17.91, NNZs: 3072, Bias: -121.397069, T: 69250000, Avg. loss: 7.123789\n",
      "Total training time: 558.48 seconds.\n",
      "-- Epoch 278\n",
      "Norm: 15.92, NNZs: 3072, Bias: -86.295816, T: 70000000, Avg. loss: 5.461013\n",
      "Total training time: 560.02 seconds.\n",
      "-- Epoch 281\n",
      "Norm: 17.89, NNZs: 3072, Bias: -121.179795, T: 69500000, Avg. loss: 7.110514\n",
      "Total training time: 560.63 seconds.\n",
      "-- Epoch 279\n",
      "Norm: 15.89, NNZs: 3072, Bias: -86.065818, T: 70250000, Avg. loss: 5.446034\n",
      "Total training time: 562.16 seconds.\n",
      "-- Epoch 282\n",
      "Norm: 17.87, NNZs: 3072, Bias: -120.963170, T: 69750000, Avg. loss: 7.097914\n",
      "Total training time: 562.75 seconds.\n",
      "-- Epoch 280\n",
      "Norm: 15.87, NNZs: 3072, Bias: -85.836050, T: 70500000, Avg. loss: 5.431456\n",
      "Total training time: 564.24 seconds.\n",
      "-- Epoch 283\n",
      "Norm: 17.85, NNZs: 3072, Bias: -120.746810, T: 70000000, Avg. loss: 7.085033\n",
      "Total training time: 565.03 seconds.\n",
      "-- Epoch 281\n",
      "Norm: 15.85, NNZs: 3072, Bias: -85.606788, T: 70750000, Avg. loss: 5.416817\n",
      "Total training time: 566.67 seconds.\n",
      "-- Epoch 284\n",
      "Norm: 17.83, NNZs: 3072, Bias: -120.532127, T: 70250000, Avg. loss: 7.072305\n",
      "Total training time: 567.28 seconds.\n",
      "-- Epoch 282\n",
      "Norm: 15.82, NNZs: 3072, Bias: -85.378956, T: 71000000, Avg. loss: 5.402219\n",
      "Total training time: 568.69 seconds.\n",
      "-- Epoch 285\n",
      "Norm: 17.81, NNZs: 3072, Bias: -120.317856, T: 70500000, Avg. loss: 7.059622\n",
      "Total training time: 569.32 seconds.\n",
      "-- Epoch 283\n",
      "Norm: 15.80, NNZs: 3072, Bias: -85.150975, T: 71250000, Avg. loss: 5.387417\n",
      "Total training time: 570.75 seconds.\n",
      "-- Epoch 286\n",
      "Norm: 17.78, NNZs: 3072, Bias: -120.105085, T: 70750000, Avg. loss: 7.046507\n",
      "Total training time: 571.41 seconds.\n",
      "-- Epoch 284\n",
      "Norm: 15.77, NNZs: 3072, Bias: -84.924377, T: 71500000, Avg. loss: 5.373011\n",
      "Total training time: 572.82 seconds.\n",
      "-- Epoch 287\n",
      "Norm: 17.77, NNZs: 3072, Bias: -119.890668, T: 71000000, Avg. loss: 7.034036\n",
      "Total training time: 573.44 seconds.\n",
      "-- Epoch 285\n",
      "Norm: 15.74, NNZs: 3072, Bias: -84.699376, T: 71750000, Avg. loss: 5.358505\n",
      "Total training time: 574.84 seconds.\n",
      "-- Epoch 288\n",
      "Norm: 17.75, NNZs: 3072, Bias: -119.678990, T: 71250000, Avg. loss: 7.021390\n",
      "Total training time: 575.44 seconds.\n",
      "-- Epoch 286\n",
      "Norm: 15.72, NNZs: 3072, Bias: -84.474518, T: 72000000, Avg. loss: 5.344325\n",
      "Total training time: 576.88 seconds.\n",
      "-- Epoch 289\n",
      "Norm: 17.73, NNZs: 3072, Bias: -119.467405, T: 71500000, Avg. loss: 7.008626\n",
      "Total training time: 577.48 seconds.\n",
      "-- Epoch 287\n",
      "Norm: 15.69, NNZs: 3072, Bias: -84.250200, T: 72250000, Avg. loss: 5.329886\n",
      "Total training time: 578.91 seconds.\n",
      "-- Epoch 290\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 17.71, NNZs: 3072, Bias: -119.256792, T: 71750000, Avg. loss: 6.996467\n",
      "Total training time: 579.51 seconds.\n",
      "-- Epoch 288\n",
      "Norm: 15.66, NNZs: 3072, Bias: -84.027727, T: 72500000, Avg. loss: 5.315650\n",
      "Total training time: 580.91 seconds.\n",
      "-- Epoch 291\n",
      "Norm: 17.69, NNZs: 3072, Bias: -119.046802, T: 72000000, Avg. loss: 6.983870\n",
      "Total training time: 581.50 seconds.\n",
      "-- Epoch 289\n",
      "Norm: 15.64, NNZs: 3072, Bias: -83.804921, T: 72750000, Avg. loss: 5.301417\n",
      "Total training time: 582.92 seconds.\n",
      "-- Epoch 292\n",
      "Norm: 17.67, NNZs: 3072, Bias: -118.837456, T: 72250000, Avg. loss: 6.971006\n",
      "Total training time: 583.51 seconds.\n",
      "-- Epoch 290\n",
      "Norm: 15.62, NNZs: 3072, Bias: -83.583404, T: 73000000, Avg. loss: 5.287332\n",
      "Total training time: 584.95 seconds.\n",
      "-- Epoch 293\n",
      "Norm: 17.65, NNZs: 3072, Bias: -118.629644, T: 72500000, Avg. loss: 6.958907\n",
      "Total training time: 585.53 seconds.\n",
      "-- Epoch 291\n",
      "Norm: 15.59, NNZs: 3072, Bias: -83.362600, T: 73250000, Avg. loss: 5.272884\n",
      "Total training time: 586.94 seconds.\n",
      "-- Epoch 294\n",
      "Norm: 17.63, NNZs: 3072, Bias: -118.422657, T: 72750000, Avg. loss: 6.946744\n",
      "Total training time: 587.53 seconds.\n",
      "-- Epoch 292\n",
      "Norm: 15.56, NNZs: 3072, Bias: -83.142647, T: 73500000, Avg. loss: 5.259041\n",
      "Total training time: 588.96 seconds.\n",
      "-- Epoch 295\n",
      "Norm: 17.60, NNZs: 3072, Bias: -118.216243, T: 73000000, Avg. loss: 6.934238\n",
      "Total training time: 589.52 seconds.\n",
      "-- Epoch 293\n",
      "Norm: 15.54, NNZs: 3072, Bias: -82.923077, T: 73750000, Avg. loss: 5.244876\n",
      "Total training time: 590.98 seconds.\n",
      "-- Epoch 296\n",
      "Norm: 17.59, NNZs: 3072, Bias: -118.008846, T: 73250000, Avg. loss: 6.922425\n",
      "Total training time: 591.55 seconds.\n",
      "-- Epoch 294\n",
      "Norm: 15.51, NNZs: 3072, Bias: -82.704792, T: 74000000, Avg. loss: 5.230903\n",
      "Total training time: 592.96 seconds.\n",
      "-- Epoch 297\n",
      "Norm: 17.58, NNZs: 3072, Bias: -117.802838, T: 73500000, Avg. loss: 6.909932\n",
      "Total training time: 593.53 seconds.\n",
      "-- Epoch 295\n",
      "Norm: 15.49, NNZs: 3072, Bias: -82.487060, T: 74250000, Avg. loss: 5.217004\n",
      "Total training time: 594.92 seconds.\n",
      "-- Epoch 298\n",
      "Norm: 17.55, NNZs: 3072, Bias: -117.599035, T: 73750000, Avg. loss: 6.897702\n",
      "Total training time: 595.54 seconds.\n",
      "-- Epoch 296\n",
      "Norm: 15.46, NNZs: 3072, Bias: -82.270087, T: 74500000, Avg. loss: 5.203179\n",
      "Total training time: 596.96 seconds.\n",
      "-- Epoch 299\n",
      "Norm: 17.53, NNZs: 3072, Bias: -117.394810, T: 74000000, Avg. loss: 6.885506\n",
      "Total training time: 597.56 seconds.\n",
      "-- Epoch 297\n",
      "Norm: 15.44, NNZs: 3072, Bias: -82.054270, T: 74750000, Avg. loss: 5.189102\n",
      "Total training time: 598.95 seconds.\n",
      "-- Epoch 300\n",
      "Norm: 17.51, NNZs: 3072, Bias: -117.191502, T: 74250000, Avg. loss: 6.873587\n",
      "Total training time: 599.53 seconds.\n",
      "-- Epoch 298\n",
      "Norm: 15.41, NNZs: 3072, Bias: -81.838538, T: 75000000, Avg. loss: 5.175586\n",
      "Total training time: 600.92 seconds.\n",
      "-- Epoch 301\n",
      "Norm: 17.50, NNZs: 3072, Bias: -116.987900, T: 74500000, Avg. loss: 6.861617\n",
      "Total training time: 601.53 seconds.\n",
      "-- Epoch 299\n",
      "Norm: 15.39, NNZs: 3072, Bias: -81.623635, T: 75250000, Avg. loss: 5.161840\n",
      "Total training time: 602.91 seconds.\n",
      "-- Epoch 302\n",
      "Norm: 17.48, NNZs: 3072, Bias: -116.786266, T: 74750000, Avg. loss: 6.849513\n",
      "Total training time: 603.53 seconds.\n",
      "-- Epoch 300\n",
      "Norm: 15.36, NNZs: 3072, Bias: -81.410076, T: 75500000, Avg. loss: 5.147954\n",
      "Total training time: 604.88 seconds.\n",
      "-- Epoch 303\n",
      "Norm: 17.46, NNZs: 3072, Bias: -116.584453, T: 75000000, Avg. loss: 6.837425\n",
      "Total training time: 605.52 seconds.\n",
      "-- Epoch 301\n",
      "Norm: 15.34, NNZs: 3072, Bias: -81.196468, T: 75750000, Avg. loss: 5.134376\n",
      "Total training time: 606.83 seconds.\n",
      "-- Epoch 304\n",
      "Norm: 17.44, NNZs: 3072, Bias: -116.384574, T: 75250000, Avg. loss: 6.825511\n",
      "Total training time: 607.50 seconds.\n",
      "-- Epoch 302\n",
      "Norm: 15.31, NNZs: 3072, Bias: -80.983646, T: 76000000, Avg. loss: 5.120737\n",
      "Total training time: 608.80 seconds.\n",
      "-- Epoch 305\n",
      "Norm: 17.42, NNZs: 3072, Bias: -116.184991, T: 75500000, Avg. loss: 6.813524\n",
      "Total training time: 609.46 seconds.\n",
      "-- Epoch 303\n",
      "Norm: 15.29, NNZs: 3072, Bias: -80.772059, T: 76250000, Avg. loss: 5.106804\n",
      "Total training time: 610.76 seconds.\n",
      "-- Epoch 306\n",
      "Norm: 17.40, NNZs: 3072, Bias: -115.985027, T: 75750000, Avg. loss: 6.802010\n",
      "Total training time: 611.49 seconds.\n",
      "-- Epoch 304\n",
      "Norm: 15.26, NNZs: 3072, Bias: -80.561034, T: 76500000, Avg. loss: 5.093869\n",
      "Total training time: 612.70 seconds.\n",
      "-- Epoch 307\n",
      "Norm: 17.38, NNZs: 3072, Bias: -115.786523, T: 76000000, Avg. loss: 6.790077\n",
      "Total training time: 613.44 seconds.\n",
      "-- Epoch 305\n",
      "Norm: 15.24, NNZs: 3072, Bias: -80.350184, T: 76750000, Avg. loss: 5.080256\n",
      "Total training time: 614.69 seconds.\n",
      "-- Epoch 308\n",
      "Norm: 17.37, NNZs: 3072, Bias: -115.587833, T: 76250000, Avg. loss: 6.778235\n",
      "Total training time: 615.43 seconds.\n",
      "-- Epoch 306\n",
      "Norm: 15.22, NNZs: 3072, Bias: -80.140625, T: 77000000, Avg. loss: 5.066900\n",
      "Total training time: 616.66 seconds.\n",
      "-- Epoch 309\n",
      "Norm: 17.34, NNZs: 3072, Bias: -115.391585, T: 76500000, Avg. loss: 6.766580\n",
      "Total training time: 617.46 seconds.\n",
      "-- Epoch 307\n",
      "Norm: 15.19, NNZs: 3072, Bias: -79.931893, T: 77250000, Avg. loss: 5.053477\n",
      "Total training time: 618.71 seconds.\n",
      "-- Epoch 310\n",
      "Norm: 17.33, NNZs: 3072, Bias: -115.194632, T: 76750000, Avg. loss: 6.755032\n",
      "Total training time: 619.44 seconds.\n",
      "-- Epoch 308\n",
      "Norm: 15.17, NNZs: 3072, Bias: -79.723573, T: 77500000, Avg. loss: 5.040179\n",
      "Total training time: 620.70 seconds.\n",
      "-- Epoch 311\n",
      "Norm: 17.31, NNZs: 3072, Bias: -114.998913, T: 77000000, Avg. loss: 6.743186\n",
      "Total training time: 621.47 seconds.\n",
      "-- Epoch 309\n",
      "Norm: 15.14, NNZs: 3072, Bias: -79.515556, T: 77750000, Avg. loss: 5.026947\n",
      "Total training time: 622.70 seconds.\n",
      "-- Epoch 312\n",
      "Norm: 17.29, NNZs: 3072, Bias: -114.803079, T: 77250000, Avg. loss: 6.731672\n",
      "Total training time: 623.49 seconds.\n",
      "-- Epoch 310\n",
      "Norm: 15.12, NNZs: 3072, Bias: -79.308991, T: 78000000, Avg. loss: 5.013708\n",
      "Total training time: 624.71 seconds.\n",
      "-- Epoch 313\n",
      "Norm: 17.27, NNZs: 3072, Bias: -114.608141, T: 77500000, Avg. loss: 6.719979\n",
      "Total training time: 625.48 seconds.\n",
      "-- Epoch 311\n",
      "Norm: 15.10, NNZs: 3072, Bias: -79.102486, T: 78250000, Avg. loss: 5.000297\n",
      "Total training time: 626.63 seconds.\n",
      "-- Epoch 314\n",
      "Norm: 17.25, NNZs: 3072, Bias: -114.414752, T: 77750000, Avg. loss: 6.708363\n",
      "Total training time: 627.45 seconds.\n",
      "-- Epoch 312\n",
      "Norm: 15.07, NNZs: 3072, Bias: -78.896496, T: 78500000, Avg. loss: 4.987347\n",
      "Total training time: 628.62 seconds.\n",
      "-- Epoch 315\n",
      "Norm: 17.23, NNZs: 3072, Bias: -114.221548, T: 78000000, Avg. loss: 6.696921\n",
      "Total training time: 629.44 seconds.\n",
      "-- Epoch 313\n",
      "Norm: 15.05, NNZs: 3072, Bias: -78.692293, T: 78750000, Avg. loss: 4.974104\n",
      "Total training time: 630.61 seconds.\n",
      "-- Epoch 316\n",
      "Norm: 17.21, NNZs: 3072, Bias: -114.028389, T: 78250000, Avg. loss: 6.685490\n",
      "Total training time: 631.44 seconds.\n",
      "-- Epoch 314\n",
      "Norm: 15.02, NNZs: 3072, Bias: -78.488010, T: 79000000, Avg. loss: 4.961090\n",
      "Total training time: 632.56 seconds.\n",
      "-- Epoch 317\n",
      "Norm: 17.19, NNZs: 3072, Bias: -113.836921, T: 78500000, Avg. loss: 6.673988\n",
      "Total training time: 633.45 seconds.\n",
      "-- Epoch 315\n",
      "Norm: 15.00, NNZs: 3072, Bias: -78.284359, T: 79250000, Avg. loss: 4.948030\n",
      "Total training time: 634.63 seconds.\n",
      "-- Epoch 318\n",
      "Norm: 17.18, NNZs: 3072, Bias: -113.644435, T: 78750000, Avg. loss: 6.662894\n",
      "Total training time: 635.52 seconds.\n",
      "-- Epoch 316\n",
      "Norm: 14.97, NNZs: 3072, Bias: -78.082211, T: 79500000, Avg. loss: 4.935088\n",
      "Total training time: 636.57 seconds.\n",
      "-- Epoch 319\n",
      "Norm: 17.16, NNZs: 3072, Bias: -113.453424, T: 79000000, Avg. loss: 6.651466\n",
      "Total training time: 637.51 seconds.\n",
      "-- Epoch 317\n",
      "Norm: 14.95, NNZs: 3072, Bias: -77.879070, T: 79750000, Avg. loss: 4.922058\n",
      "Total training time: 638.54 seconds.\n",
      "-- Epoch 320\n",
      "Norm: 17.14, NNZs: 3072, Bias: -113.263163, T: 79250000, Avg. loss: 6.640188\n",
      "Total training time: 639.50 seconds.\n",
      "-- Epoch 318\n",
      "Norm: 14.93, NNZs: 3072, Bias: -77.677406, T: 80000000, Avg. loss: 4.909244\n",
      "Total training time: 640.53 seconds.\n",
      "-- Epoch 321\n",
      "Norm: 17.12, NNZs: 3072, Bias: -113.073507, T: 79500000, Avg. loss: 6.628903\n",
      "Total training time: 641.53 seconds.\n",
      "-- Epoch 319\n",
      "Norm: 14.90, NNZs: 3072, Bias: -77.477051, T: 80250000, Avg. loss: 4.896523\n",
      "Total training time: 642.51 seconds.\n",
      "-- Epoch 322\n",
      "Norm: 17.10, NNZs: 3072, Bias: -112.884934, T: 79750000, Avg. loss: 6.617634\n",
      "Total training time: 643.46 seconds.\n",
      "-- Epoch 320\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 14.88, NNZs: 3072, Bias: -77.277117, T: 80500000, Avg. loss: 4.883780\n",
      "Total training time: 644.48 seconds.\n",
      "-- Epoch 323\n",
      "Norm: 17.08, NNZs: 3072, Bias: -112.696652, T: 80000000, Avg. loss: 6.606433\n",
      "Total training time: 645.47 seconds.\n",
      "-- Epoch 321\n",
      "Norm: 14.86, NNZs: 3072, Bias: -77.076729, T: 80750000, Avg. loss: 4.870874\n",
      "Total training time: 646.43 seconds.\n",
      "-- Epoch 324\n",
      "Norm: 17.07, NNZs: 3072, Bias: -112.508240, T: 80250000, Avg. loss: 6.595263\n",
      "Total training time: 647.47 seconds.\n",
      "-- Epoch 322\n",
      "Norm: 14.83, NNZs: 3072, Bias: -76.878329, T: 81000000, Avg. loss: 4.858168\n",
      "Total training time: 648.52 seconds.\n",
      "-- Epoch 325\n",
      "Norm: 17.05, NNZs: 3072, Bias: -112.321096, T: 80500000, Avg. loss: 6.583855\n",
      "Total training time: 649.58 seconds.\n",
      "-- Epoch 323\n",
      "Norm: 14.81, NNZs: 3072, Bias: -76.679514, T: 81250000, Avg. loss: 4.845669\n",
      "Total training time: 650.61 seconds.\n",
      "-- Epoch 326\n",
      "Norm: 17.04, NNZs: 3072, Bias: -112.133872, T: 80750000, Avg. loss: 6.573008\n",
      "Total training time: 651.61 seconds.\n",
      "-- Epoch 324\n",
      "Norm: 14.79, NNZs: 3072, Bias: -76.481561, T: 81500000, Avg. loss: 4.832892\n",
      "Total training time: 652.67 seconds.\n",
      "-- Epoch 327\n",
      "Norm: 17.02, NNZs: 3072, Bias: -111.947933, T: 81000000, Avg. loss: 6.561835\n",
      "Total training time: 653.73 seconds.\n",
      "-- Epoch 325\n",
      "Norm: 14.76, NNZs: 3072, Bias: -76.284668, T: 81750000, Avg. loss: 4.820350\n",
      "Total training time: 654.73 seconds.\n",
      "-- Epoch 328\n",
      "Norm: 17.00, NNZs: 3072, Bias: -111.762430, T: 81250000, Avg. loss: 6.550977\n",
      "Total training time: 655.77 seconds.\n",
      "-- Epoch 326\n",
      "Norm: 14.74, NNZs: 3072, Bias: -76.088409, T: 82000000, Avg. loss: 4.807858\n",
      "Total training time: 656.78 seconds.\n",
      "-- Epoch 329\n",
      "Norm: 16.98, NNZs: 3072, Bias: -111.577764, T: 81500000, Avg. loss: 6.539855\n",
      "Total training time: 657.86 seconds.\n",
      "-- Epoch 327\n",
      "Norm: 14.72, NNZs: 3072, Bias: -75.891858, T: 82250000, Avg. loss: 4.795016\n",
      "Total training time: 658.88 seconds.\n",
      "-- Epoch 330\n",
      "Norm: 16.96, NNZs: 3072, Bias: -111.393605, T: 81750000, Avg. loss: 6.528989\n",
      "Total training time: 660.05 seconds.\n",
      "-- Epoch 328\n",
      "Norm: 14.69, NNZs: 3072, Bias: -75.697284, T: 82500000, Avg. loss: 4.782786\n",
      "Total training time: 661.07 seconds.\n",
      "-- Epoch 331\n",
      "Norm: 16.95, NNZs: 3072, Bias: -111.209342, T: 82000000, Avg. loss: 6.518202\n",
      "Total training time: 662.26 seconds.\n",
      "-- Epoch 329\n",
      "Norm: 14.67, NNZs: 3072, Bias: -75.502112, T: 82750000, Avg. loss: 4.770359\n",
      "Total training time: 663.27 seconds.\n",
      "-- Epoch 332\n",
      "Norm: 16.93, NNZs: 3072, Bias: -111.026193, T: 82250000, Avg. loss: 6.507215\n",
      "Total training time: 664.39 seconds.\n",
      "-- Epoch 330\n",
      "Norm: 14.64, NNZs: 3072, Bias: -75.308340, T: 83000000, Avg. loss: 4.757934\n",
      "Total training time: 665.38 seconds.\n",
      "-- Epoch 333\n",
      "Norm: 16.91, NNZs: 3072, Bias: -110.843478, T: 82500000, Avg. loss: 6.496434\n",
      "Total training time: 666.45 seconds.\n",
      "-- Epoch 331\n",
      "Norm: 14.62, NNZs: 3072, Bias: -75.114912, T: 83250000, Avg. loss: 4.745578\n",
      "Total training time: 667.44 seconds.\n",
      "-- Epoch 334\n",
      "Norm: 16.89, NNZs: 3072, Bias: -110.661800, T: 82750000, Avg. loss: 6.485683\n",
      "Total training time: 668.54 seconds.\n",
      "-- Epoch 332\n",
      "Norm: 14.60, NNZs: 3072, Bias: -74.922207, T: 83500000, Avg. loss: 4.732934\n",
      "Total training time: 669.51 seconds.\n",
      "-- Epoch 335\n",
      "Norm: 16.87, NNZs: 3072, Bias: -110.480013, T: 83000000, Avg. loss: 6.474882\n",
      "Total training time: 670.65 seconds.\n",
      "-- Epoch 333\n",
      "Norm: 14.58, NNZs: 3072, Bias: -74.729231, T: 83750000, Avg. loss: 4.720989\n",
      "Total training time: 671.59 seconds.\n",
      "-- Epoch 336\n",
      "Norm: 16.86, NNZs: 3072, Bias: -110.298775, T: 83250000, Avg. loss: 6.463404\n",
      "Total training time: 672.72 seconds.\n",
      "-- Epoch 334\n",
      "Norm: 14.55, NNZs: 3072, Bias: -74.537813, T: 84000000, Avg. loss: 4.708632\n",
      "Total training time: 673.66 seconds.\n",
      "-- Epoch 337\n",
      "Norm: 16.84, NNZs: 3072, Bias: -110.118567, T: 83500000, Avg. loss: 6.453450\n",
      "Total training time: 674.76 seconds.\n",
      "-- Epoch 335\n",
      "Norm: 14.53, NNZs: 3072, Bias: -74.346603, T: 84250000, Avg. loss: 4.696487\n",
      "Total training time: 675.68 seconds.\n",
      "-- Epoch 338\n",
      "Norm: 16.82, NNZs: 3072, Bias: -109.938814, T: 83750000, Avg. loss: 6.442416\n",
      "Total training time: 676.78 seconds.\n",
      "-- Epoch 336\n",
      "Norm: 14.51, NNZs: 3072, Bias: -74.156515, T: 84500000, Avg. loss: 4.683966\n",
      "Total training time: 677.68 seconds.\n",
      "-- Epoch 339\n",
      "Norm: 16.80, NNZs: 3072, Bias: -109.760143, T: 84000000, Avg. loss: 6.431994\n",
      "Total training time: 678.82 seconds.\n",
      "-- Epoch 337\n",
      "Norm: 14.49, NNZs: 3072, Bias: -73.965995, T: 84750000, Avg. loss: 4.672409\n",
      "Total training time: 679.79 seconds.\n",
      "-- Epoch 340\n",
      "Norm: 16.78, NNZs: 3072, Bias: -109.581013, T: 84250000, Avg. loss: 6.421537\n",
      "Total training time: 680.97 seconds.\n",
      "-- Epoch 338\n",
      "Norm: 14.46, NNZs: 3072, Bias: -73.777559, T: 85000000, Avg. loss: 4.659883\n",
      "Total training time: 681.90 seconds.\n",
      "-- Epoch 341\n",
      "Norm: 16.77, NNZs: 3072, Bias: -109.402067, T: 84500000, Avg. loss: 6.410903\n",
      "Total training time: 683.10 seconds.\n",
      "-- Epoch 339\n",
      "Norm: 14.44, NNZs: 3072, Bias: -73.588604, T: 85250000, Avg. loss: 4.648021\n",
      "Total training time: 684.03 seconds.\n",
      "-- Epoch 342\n",
      "Norm: 16.75, NNZs: 3072, Bias: -109.224258, T: 84750000, Avg. loss: 6.400160\n",
      "Total training time: 685.22 seconds.\n",
      "-- Epoch 340\n",
      "Norm: 14.42, NNZs: 3072, Bias: -73.400127, T: 85500000, Avg. loss: 4.636044\n",
      "Total training time: 686.24 seconds.\n",
      "-- Epoch 343\n",
      "Norm: 16.73, NNZs: 3072, Bias: -109.047781, T: 85000000, Avg. loss: 6.389812\n",
      "Total training time: 687.50 seconds.\n",
      "-- Epoch 341\n",
      "Norm: 14.39, NNZs: 3072, Bias: -73.212691, T: 85750000, Avg. loss: 4.623923\n",
      "Total training time: 688.48 seconds.\n",
      "-- Epoch 344\n",
      "Norm: 16.72, NNZs: 3072, Bias: -108.870500, T: 85250000, Avg. loss: 6.379188\n",
      "Total training time: 689.66 seconds.\n",
      "-- Epoch 342\n",
      "Norm: 14.37, NNZs: 3072, Bias: -73.025602, T: 86000000, Avg. loss: 4.612039\n",
      "Total training time: 690.62 seconds.\n",
      "-- Epoch 345\n",
      "Norm: 16.70, NNZs: 3072, Bias: -108.694765, T: 85500000, Avg. loss: 6.368859\n",
      "Total training time: 691.78 seconds.\n",
      "-- Epoch 343\n",
      "Norm: 14.35, NNZs: 3072, Bias: -72.839525, T: 86250000, Avg. loss: 4.600044\n",
      "Total training time: 692.80 seconds.\n",
      "-- Epoch 346\n",
      "Norm: 16.68, NNZs: 3072, Bias: -108.518915, T: 85750000, Avg. loss: 6.358472\n",
      "Total training time: 694.04 seconds.\n",
      "-- Epoch 344\n",
      "Norm: 14.33, NNZs: 3072, Bias: -72.652636, T: 86500000, Avg. loss: 4.588234\n",
      "Total training time: 695.07 seconds.\n",
      "-- Epoch 347\n",
      "Norm: 16.67, NNZs: 3072, Bias: -108.343879, T: 86000000, Avg. loss: 6.347946\n",
      "Total training time: 696.26 seconds.\n",
      "-- Epoch 345\n",
      "Norm: 14.31, NNZs: 3072, Bias: -72.467566, T: 86750000, Avg. loss: 4.576598\n",
      "Total training time: 697.25 seconds.\n",
      "-- Epoch 348\n",
      "Norm: 16.65, NNZs: 3072, Bias: -108.169362, T: 86250000, Avg. loss: 6.337664\n",
      "Total training time: 698.42 seconds.\n",
      "-- Epoch 346\n",
      "Norm: 14.28, NNZs: 3072, Bias: -72.282725, T: 87000000, Avg. loss: 4.564403\n",
      "Total training time: 699.39 seconds.\n",
      "-- Epoch 349\n",
      "Norm: 16.63, NNZs: 3072, Bias: -107.995897, T: 86500000, Avg. loss: 6.327121\n",
      "Total training time: 700.56 seconds.\n",
      "-- Epoch 347\n",
      "Norm: 14.26, NNZs: 3072, Bias: -72.098183, T: 87250000, Avg. loss: 4.552651\n",
      "Total training time: 701.53 seconds.\n",
      "-- Epoch 350\n",
      "Norm: 16.61, NNZs: 3072, Bias: -107.822340, T: 86750000, Avg. loss: 6.317092\n",
      "Total training time: 702.73 seconds.\n",
      "-- Epoch 348\n",
      "Norm: 14.24, NNZs: 3072, Bias: -71.915190, T: 87500000, Avg. loss: 4.541150\n",
      "Total training time: 703.71 seconds.\n",
      "-- Epoch 351\n",
      "Norm: 16.60, NNZs: 3072, Bias: -107.648652, T: 87000000, Avg. loss: 6.306837\n",
      "Total training time: 704.92 seconds.\n",
      "-- Epoch 349\n",
      "Norm: 14.22, NNZs: 3072, Bias: -71.731636, T: 87750000, Avg. loss: 4.529498\n",
      "Total training time: 705.89 seconds.\n",
      "-- Epoch 352\n",
      "Norm: 16.59, NNZs: 3072, Bias: -107.475687, T: 87250000, Avg. loss: 6.296194\n",
      "Total training time: 707.10 seconds.\n",
      "-- Epoch 350\n",
      "Norm: 14.19, NNZs: 3072, Bias: -71.549021, T: 88000000, Avg. loss: 4.517935\n",
      "Total training time: 708.06 seconds.\n",
      "-- Epoch 353\n",
      "Norm: 16.56, NNZs: 3072, Bias: -107.304336, T: 87500000, Avg. loss: 6.286397\n",
      "Total training time: 709.25 seconds.\n",
      "-- Epoch 351\n",
      "Norm: 14.17, NNZs: 3072, Bias: -71.366727, T: 88250000, Avg. loss: 4.506051\n",
      "Total training time: 710.22 seconds.\n",
      "-- Epoch 354\n",
      "Norm: 16.55, NNZs: 3072, Bias: -107.132910, T: 87750000, Avg. loss: 6.276009\n",
      "Total training time: 711.41 seconds.\n",
      "-- Epoch 352\n",
      "Norm: 14.15, NNZs: 3072, Bias: -71.185186, T: 88500000, Avg. loss: 4.494651\n",
      "Total training time: 712.39 seconds.\n",
      "-- Epoch 355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 16.53, NNZs: 3072, Bias: -106.961818, T: 88000000, Avg. loss: 6.266032\n",
      "Total training time: 713.58 seconds.\n",
      "-- Epoch 353\n",
      "Norm: 14.13, NNZs: 3072, Bias: -71.004067, T: 88750000, Avg. loss: 4.483196\n",
      "Total training time: 714.56 seconds.\n",
      "-- Epoch 356\n",
      "Norm: 16.51, NNZs: 3072, Bias: -106.791637, T: 88250000, Avg. loss: 6.255889\n",
      "Total training time: 715.76 seconds.\n",
      "-- Epoch 354\n",
      "Norm: 14.11, NNZs: 3072, Bias: -70.823944, T: 89000000, Avg. loss: 4.471569\n",
      "Total training time: 716.72 seconds.\n",
      "-- Epoch 357\n",
      "Norm: 16.50, NNZs: 3072, Bias: -106.621083, T: 88500000, Avg. loss: 6.245751\n",
      "Total training time: 717.94 seconds.\n",
      "-- Epoch 355\n",
      "Norm: 14.09, NNZs: 3072, Bias: -70.643535, T: 89250000, Avg. loss: 4.460144\n",
      "Total training time: 718.89 seconds.\n",
      "-- Epoch 358\n",
      "Norm: 16.48, NNZs: 3072, Bias: -106.451560, T: 88750000, Avg. loss: 6.235641\n",
      "Total training time: 720.14 seconds.\n",
      "-- Epoch 356\n",
      "Norm: 14.06, NNZs: 3072, Bias: -70.464062, T: 89500000, Avg. loss: 4.448591\n",
      "Total training time: 721.06 seconds.\n",
      "-- Epoch 359\n",
      "Norm: 16.47, NNZs: 3072, Bias: -106.282353, T: 89000000, Avg. loss: 6.225735\n",
      "Total training time: 722.30 seconds.\n",
      "-- Epoch 357\n",
      "Norm: 14.04, NNZs: 3072, Bias: -70.284865, T: 89750000, Avg. loss: 4.437254\n",
      "Total training time: 723.22 seconds.\n",
      "-- Epoch 360\n",
      "Norm: 16.45, NNZs: 3072, Bias: -106.113941, T: 89250000, Avg. loss: 6.215533\n",
      "Total training time: 724.45 seconds.\n",
      "-- Epoch 358\n",
      "Norm: 14.02, NNZs: 3072, Bias: -70.106784, T: 90000000, Avg. loss: 4.425761\n",
      "Total training time: 725.40 seconds.\n",
      "-- Epoch 361\n",
      "Norm: 16.44, NNZs: 3072, Bias: -105.945196, T: 89500000, Avg. loss: 6.205537\n",
      "Total training time: 726.71 seconds.\n",
      "-- Epoch 359\n",
      "Norm: 14.00, NNZs: 3072, Bias: -69.928778, T: 90250000, Avg. loss: 4.414491\n",
      "Total training time: 727.68 seconds.\n",
      "-- Epoch 362\n",
      "Norm: 16.42, NNZs: 3072, Bias: -105.777995, T: 89750000, Avg. loss: 6.195887\n",
      "Total training time: 729.11 seconds.\n",
      "-- Epoch 360\n",
      "Norm: 13.98, NNZs: 3072, Bias: -69.751592, T: 90500000, Avg. loss: 4.402989\n",
      "Total training time: 730.10 seconds.\n",
      "-- Epoch 363\n",
      "Norm: 16.40, NNZs: 3072, Bias: -105.610542, T: 90000000, Avg. loss: 6.185846\n",
      "Total training time: 731.47 seconds.\n",
      "-- Epoch 361\n",
      "Norm: 13.96, NNZs: 3072, Bias: -69.574568, T: 90750000, Avg. loss: 4.391816\n",
      "Total training time: 732.43 seconds.\n",
      "-- Epoch 364\n",
      "Norm: 16.39, NNZs: 3072, Bias: -105.443398, T: 90250000, Avg. loss: 6.175360\n",
      "Total training time: 733.74 seconds.\n",
      "-- Epoch 362\n",
      "Norm: 13.94, NNZs: 3072, Bias: -69.397891, T: 91000000, Avg. loss: 4.380725\n",
      "Total training time: 734.69 seconds.\n",
      "-- Epoch 365\n",
      "Norm: 16.37, NNZs: 3072, Bias: -105.277974, T: 90500000, Avg. loss: 6.166138\n",
      "Total training time: 736.01 seconds.\n",
      "-- Epoch 363\n",
      "Norm: 13.91, NNZs: 3072, Bias: -69.222007, T: 91250000, Avg. loss: 4.369462\n",
      "Total training time: 736.94 seconds.\n",
      "-- Epoch 366\n",
      "Norm: 16.35, NNZs: 3072, Bias: -105.112054, T: 90750000, Avg. loss: 6.156253\n",
      "Total training time: 738.33 seconds.\n",
      "-- Epoch 364\n",
      "Norm: 13.89, NNZs: 3072, Bias: -69.046986, T: 91500000, Avg. loss: 4.358262\n",
      "Total training time: 739.22 seconds.\n",
      "-- Epoch 367\n",
      "Norm: 16.34, NNZs: 3072, Bias: -104.946881, T: 91000000, Avg. loss: 6.146585\n",
      "Total training time: 740.55 seconds.\n",
      "-- Epoch 365\n",
      "Norm: 13.87, NNZs: 3072, Bias: -68.871515, T: 91750000, Avg. loss: 4.347140\n",
      "Total training time: 741.43 seconds.\n",
      "-- Epoch 368\n",
      "Norm: 16.32, NNZs: 3072, Bias: -104.782307, T: 91250000, Avg. loss: 6.136486\n",
      "Total training time: 742.77 seconds.\n",
      "-- Epoch 366\n",
      "Norm: 13.85, NNZs: 3072, Bias: -68.697081, T: 92000000, Avg. loss: 4.335924\n",
      "Total training time: 743.69 seconds.\n",
      "-- Epoch 369\n",
      "Norm: 16.31, NNZs: 3072, Bias: -104.617660, T: 91500000, Avg. loss: 6.126966\n",
      "Total training time: 745.02 seconds.\n",
      "-- Epoch 367\n",
      "Norm: 13.83, NNZs: 3072, Bias: -68.523425, T: 92250000, Avg. loss: 4.324628\n",
      "Total training time: 745.89 seconds.\n",
      "-- Epoch 370\n",
      "Norm: 16.29, NNZs: 3072, Bias: -104.453944, T: 91750000, Avg. loss: 6.117212\n",
      "Total training time: 747.20 seconds.\n",
      "-- Epoch 368\n",
      "Norm: 13.81, NNZs: 3072, Bias: -68.349496, T: 92500000, Avg. loss: 4.313768\n",
      "Total training time: 748.06 seconds.\n",
      "-- Epoch 371\n",
      "Norm: 16.27, NNZs: 3072, Bias: -104.290917, T: 92000000, Avg. loss: 6.107453\n",
      "Total training time: 749.43 seconds.\n",
      "-- Epoch 369\n",
      "Norm: 13.79, NNZs: 3072, Bias: -68.176753, T: 92750000, Avg. loss: 4.302331\n",
      "Total training time: 750.33 seconds.\n",
      "-- Epoch 372\n",
      "Norm: 16.26, NNZs: 3072, Bias: -104.127252, T: 92250000, Avg. loss: 6.097959\n",
      "Total training time: 751.67 seconds.\n",
      "-- Epoch 370\n",
      "Norm: 13.76, NNZs: 3072, Bias: -68.004474, T: 93000000, Avg. loss: 4.291981\n",
      "Total training time: 752.52 seconds.\n",
      "-- Epoch 373\n",
      "Norm: 16.24, NNZs: 3072, Bias: -103.964943, T: 92500000, Avg. loss: 6.088179\n",
      "Total training time: 753.86 seconds.\n",
      "Convergence after 370 epochs took 753.86 seconds\n",
      "Norm: 13.75, NNZs: 3072, Bias: -67.832019, T: 93250000, Avg. loss: 4.280803\n",
      "Total training time: 754.69 seconds.\n",
      "-- Epoch 374\n",
      "Norm: 13.72, NNZs: 3072, Bias: -67.660814, T: 93500000, Avg. loss: 4.269931\n",
      "Total training time: 756.81 seconds.\n",
      "-- Epoch 375\n",
      "Norm: 13.70, NNZs: 3072, Bias: -67.489558, T: 93750000, Avg. loss: 4.259073\n",
      "Total training time: 758.93 seconds.\n",
      "-- Epoch 376\n",
      "Norm: 13.68, NNZs: 3072, Bias: -67.319744, T: 94000000, Avg. loss: 4.248001\n",
      "Total training time: 761.06 seconds.\n",
      "-- Epoch 377\n",
      "Norm: 13.66, NNZs: 3072, Bias: -67.149138, T: 94250000, Avg. loss: 4.237192\n",
      "Total training time: 763.21 seconds.\n",
      "-- Epoch 378\n",
      "Norm: 13.64, NNZs: 3072, Bias: -66.979153, T: 94500000, Avg. loss: 4.226295\n",
      "Total training time: 765.37 seconds.\n",
      "-- Epoch 379\n",
      "Norm: 13.62, NNZs: 3072, Bias: -66.810854, T: 94750000, Avg. loss: 4.215536\n",
      "Total training time: 767.48 seconds.\n",
      "-- Epoch 380\n",
      "Norm: 13.60, NNZs: 3072, Bias: -66.641937, T: 95000000, Avg. loss: 4.204945\n",
      "Total training time: 769.64 seconds.\n",
      "-- Epoch 381\n",
      "Norm: 13.58, NNZs: 3072, Bias: -66.473189, T: 95250000, Avg. loss: 4.194261\n",
      "Total training time: 771.78 seconds.\n",
      "-- Epoch 382\n",
      "Norm: 13.55, NNZs: 3072, Bias: -66.306282, T: 95500000, Avg. loss: 4.183154\n",
      "Total training time: 773.92 seconds.\n",
      "-- Epoch 383\n",
      "Norm: 13.54, NNZs: 3072, Bias: -66.138022, T: 95750000, Avg. loss: 4.172611\n",
      "Total training time: 776.10 seconds.\n",
      "-- Epoch 384\n",
      "Norm: 13.52, NNZs: 3072, Bias: -65.971169, T: 96000000, Avg. loss: 4.162214\n",
      "Total training time: 778.20 seconds.\n",
      "-- Epoch 385\n",
      "Norm: 13.49, NNZs: 3072, Bias: -65.804911, T: 96250000, Avg. loss: 4.151492\n",
      "Total training time: 780.38 seconds.\n",
      "-- Epoch 386\n",
      "Norm: 13.47, NNZs: 3072, Bias: -65.639118, T: 96500000, Avg. loss: 4.140891\n",
      "Total training time: 782.52 seconds.\n",
      "-- Epoch 387\n",
      "Norm: 13.45, NNZs: 3072, Bias: -65.473380, T: 96750000, Avg. loss: 4.130566\n",
      "Total training time: 784.66 seconds.\n",
      "-- Epoch 388\n",
      "Norm: 13.43, NNZs: 3072, Bias: -65.308564, T: 97000000, Avg. loss: 4.119931\n",
      "Total training time: 786.77 seconds.\n",
      "-- Epoch 389\n",
      "Norm: 13.41, NNZs: 3072, Bias: -65.143895, T: 97250000, Avg. loss: 4.109433\n",
      "Total training time: 788.91 seconds.\n",
      "-- Epoch 390\n",
      "Norm: 13.39, NNZs: 3072, Bias: -64.979152, T: 97500000, Avg. loss: 4.098944\n",
      "Total training time: 791.06 seconds.\n",
      "-- Epoch 391\n",
      "Norm: 13.37, NNZs: 3072, Bias: -64.815793, T: 97750000, Avg. loss: 4.088569\n",
      "Total training time: 793.18 seconds.\n",
      "-- Epoch 392\n",
      "Norm: 13.35, NNZs: 3072, Bias: -64.652144, T: 98000000, Avg. loss: 4.078124\n",
      "Total training time: 795.33 seconds.\n",
      "-- Epoch 393\n",
      "Norm: 13.33, NNZs: 3072, Bias: -64.489070, T: 98250000, Avg. loss: 4.067752\n",
      "Total training time: 797.42 seconds.\n",
      "-- Epoch 394\n",
      "Norm: 13.31, NNZs: 3072, Bias: -64.326799, T: 98500000, Avg. loss: 4.057316\n",
      "Total training time: 799.51 seconds.\n",
      "-- Epoch 395\n",
      "Norm: 13.29, NNZs: 3072, Bias: -64.164634, T: 98750000, Avg. loss: 4.047244\n",
      "Total training time: 801.62 seconds.\n",
      "-- Epoch 396\n",
      "Norm: 13.27, NNZs: 3072, Bias: -64.003464, T: 99000000, Avg. loss: 4.036885\n",
      "Total training time: 803.72 seconds.\n",
      "-- Epoch 397\n",
      "Norm: 13.25, NNZs: 3072, Bias: -63.842009, T: 99250000, Avg. loss: 4.026613\n",
      "Total training time: 805.78 seconds.\n",
      "-- Epoch 398\n",
      "Norm: 13.23, NNZs: 3072, Bias: -63.681280, T: 99500000, Avg. loss: 4.016287\n",
      "Total training time: 807.92 seconds.\n",
      "-- Epoch 399\n",
      "Norm: 13.21, NNZs: 3072, Bias: -63.520490, T: 99750000, Avg. loss: 4.006236\n",
      "Total training time: 810.09 seconds.\n",
      "-- Epoch 400\n",
      "Norm: 13.19, NNZs: 3072, Bias: -63.361176, T: 100000000, Avg. loss: 3.995771\n",
      "Total training time: 812.24 seconds.\n",
      "-- Epoch 401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 13.17, NNZs: 3072, Bias: -63.201447, T: 100250000, Avg. loss: 3.985762\n",
      "Total training time: 814.44 seconds.\n",
      "-- Epoch 402\n",
      "Norm: 13.15, NNZs: 3072, Bias: -63.041904, T: 100500000, Avg. loss: 3.975641\n",
      "Total training time: 816.59 seconds.\n",
      "-- Epoch 403\n",
      "Norm: 13.13, NNZs: 3072, Bias: -62.883727, T: 100750000, Avg. loss: 3.965406\n",
      "Total training time: 818.73 seconds.\n",
      "-- Epoch 404\n",
      "Norm: 13.11, NNZs: 3072, Bias: -62.724748, T: 101000000, Avg. loss: 3.955441\n",
      "Total training time: 820.88 seconds.\n",
      "-- Epoch 405\n",
      "Norm: 13.09, NNZs: 3072, Bias: -62.567044, T: 101250000, Avg. loss: 3.945362\n",
      "Total training time: 823.07 seconds.\n",
      "-- Epoch 406\n",
      "Norm: 13.07, NNZs: 3072, Bias: -62.409078, T: 101500000, Avg. loss: 3.935182\n",
      "Total training time: 825.14 seconds.\n",
      "-- Epoch 407\n",
      "Norm: 13.05, NNZs: 3072, Bias: -62.253301, T: 101750000, Avg. loss: 3.925203\n",
      "Total training time: 827.21 seconds.\n",
      "-- Epoch 408\n",
      "Norm: 13.03, NNZs: 3072, Bias: -62.096070, T: 102000000, Avg. loss: 3.915525\n",
      "Total training time: 829.31 seconds.\n",
      "-- Epoch 409\n",
      "Norm: 13.01, NNZs: 3072, Bias: -61.939560, T: 102250000, Avg. loss: 3.905555\n",
      "Total training time: 831.43 seconds.\n",
      "-- Epoch 410\n",
      "Norm: 12.99, NNZs: 3072, Bias: -61.784311, T: 102500000, Avg. loss: 3.895491\n",
      "Total training time: 833.54 seconds.\n",
      "-- Epoch 411\n",
      "Norm: 12.97, NNZs: 3072, Bias: -61.628118, T: 102750000, Avg. loss: 3.885528\n",
      "Total training time: 835.72 seconds.\n",
      "-- Epoch 412\n",
      "Norm: 12.95, NNZs: 3072, Bias: -61.472670, T: 103000000, Avg. loss: 3.875736\n",
      "Total training time: 837.90 seconds.\n",
      "-- Epoch 413\n",
      "Norm: 12.93, NNZs: 3072, Bias: -61.318109, T: 103250000, Avg. loss: 3.865778\n",
      "Total training time: 840.05 seconds.\n",
      "-- Epoch 414\n",
      "Norm: 12.91, NNZs: 3072, Bias: -61.164490, T: 103500000, Avg. loss: 3.856053\n",
      "Total training time: 842.22 seconds.\n",
      "-- Epoch 415\n",
      "Norm: 12.89, NNZs: 3072, Bias: -61.010310, T: 103750000, Avg. loss: 3.846367\n",
      "Total training time: 844.36 seconds.\n",
      "Convergence after 415 epochs took 844.36 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   3 out of   3 | elapsed: 14.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svmScore score :% 79.84\n"
     ]
    }
   ],
   "source": [
    "svm, svmScore = buildmodel('svm',Xmlp,Ycatdog,x_testmlp,y_test_catdog,alpha=0.001, max_iter=500, \n",
    "                           tol=.01,verbose=1,n_jobs=4,loss=\"log\",n_iter_no_change =5)\n",
    "print(\"svmScore score :%\",round(svmScore,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 Model A\n",
    "Cats and Dogs- Naive Bayes, Multi-layer Perceptron classifier and Support Vector Machine\n",
    "    \n",
    "We chose several implementations of the Naive Bayes classifier: GaussianNB, MultinomialNB, and BernoulliNB.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Naive Bayes model, while MultinomialNB and GaussianNB scores were relatively similar(54.28% and 54.31%, respectively), BernoulliNB  was higher at 77.17%.  \n",
    "The Multi-layer Perceptron classifier resulted in 80% which surpasses Naive Bayes.\n",
    "The Support Vector Machine returned a 79.7%.\n",
    "Being that BernoulliNB, Multi-layer Perceptron classifier and Support Vector Machine are all very similar in score, other factors will be taken into consideration when finding the optimal model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task2 Model A\n",
    "(Birds and Airplanes- Naive Bayes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task2 Model B\n",
    "(Birds and Airplanes- Decision Tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task2 Model C\n",
    "(Birds and Airplanes- SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Analyze the results using your chosen method of evaluation. Use visualizations of the results to bolster the analysis. Explain any visuals and analyze why they are interesting to someone that might use this model.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantages of Task 1: Cats and dogs Model A: Naive Bayes\n",
    "\n",
    "    *\n",
    "Advantages of Task 1: Cats and dogsModel B: Multi-layer Perceptron classifier (MLP)\n",
    "\n",
    "    *\n",
    "Advantages of Task 1: Cats and dogsModel C: Support Vector Machine (SVM)\n",
    "\n",
    "    *\n",
    "Advantages of Task 2: Birds and airplanes Model A: Naive Bayes\n",
    "\n",
    "    *\n",
    "Advantages of Task 2: Birds and airplanes Model B: Decision Tree\n",
    "\n",
    "    *\n",
    "Advantages of Task 2: Birds and airplanes Model C: SVM\n",
    "\n",
    "    *\n",
    "\n",
    "\"Discuss the advantages of each model for each classification task, if any. If there are not advantages, explain why. Is any model better than another? Is the difference significant with 95% confidence? Use proper statistical comparison methods. You must use statistical comparison techniquesbe sure they are appropriate for your chosen method of validation as discussed in unit 7 of the course.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Which attributes from your analysis are most important? Use proper methods discussed in class to evaluate the importance of different attributes. Discuss the results and hypothesize about why certain attributes are more important than others for a given classication task. Attributes are pixels representing a range of values 0-255 for each pixel. Together they represent a coefficients or loadings that together build an image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 255)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Range of values in our elements\n",
    "np.min(X),np.max(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:595: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "#We scale the data due to large ranges in values from 0-255\n",
    "scaler = StandardScaler()\n",
    "# Fit on training set only.\n",
    "scaler.fit(X)\n",
    "# Apply transform to both the training set\n",
    "X_trans = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-2.2025231485140195, 2.6196548440090894)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#new scaled range\n",
    "np.min(X_trans),np.max(X_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel 0</th>\n",
       "      <th>pixel 1</th>\n",
       "      <th>pixel 2</th>\n",
       "      <th>pixel 3</th>\n",
       "      <th>pixel 4</th>\n",
       "      <th>pixel 5</th>\n",
       "      <th>pixel 6</th>\n",
       "      <th>pixel 7</th>\n",
       "      <th>pixel 8</th>\n",
       "      <th>pixel 9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel 3063</th>\n",
       "      <th>pixel 3064</th>\n",
       "      <th>pixel 3065</th>\n",
       "      <th>pixel 3066</th>\n",
       "      <th>pixel 3067</th>\n",
       "      <th>pixel 3068</th>\n",
       "      <th>pixel 3069</th>\n",
       "      <th>pixel 3070</th>\n",
       "      <th>pixel 3071</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.225223</td>\n",
       "      <td>-0.736220</td>\n",
       "      <td>-0.657766</td>\n",
       "      <td>-0.446950</td>\n",
       "      <td>-0.091080</td>\n",
       "      <td>0.156224</td>\n",
       "      <td>0.391895</td>\n",
       "      <td>0.458910</td>\n",
       "      <td>0.501801</td>\n",
       "      <td>0.497307</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.443551</td>\n",
       "      <td>-0.348145</td>\n",
       "      <td>-0.430406</td>\n",
       "      <td>-0.608322</td>\n",
       "      <td>-0.458606</td>\n",
       "      <td>0.181267</td>\n",
       "      <td>0.668683</td>\n",
       "      <td>-0.094181</td>\n",
       "      <td>0.047960</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.889738</td>\n",
       "      <td>0.261446</td>\n",
       "      <td>0.004876</td>\n",
       "      <td>-0.037073</td>\n",
       "      <td>0.234552</td>\n",
       "      <td>0.590383</td>\n",
       "      <td>0.789757</td>\n",
       "      <td>0.880676</td>\n",
       "      <td>0.417485</td>\n",
       "      <td>0.039676</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.662666</td>\n",
       "      <td>-0.320783</td>\n",
       "      <td>0.143789</td>\n",
       "      <td>0.429843</td>\n",
       "      <td>0.578210</td>\n",
       "      <td>0.617075</td>\n",
       "      <td>0.655099</td>\n",
       "      <td>0.689827</td>\n",
       "      <td>1.001560</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.075118</td>\n",
       "      <td>1.787994</td>\n",
       "      <td>1.787985</td>\n",
       "      <td>1.783263</td>\n",
       "      <td>1.778289</td>\n",
       "      <td>1.772261</td>\n",
       "      <td>1.766327</td>\n",
       "      <td>1.760360</td>\n",
       "      <td>1.754502</td>\n",
       "      <td>1.749770</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.101185</td>\n",
       "      <td>-0.142927</td>\n",
       "      <td>-0.293693</td>\n",
       "      <td>-0.335121</td>\n",
       "      <td>-0.253972</td>\n",
       "      <td>-0.159208</td>\n",
       "      <td>-0.105605</td>\n",
       "      <td>-0.107699</td>\n",
       "      <td>0.206893</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.589053</td>\n",
       "      <td>-0.808340</td>\n",
       "      <td>-0.802343</td>\n",
       "      <td>-0.760386</td>\n",
       "      <td>-0.742344</td>\n",
       "      <td>-0.796514</td>\n",
       "      <td>-0.801690</td>\n",
       "      <td>-0.999195</td>\n",
       "      <td>-0.907488</td>\n",
       "      <td>-0.779242</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.703750</td>\n",
       "      <td>-0.430232</td>\n",
       "      <td>-0.662819</td>\n",
       "      <td>-0.635642</td>\n",
       "      <td>-0.581387</td>\n",
       "      <td>-0.717586</td>\n",
       "      <td>-0.852725</td>\n",
       "      <td>-0.729498</td>\n",
       "      <td>-0.296396</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.077521</td>\n",
       "      <td>0.766289</td>\n",
       "      <td>0.872334</td>\n",
       "      <td>0.939399</td>\n",
       "      <td>0.909937</td>\n",
       "      <td>0.855703</td>\n",
       "      <td>0.898265</td>\n",
       "      <td>0.928878</td>\n",
       "      <td>0.983609</td>\n",
       "      <td>0.979023</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.032712</td>\n",
       "      <td>-0.074521</td>\n",
       "      <td>-0.115966</td>\n",
       "      <td>-0.102900</td>\n",
       "      <td>-0.158475</td>\n",
       "      <td>-0.172827</td>\n",
       "      <td>-0.119189</td>\n",
       "      <td>-0.175286</td>\n",
       "      <td>0.153915</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  3073 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    pixel 0   pixel 1   pixel 2   pixel 3   pixel 4   pixel 5   pixel 6  \\\n",
       "0 -0.225223 -0.736220 -0.657766 -0.446950 -0.091080  0.156224  0.391895   \n",
       "1  0.889738  0.261446  0.004876 -0.037073  0.234552  0.590383  0.789757   \n",
       "2  2.075118  1.787994  1.787985  1.783263  1.778289  1.772261  1.766327   \n",
       "3 -0.589053 -0.808340 -0.802343 -0.760386 -0.742344 -0.796514 -0.801690   \n",
       "4  1.077521  0.766289  0.872334  0.939399  0.909937  0.855703  0.898265   \n",
       "\n",
       "    pixel 7   pixel 8   pixel 9  ...  pixel 3063  pixel 3064  pixel 3065  \\\n",
       "0  0.458910  0.501801  0.497307  ...   -0.443551   -0.348145   -0.430406   \n",
       "1  0.880676  0.417485  0.039676  ...   -0.662666   -0.320783    0.143789   \n",
       "2  1.760360  1.754502  1.749770  ...   -0.101185   -0.142927   -0.293693   \n",
       "3 -0.999195 -0.907488 -0.779242  ...   -0.703750   -0.430232   -0.662819   \n",
       "4  0.928878  0.983609  0.979023  ...   -0.032712   -0.074521   -0.115966   \n",
       "\n",
       "   pixel 3066  pixel 3067  pixel 3068  pixel 3069  pixel 3070  pixel 3071  \\\n",
       "0   -0.608322   -0.458606    0.181267    0.668683   -0.094181    0.047960   \n",
       "1    0.429843    0.578210    0.617075    0.655099    0.689827    1.001560   \n",
       "2   -0.335121   -0.253972   -0.159208   -0.105605   -0.107699    0.206893   \n",
       "3   -0.635642   -0.581387   -0.717586   -0.852725   -0.729498   -0.296396   \n",
       "4   -0.102900   -0.158475   -0.172827   -0.119189   -0.175286    0.153915   \n",
       "\n",
       "   label  \n",
       "0      6  \n",
       "1      9  \n",
       "2      9  \n",
       "3      4  \n",
       "4      1  \n",
       "\n",
       "[5 rows x 3073 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We add attribute names to each of our columns\n",
    "feat_cols = ['pixel '+str(i) for i in range(X_trans.shape[1])]\n",
    "#Create dataframe for our scaled elements and incorporate column names\n",
    "df_cifar = pd.DataFrame(X_trans,columns=feat_cols)\n",
    "df_cifar['label'] = Y\n",
    "#preview \n",
    "df_cifar.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#break into two new components, prin1 and prin2\n",
    "pca_cifar = PCA(n_components=2)\n",
    "principalComponents_cifar = pca_cifar.fit_transform(X_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add labels\n",
    "principal_cifar_Df = pd.DataFrame(data = principalComponents_cifar\n",
    "             , columns = ['prin 1', 'prin 2'])\n",
    "principal_cifar_Df['y'] = Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variation per principal component: [0.24495932 0.1067251 ]\n"
     ]
    }
   ],
   "source": [
    "#percentage of variance explained\n",
    "print('Explained variation per principal component: {}'.format(pca_cifar.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prin 1</th>\n",
       "      <th>prin 2</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-19.909476</td>\n",
       "      <td>12.550583</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.236405</td>\n",
       "      <td>-4.361443</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21.635929</td>\n",
       "      <td>-46.976388</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-35.820946</td>\n",
       "      <td>2.671023</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-13.659772</td>\n",
       "      <td>-15.108336</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      prin 1     prin 2  y\n",
       "0 -19.909476  12.550583  6\n",
       "1   7.236405  -4.361443  9\n",
       "2  21.635929 -46.976388  9\n",
       "3 -35.820946   2.671023  4\n",
       "4 -13.659772 -15.108336  1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "principal_cifar_Df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We believe that our model would be best suited for\n",
    "\n",
    "\"How useful is your model for interested parties (i.e., the companies or organizations that might want to use it for prediction)? How would you measure the model's value if it was used by these parties? How would your deploy your model for interested parties? What other data should be collected? How often would the model need to be updated, etc.?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OLD- MAY DELETE -Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape:  (50000, 3072)\n",
      "Labels shape:  (50000,)\n",
      "Test Data shape:  (10000, 3072)\n",
      "Test Labels shape:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Build Logistic Regression Model with Stochastic Gradient Descent \n",
    "X,Y = load_cfar10_batch(\"data/\",1,False)\n",
    "\n",
    "for n in range(2,6):\n",
    "    x,y = load_cfar10_batch(\"data/\",n,False)    \n",
    "    X = np.concatenate((X,x),axis=0)\n",
    "    Y = np.concatenate((Y,y),axis=0)\n",
    "\n",
    "test_X,test_Y = load_cfar10_batch(\"data/test_batch\",None,False)\n",
    "\n",
    "# Modify the dataset labels to lable cats/not/cats\n",
    "#(if_test_is_false, if_test_is_true)[test]\n",
    "catY = (Y == 3)\n",
    "cat_test_Y = (test_Y == 3) \n",
    "\n",
    "sgdlr = SGDClassifier(alpha=0.001, max_iter=5000, tol=1e-3,verbose=1,n_jobs=4,loss=\"log\")\n",
    "\n",
    "print(\"Data shape: \",X.shape)\n",
    "print(\"Labels shape: \",catY.shape)\n",
    "\n",
    "print(\"Test Data shape: \",test_X.shape)\n",
    "print(\"Test Labels shape: \",cat_test_Y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 2028.12, NNZs: 3072, Bias: -4.621707, T: 50000, Avg. loss: 518923.052277\n",
      "Total training time: 0.26 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1318.44, NNZs: 3072, Bias: -5.129228, T: 100000, Avg. loss: 61945.797956\n",
      "Total training time: 0.52 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1017.41, NNZs: 3072, Bias: -5.601844, T: 150000, Avg. loss: 36642.335273\n",
      "Total training time: 0.79 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 855.22, NNZs: 3072, Bias: -6.018181, T: 200000, Avg. loss: 25586.749852\n",
      "Total training time: 1.04 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 759.26, NNZs: 3072, Bias: -6.375292, T: 250000, Avg. loss: 20013.708300\n",
      "Total training time: 1.31 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 681.32, NNZs: 3072, Bias: -6.689832, T: 300000, Avg. loss: 16059.164443\n",
      "Total training time: 1.58 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 629.57, NNZs: 3072, Bias: -6.971775, T: 350000, Avg. loss: 13700.989168\n",
      "Total training time: 1.84 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 588.74, NNZs: 3072, Bias: -7.125530, T: 400000, Avg. loss: 11935.591543\n",
      "Total training time: 2.11 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 553.29, NNZs: 3072, Bias: -7.294762, T: 450000, Avg. loss: 10525.002829\n",
      "Total training time: 2.37 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 524.38, NNZs: 3072, Bias: -7.465839, T: 500000, Avg. loss: 9360.873525\n",
      "Total training time: 2.63 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 504.47, NNZs: 3072, Bias: -7.602253, T: 550000, Avg. loss: 8381.728467\n",
      "Total training time: 2.90 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 480.49, NNZs: 3072, Bias: -7.735608, T: 600000, Avg. loss: 7730.539698\n",
      "Total training time: 3.16 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 460.98, NNZs: 3072, Bias: -7.835540, T: 650000, Avg. loss: 7056.408810\n",
      "Total training time: 3.41 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 444.59, NNZs: 3072, Bias: -7.937050, T: 700000, Avg. loss: 6475.329829\n",
      "Total training time: 3.67 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 427.20, NNZs: 3072, Bias: -8.017994, T: 750000, Avg. loss: 6102.911964\n",
      "Total training time: 3.93 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 415.44, NNZs: 3072, Bias: -8.096927, T: 800000, Avg. loss: 5718.281669\n",
      "Total training time: 4.18 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 401.44, NNZs: 3072, Bias: -8.229748, T: 850000, Avg. loss: 5353.741779\n",
      "Total training time: 4.45 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 388.15, NNZs: 3072, Bias: -8.303714, T: 900000, Avg. loss: 5083.762156\n",
      "Total training time: 4.71 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 377.99, NNZs: 3072, Bias: -8.383903, T: 950000, Avg. loss: 4706.492850\n",
      "Total training time: 4.97 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 370.12, NNZs: 3072, Bias: -8.474576, T: 1000000, Avg. loss: 4498.250151\n",
      "Total training time: 5.23 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 359.36, NNZs: 3072, Bias: -8.556856, T: 1050000, Avg. loss: 4248.867148\n",
      "Total training time: 5.49 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 351.86, NNZs: 3072, Bias: -8.629475, T: 1100000, Avg. loss: 4080.963411\n",
      "Total training time: 5.74 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 345.48, NNZs: 3072, Bias: -8.686127, T: 1150000, Avg. loss: 3911.545757\n",
      "Total training time: 5.99 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 338.16, NNZs: 3072, Bias: -8.758009, T: 1200000, Avg. loss: 3734.358707\n",
      "Total training time: 6.24 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 331.38, NNZs: 3072, Bias: -8.833570, T: 1250000, Avg. loss: 3575.251249\n",
      "Total training time: 6.49 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 324.87, NNZs: 3072, Bias: -8.882714, T: 1300000, Avg. loss: 3459.639216\n",
      "Total training time: 6.74 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 316.15, NNZs: 3072, Bias: -8.927487, T: 1350000, Avg. loss: 3331.203185\n",
      "Total training time: 7.00 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 311.39, NNZs: 3072, Bias: -8.990452, T: 1400000, Avg. loss: 3203.825043\n",
      "Total training time: 7.27 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 306.43, NNZs: 3072, Bias: -9.065179, T: 1450000, Avg. loss: 3061.382866\n",
      "Total training time: 7.53 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 301.15, NNZs: 3072, Bias: -9.115985, T: 1500000, Avg. loss: 2979.384176\n",
      "Total training time: 7.80 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 296.76, NNZs: 3072, Bias: -9.167576, T: 1550000, Avg. loss: 2868.779635\n",
      "Total training time: 8.05 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 291.72, NNZs: 3072, Bias: -9.215686, T: 1600000, Avg. loss: 2754.353494\n",
      "Total training time: 8.31 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 286.48, NNZs: 3072, Bias: -9.248903, T: 1650000, Avg. loss: 2719.430539\n",
      "Total training time: 8.57 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 283.10, NNZs: 3072, Bias: -9.282110, T: 1700000, Avg. loss: 2588.941046\n",
      "Total training time: 8.84 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 280.02, NNZs: 3072, Bias: -9.309073, T: 1750000, Avg. loss: 2523.290959\n",
      "Total training time: 9.09 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 275.62, NNZs: 3072, Bias: -9.345915, T: 1800000, Avg. loss: 2462.589639\n",
      "Total training time: 9.35 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 271.55, NNZs: 3072, Bias: -9.403887, T: 1850000, Avg. loss: 2398.226943\n",
      "Total training time: 9.61 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 268.32, NNZs: 3072, Bias: -9.450776, T: 1900000, Avg. loss: 2324.032660\n",
      "Total training time: 9.86 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 264.45, NNZs: 3072, Bias: -9.468359, T: 1950000, Avg. loss: 2272.250487\n",
      "Total training time: 10.11 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 260.31, NNZs: 3072, Bias: -9.498897, T: 2000000, Avg. loss: 2226.842754\n",
      "Total training time: 10.36 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 256.92, NNZs: 3072, Bias: -9.533844, T: 2050000, Avg. loss: 2174.496991\n",
      "Total training time: 10.61 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 253.67, NNZs: 3072, Bias: -9.573604, T: 2100000, Avg. loss: 2097.806766\n",
      "Total training time: 10.87 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 251.29, NNZs: 3072, Bias: -9.608452, T: 2150000, Avg. loss: 2056.128011\n",
      "Total training time: 11.13 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 248.41, NNZs: 3072, Bias: -9.644281, T: 2200000, Avg. loss: 2015.493151\n",
      "Total training time: 11.38 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 245.56, NNZs: 3072, Bias: -9.677858, T: 2250000, Avg. loss: 1963.582638\n",
      "Total training time: 11.64 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 242.94, NNZs: 3072, Bias: -9.724793, T: 2300000, Avg. loss: 1906.177654\n",
      "Total training time: 11.89 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 240.34, NNZs: 3072, Bias: -9.756362, T: 2350000, Avg. loss: 1888.013969\n",
      "Total training time: 12.15 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 237.84, NNZs: 3072, Bias: -9.784549, T: 2400000, Avg. loss: 1844.664034\n",
      "Total training time: 12.40 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 235.08, NNZs: 3072, Bias: -9.814539, T: 2450000, Avg. loss: 1784.755746\n",
      "Total training time: 12.65 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 233.15, NNZs: 3072, Bias: -9.848421, T: 2500000, Avg. loss: 1767.520462\n",
      "Total training time: 12.90 seconds.\n",
      "-- Epoch 51\n",
      "Norm: 231.00, NNZs: 3072, Bias: -9.866481, T: 2550000, Avg. loss: 1727.561458\n",
      "Total training time: 13.16 seconds.\n",
      "-- Epoch 52\n",
      "Norm: 229.17, NNZs: 3072, Bias: -9.893149, T: 2600000, Avg. loss: 1687.070637\n",
      "Total training time: 13.41 seconds.\n",
      "-- Epoch 53\n",
      "Norm: 227.15, NNZs: 3072, Bias: -9.919019, T: 2650000, Avg. loss: 1673.793057\n",
      "Total training time: 13.66 seconds.\n",
      "-- Epoch 54\n",
      "Norm: 224.85, NNZs: 3072, Bias: -9.949089, T: 2700000, Avg. loss: 1626.818680\n",
      "Total training time: 13.92 seconds.\n",
      "-- Epoch 55\n",
      "Norm: 222.24, NNZs: 3072, Bias: -9.970743, T: 2750000, Avg. loss: 1591.524571\n",
      "Total training time: 14.17 seconds.\n",
      "-- Epoch 56\n",
      "Norm: 220.88, NNZs: 3072, Bias: -9.999856, T: 2800000, Avg. loss: 1557.635031\n",
      "Total training time: 14.42 seconds.\n",
      "-- Epoch 57\n",
      "Norm: 218.96, NNZs: 3072, Bias: -10.021926, T: 2850000, Avg. loss: 1558.906946\n",
      "Total training time: 14.68 seconds.\n",
      "-- Epoch 58\n",
      "Norm: 217.63, NNZs: 3072, Bias: -10.050384, T: 2900000, Avg. loss: 1502.834302\n",
      "Total training time: 14.93 seconds.\n",
      "-- Epoch 59\n",
      "Norm: 216.18, NNZs: 3072, Bias: -10.079045, T: 2950000, Avg. loss: 1474.321952\n",
      "Total training time: 15.19 seconds.\n",
      "-- Epoch 60\n",
      "Norm: 214.32, NNZs: 3072, Bias: -10.105618, T: 3000000, Avg. loss: 1448.352818\n",
      "Total training time: 15.44 seconds.\n",
      "-- Epoch 61\n",
      "Norm: 212.66, NNZs: 3072, Bias: -10.124852, T: 3050000, Avg. loss: 1437.055824\n",
      "Total training time: 15.70 seconds.\n",
      "-- Epoch 62\n",
      "Norm: 210.67, NNZs: 3072, Bias: -10.143176, T: 3100000, Avg. loss: 1420.414201\n",
      "Total training time: 15.95 seconds.\n",
      "-- Epoch 63\n",
      "Norm: 209.03, NNZs: 3072, Bias: -10.177392, T: 3150000, Avg. loss: 1376.651860\n",
      "Total training time: 16.21 seconds.\n",
      "-- Epoch 64\n",
      "Norm: 207.02, NNZs: 3072, Bias: -10.212510, T: 3200000, Avg. loss: 1379.771903\n",
      "Total training time: 16.46 seconds.\n",
      "-- Epoch 65\n",
      "Norm: 205.50, NNZs: 3072, Bias: -10.245546, T: 3250000, Avg. loss: 1351.035508\n",
      "Total training time: 16.72 seconds.\n",
      "-- Epoch 66\n",
      "Norm: 204.08, NNZs: 3072, Bias: -10.270085, T: 3300000, Avg. loss: 1335.557009\n",
      "Total training time: 16.98 seconds.\n",
      "-- Epoch 67\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 202.98, NNZs: 3072, Bias: -10.291214, T: 3350000, Avg. loss: 1314.678264\n",
      "Total training time: 17.23 seconds.\n",
      "-- Epoch 68\n",
      "Norm: 201.43, NNZs: 3072, Bias: -10.312061, T: 3400000, Avg. loss: 1284.197115\n",
      "Total training time: 17.50 seconds.\n",
      "-- Epoch 69\n",
      "Norm: 200.23, NNZs: 3072, Bias: -10.337139, T: 3450000, Avg. loss: 1269.183927\n",
      "Total training time: 17.76 seconds.\n",
      "-- Epoch 70\n",
      "Norm: 198.44, NNZs: 3072, Bias: -10.358921, T: 3500000, Avg. loss: 1253.820921\n",
      "Total training time: 18.02 seconds.\n",
      "-- Epoch 71\n",
      "Norm: 196.59, NNZs: 3072, Bias: -10.382227, T: 3550000, Avg. loss: 1251.795347\n",
      "Total training time: 18.28 seconds.\n",
      "-- Epoch 72\n",
      "Norm: 195.71, NNZs: 3072, Bias: -10.396134, T: 3600000, Avg. loss: 1210.465651\n",
      "Total training time: 18.53 seconds.\n",
      "-- Epoch 73\n",
      "Norm: 194.16, NNZs: 3072, Bias: -10.410799, T: 3650000, Avg. loss: 1196.380926\n",
      "Total training time: 18.79 seconds.\n",
      "-- Epoch 74\n",
      "Norm: 193.00, NNZs: 3072, Bias: -10.434616, T: 3700000, Avg. loss: 1177.403149\n",
      "Total training time: 19.05 seconds.\n",
      "-- Epoch 75\n",
      "Norm: 192.01, NNZs: 3072, Bias: -10.457953, T: 3750000, Avg. loss: 1170.166583\n",
      "Total training time: 19.31 seconds.\n",
      "-- Epoch 76\n",
      "Norm: 190.66, NNZs: 3072, Bias: -10.481053, T: 3800000, Avg. loss: 1165.788033\n",
      "Total training time: 19.57 seconds.\n",
      "-- Epoch 77\n",
      "Norm: 189.60, NNZs: 3072, Bias: -10.507126, T: 3850000, Avg. loss: 1145.378751\n",
      "Total training time: 19.82 seconds.\n",
      "-- Epoch 78\n",
      "Norm: 188.60, NNZs: 3072, Bias: -10.525448, T: 3900000, Avg. loss: 1119.550008\n",
      "Total training time: 20.08 seconds.\n",
      "-- Epoch 79\n",
      "Norm: 187.16, NNZs: 3072, Bias: -10.545414, T: 3950000, Avg. loss: 1110.553378\n",
      "Total training time: 20.34 seconds.\n",
      "-- Epoch 80\n",
      "Norm: 185.72, NNZs: 3072, Bias: -10.571826, T: 4000000, Avg. loss: 1087.047382\n",
      "Total training time: 20.60 seconds.\n",
      "-- Epoch 81\n",
      "Norm: 184.97, NNZs: 3072, Bias: -10.593866, T: 4050000, Avg. loss: 1078.832183\n",
      "Total training time: 20.85 seconds.\n",
      "-- Epoch 82\n",
      "Norm: 183.74, NNZs: 3072, Bias: -10.613590, T: 4100000, Avg. loss: 1068.603980\n",
      "Total training time: 21.11 seconds.\n",
      "-- Epoch 83\n",
      "Norm: 182.76, NNZs: 3072, Bias: -10.632199, T: 4150000, Avg. loss: 1061.232655\n",
      "Total training time: 21.36 seconds.\n",
      "-- Epoch 84\n",
      "Norm: 181.82, NNZs: 3072, Bias: -10.647141, T: 4200000, Avg. loss: 1044.027936\n",
      "Total training time: 21.62 seconds.\n",
      "-- Epoch 85\n",
      "Norm: 180.95, NNZs: 3072, Bias: -10.666605, T: 4250000, Avg. loss: 1030.889709\n",
      "Total training time: 21.87 seconds.\n",
      "-- Epoch 86\n",
      "Norm: 179.98, NNZs: 3072, Bias: -10.679879, T: 4300000, Avg. loss: 1013.811383\n",
      "Total training time: 22.13 seconds.\n",
      "-- Epoch 87\n",
      "Norm: 178.78, NNZs: 3072, Bias: -10.700696, T: 4350000, Avg. loss: 1009.341863\n",
      "Total training time: 22.38 seconds.\n",
      "-- Epoch 88\n",
      "Norm: 177.95, NNZs: 3072, Bias: -10.718648, T: 4400000, Avg. loss: 997.115993\n",
      "Total training time: 22.64 seconds.\n",
      "-- Epoch 89\n",
      "Norm: 176.73, NNZs: 3072, Bias: -10.737568, T: 4450000, Avg. loss: 985.755195\n",
      "Total training time: 22.90 seconds.\n",
      "-- Epoch 90\n",
      "Norm: 175.77, NNZs: 3072, Bias: -10.752895, T: 4500000, Avg. loss: 972.025444\n",
      "Total training time: 23.15 seconds.\n",
      "-- Epoch 91\n",
      "Norm: 174.87, NNZs: 3072, Bias: -10.774165, T: 4550000, Avg. loss: 958.942908\n",
      "Total training time: 23.40 seconds.\n",
      "-- Epoch 92\n",
      "Norm: 173.88, NNZs: 3072, Bias: -10.792213, T: 4600000, Avg. loss: 941.332393\n",
      "Total training time: 23.66 seconds.\n",
      "-- Epoch 93\n",
      "Norm: 172.85, NNZs: 3072, Bias: -10.817031, T: 4650000, Avg. loss: 939.268279\n",
      "Total training time: 23.92 seconds.\n",
      "-- Epoch 94\n",
      "Norm: 172.17, NNZs: 3072, Bias: -10.830666, T: 4700000, Avg. loss: 926.567454\n",
      "Total training time: 24.18 seconds.\n",
      "-- Epoch 95\n",
      "Norm: 171.05, NNZs: 3072, Bias: -10.845402, T: 4750000, Avg. loss: 919.158124\n",
      "Total training time: 24.44 seconds.\n",
      "-- Epoch 96\n",
      "Norm: 170.04, NNZs: 3072, Bias: -10.861688, T: 4800000, Avg. loss: 912.791253\n",
      "Total training time: 24.70 seconds.\n",
      "-- Epoch 97\n",
      "Norm: 168.85, NNZs: 3072, Bias: -10.884113, T: 4850000, Avg. loss: 902.540894\n",
      "Total training time: 24.95 seconds.\n",
      "-- Epoch 98\n",
      "Norm: 167.97, NNZs: 3072, Bias: -10.899271, T: 4900000, Avg. loss: 886.330726\n",
      "Total training time: 25.21 seconds.\n",
      "-- Epoch 99\n",
      "Norm: 167.11, NNZs: 3072, Bias: -10.913084, T: 4950000, Avg. loss: 887.072313\n",
      "Total training time: 25.47 seconds.\n",
      "-- Epoch 100\n",
      "Norm: 166.57, NNZs: 3072, Bias: -10.926871, T: 5000000, Avg. loss: 872.859152\n",
      "Total training time: 25.72 seconds.\n",
      "-- Epoch 101\n",
      "Norm: 165.90, NNZs: 3072, Bias: -10.938464, T: 5050000, Avg. loss: 853.513495\n",
      "Total training time: 25.99 seconds.\n",
      "-- Epoch 102\n",
      "Norm: 165.10, NNZs: 3072, Bias: -10.955845, T: 5100000, Avg. loss: 845.701561\n",
      "Total training time: 26.25 seconds.\n",
      "-- Epoch 103\n",
      "Norm: 164.35, NNZs: 3072, Bias: -10.969108, T: 5150000, Avg. loss: 851.306474\n",
      "Total training time: 26.50 seconds.\n",
      "-- Epoch 104\n",
      "Norm: 163.23, NNZs: 3072, Bias: -10.978994, T: 5200000, Avg. loss: 848.657613\n",
      "Total training time: 26.76 seconds.\n",
      "-- Epoch 105\n",
      "Norm: 162.49, NNZs: 3072, Bias: -10.993325, T: 5250000, Avg. loss: 835.058785\n",
      "Total training time: 27.03 seconds.\n",
      "-- Epoch 106\n",
      "Norm: 161.67, NNZs: 3072, Bias: -11.008101, T: 5300000, Avg. loss: 815.745954\n",
      "Total training time: 27.30 seconds.\n",
      "-- Epoch 107\n",
      "Norm: 161.03, NNZs: 3072, Bias: -11.019731, T: 5350000, Avg. loss: 814.991862\n",
      "Total training time: 27.55 seconds.\n",
      "-- Epoch 108\n",
      "Norm: 160.30, NNZs: 3072, Bias: -11.037403, T: 5400000, Avg. loss: 813.980982\n",
      "Total training time: 27.81 seconds.\n",
      "-- Epoch 109\n",
      "Norm: 159.67, NNZs: 3072, Bias: -11.055983, T: 5450000, Avg. loss: 798.182453\n",
      "Total training time: 28.07 seconds.\n",
      "-- Epoch 110\n",
      "Norm: 158.88, NNZs: 3072, Bias: -11.072396, T: 5500000, Avg. loss: 791.171291\n",
      "Total training time: 28.33 seconds.\n",
      "-- Epoch 111\n",
      "Norm: 158.15, NNZs: 3072, Bias: -11.086744, T: 5550000, Avg. loss: 782.101984\n",
      "Total training time: 28.59 seconds.\n",
      "-- Epoch 112\n",
      "Norm: 157.31, NNZs: 3072, Bias: -11.095684, T: 5600000, Avg. loss: 776.727092\n",
      "Total training time: 28.86 seconds.\n",
      "-- Epoch 113\n",
      "Norm: 156.65, NNZs: 3072, Bias: -11.110389, T: 5650000, Avg. loss: 768.593451\n",
      "Total training time: 29.12 seconds.\n",
      "-- Epoch 114\n",
      "Norm: 155.95, NNZs: 3072, Bias: -11.128941, T: 5700000, Avg. loss: 762.390258\n",
      "Total training time: 29.37 seconds.\n",
      "-- Epoch 115\n",
      "Norm: 155.25, NNZs: 3072, Bias: -11.144157, T: 5750000, Avg. loss: 744.277759\n",
      "Total training time: 29.63 seconds.\n",
      "-- Epoch 116\n",
      "Norm: 154.52, NNZs: 3072, Bias: -11.157285, T: 5800000, Avg. loss: 746.053676\n",
      "Total training time: 29.89 seconds.\n",
      "-- Epoch 117\n",
      "Norm: 154.02, NNZs: 3072, Bias: -11.173785, T: 5850000, Avg. loss: 745.170001\n",
      "Total training time: 30.15 seconds.\n",
      "-- Epoch 118\n",
      "Norm: 153.66, NNZs: 3072, Bias: -11.185872, T: 5900000, Avg. loss: 740.374920\n",
      "Total training time: 30.41 seconds.\n",
      "-- Epoch 119\n",
      "Norm: 153.04, NNZs: 3072, Bias: -11.197107, T: 5950000, Avg. loss: 722.705505\n",
      "Total training time: 30.67 seconds.\n",
      "-- Epoch 120\n",
      "Norm: 152.38, NNZs: 3072, Bias: -11.209661, T: 6000000, Avg. loss: 724.889052\n",
      "Total training time: 30.94 seconds.\n",
      "-- Epoch 121\n",
      "Norm: 151.77, NNZs: 3072, Bias: -11.223086, T: 6050000, Avg. loss: 712.069295\n",
      "Total training time: 31.19 seconds.\n",
      "-- Epoch 122\n",
      "Norm: 151.18, NNZs: 3072, Bias: -11.237030, T: 6100000, Avg. loss: 708.077297\n",
      "Total training time: 31.45 seconds.\n",
      "-- Epoch 123\n",
      "Norm: 150.62, NNZs: 3072, Bias: -11.252870, T: 6150000, Avg. loss: 702.330506\n",
      "Total training time: 31.71 seconds.\n",
      "-- Epoch 124\n",
      "Norm: 150.10, NNZs: 3072, Bias: -11.268839, T: 6200000, Avg. loss: 698.769303\n",
      "Total training time: 31.97 seconds.\n",
      "-- Epoch 125\n",
      "Norm: 149.47, NNZs: 3072, Bias: -11.280396, T: 6250000, Avg. loss: 697.370175\n",
      "Total training time: 32.23 seconds.\n",
      "-- Epoch 126\n",
      "Norm: 148.95, NNZs: 3072, Bias: -11.291566, T: 6300000, Avg. loss: 680.156652\n",
      "Total training time: 32.48 seconds.\n",
      "-- Epoch 127\n",
      "Norm: 148.12, NNZs: 3072, Bias: -11.305292, T: 6350000, Avg. loss: 682.346301\n",
      "Total training time: 32.74 seconds.\n",
      "-- Epoch 128\n",
      "Norm: 147.50, NNZs: 3072, Bias: -11.320339, T: 6400000, Avg. loss: 685.130387\n",
      "Total training time: 33.00 seconds.\n",
      "-- Epoch 129\n",
      "Norm: 146.70, NNZs: 3072, Bias: -11.336633, T: 6450000, Avg. loss: 673.708493\n",
      "Total training time: 33.26 seconds.\n",
      "-- Epoch 130\n",
      "Norm: 146.20, NNZs: 3072, Bias: -11.347760, T: 6500000, Avg. loss: 673.608520\n",
      "Total training time: 33.52 seconds.\n",
      "-- Epoch 131\n",
      "Norm: 145.72, NNZs: 3072, Bias: -11.360405, T: 6550000, Avg. loss: 651.117108\n",
      "Total training time: 33.77 seconds.\n",
      "-- Epoch 132\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 145.25, NNZs: 3072, Bias: -11.370417, T: 6600000, Avg. loss: 650.063942\n",
      "Total training time: 34.03 seconds.\n",
      "-- Epoch 133\n",
      "Norm: 144.94, NNZs: 3072, Bias: -11.378379, T: 6650000, Avg. loss: 649.093272\n",
      "Total training time: 34.29 seconds.\n",
      "-- Epoch 134\n",
      "Norm: 144.38, NNZs: 3072, Bias: -11.388052, T: 6700000, Avg. loss: 646.952611\n",
      "Total training time: 34.55 seconds.\n",
      "-- Epoch 135\n",
      "Norm: 143.73, NNZs: 3072, Bias: -11.401812, T: 6750000, Avg. loss: 642.292831\n",
      "Total training time: 34.81 seconds.\n",
      "-- Epoch 136\n",
      "Norm: 143.34, NNZs: 3072, Bias: -11.413757, T: 6800000, Avg. loss: 643.021503\n",
      "Total training time: 35.07 seconds.\n",
      "-- Epoch 137\n",
      "Norm: 142.84, NNZs: 3072, Bias: -11.424999, T: 6850000, Avg. loss: 636.178236\n",
      "Total training time: 35.32 seconds.\n",
      "-- Epoch 138\n",
      "Norm: 142.31, NNZs: 3072, Bias: -11.436833, T: 6900000, Avg. loss: 634.174082\n",
      "Total training time: 35.58 seconds.\n",
      "-- Epoch 139\n",
      "Norm: 141.64, NNZs: 3072, Bias: -11.450436, T: 6950000, Avg. loss: 625.100283\n",
      "Total training time: 35.84 seconds.\n",
      "-- Epoch 140\n",
      "Norm: 141.09, NNZs: 3072, Bias: -11.458712, T: 7000000, Avg. loss: 615.829411\n",
      "Total training time: 36.10 seconds.\n",
      "-- Epoch 141\n",
      "Norm: 140.47, NNZs: 3072, Bias: -11.467219, T: 7050000, Avg. loss: 615.974898\n",
      "Total training time: 36.36 seconds.\n",
      "-- Epoch 142\n",
      "Norm: 139.81, NNZs: 3072, Bias: -11.483514, T: 7100000, Avg. loss: 610.065344\n",
      "Total training time: 36.62 seconds.\n",
      "-- Epoch 143\n",
      "Norm: 139.48, NNZs: 3072, Bias: -11.495807, T: 7150000, Avg. loss: 598.774585\n",
      "Total training time: 36.87 seconds.\n",
      "-- Epoch 144\n",
      "Norm: 139.12, NNZs: 3072, Bias: -11.508265, T: 7200000, Avg. loss: 604.430865\n",
      "Total training time: 37.13 seconds.\n",
      "-- Epoch 145\n",
      "Norm: 138.71, NNZs: 3072, Bias: -11.522519, T: 7250000, Avg. loss: 603.573760\n",
      "Total training time: 37.39 seconds.\n",
      "-- Epoch 146\n",
      "Norm: 138.25, NNZs: 3072, Bias: -11.535190, T: 7300000, Avg. loss: 592.591018\n",
      "Total training time: 37.65 seconds.\n",
      "-- Epoch 147\n",
      "Norm: 137.76, NNZs: 3072, Bias: -11.545007, T: 7350000, Avg. loss: 591.659188\n",
      "Total training time: 37.91 seconds.\n",
      "-- Epoch 148\n",
      "Norm: 137.22, NNZs: 3072, Bias: -11.554409, T: 7400000, Avg. loss: 588.888882\n",
      "Total training time: 38.17 seconds.\n",
      "-- Epoch 149\n",
      "Norm: 136.89, NNZs: 3072, Bias: -11.563092, T: 7450000, Avg. loss: 578.323033\n",
      "Total training time: 38.42 seconds.\n",
      "-- Epoch 150\n",
      "Norm: 136.49, NNZs: 3072, Bias: -11.572803, T: 7500000, Avg. loss: 579.346472\n",
      "Total training time: 38.70 seconds.\n",
      "-- Epoch 151\n",
      "Norm: 136.01, NNZs: 3072, Bias: -11.579145, T: 7550000, Avg. loss: 570.886716\n",
      "Total training time: 38.97 seconds.\n",
      "-- Epoch 152\n",
      "Norm: 135.66, NNZs: 3072, Bias: -11.587612, T: 7600000, Avg. loss: 570.094872\n",
      "Total training time: 39.23 seconds.\n",
      "-- Epoch 153\n",
      "Norm: 135.19, NNZs: 3072, Bias: -11.601028, T: 7650000, Avg. loss: 560.907157\n",
      "Total training time: 39.51 seconds.\n",
      "-- Epoch 154\n",
      "Norm: 134.80, NNZs: 3072, Bias: -11.610724, T: 7700000, Avg. loss: 560.027097\n",
      "Total training time: 39.79 seconds.\n",
      "-- Epoch 155\n",
      "Norm: 134.34, NNZs: 3072, Bias: -11.619410, T: 7750000, Avg. loss: 558.342010\n",
      "Total training time: 40.06 seconds.\n",
      "-- Epoch 156\n",
      "Norm: 133.93, NNZs: 3072, Bias: -11.628008, T: 7800000, Avg. loss: 553.467121\n",
      "Total training time: 40.33 seconds.\n",
      "-- Epoch 157\n",
      "Norm: 133.49, NNZs: 3072, Bias: -11.637916, T: 7850000, Avg. loss: 554.304496\n",
      "Total training time: 40.58 seconds.\n",
      "-- Epoch 158\n",
      "Norm: 132.98, NNZs: 3072, Bias: -11.648358, T: 7900000, Avg. loss: 556.519011\n",
      "Total training time: 40.86 seconds.\n",
      "-- Epoch 159\n",
      "Norm: 132.73, NNZs: 3072, Bias: -11.656872, T: 7950000, Avg. loss: 542.988035\n",
      "Total training time: 41.14 seconds.\n",
      "-- Epoch 160\n",
      "Norm: 132.21, NNZs: 3072, Bias: -11.668669, T: 8000000, Avg. loss: 546.331429\n",
      "Total training time: 41.41 seconds.\n",
      "-- Epoch 161\n",
      "Norm: 131.79, NNZs: 3072, Bias: -11.677431, T: 8050000, Avg. loss: 539.600475\n",
      "Total training time: 41.69 seconds.\n",
      "-- Epoch 162\n",
      "Norm: 131.35, NNZs: 3072, Bias: -11.685287, T: 8100000, Avg. loss: 533.710027\n",
      "Total training time: 41.95 seconds.\n",
      "-- Epoch 163\n",
      "Norm: 130.83, NNZs: 3072, Bias: -11.697025, T: 8150000, Avg. loss: 534.554970\n",
      "Total training time: 42.22 seconds.\n",
      "-- Epoch 164\n",
      "Norm: 130.38, NNZs: 3072, Bias: -11.705531, T: 8200000, Avg. loss: 529.149494\n",
      "Total training time: 42.49 seconds.\n",
      "-- Epoch 165\n",
      "Norm: 130.19, NNZs: 3072, Bias: -11.716403, T: 8250000, Avg. loss: 518.843085\n",
      "Total training time: 42.76 seconds.\n",
      "-- Epoch 166\n",
      "Norm: 129.77, NNZs: 3072, Bias: -11.725514, T: 8300000, Avg. loss: 523.750523\n",
      "Total training time: 43.03 seconds.\n",
      "-- Epoch 167\n",
      "Norm: 129.38, NNZs: 3072, Bias: -11.734973, T: 8350000, Avg. loss: 519.232760\n",
      "Total training time: 43.29 seconds.\n",
      "-- Epoch 168\n",
      "Norm: 128.98, NNZs: 3072, Bias: -11.743664, T: 8400000, Avg. loss: 522.216727\n",
      "Total training time: 43.56 seconds.\n",
      "-- Epoch 169\n",
      "Norm: 128.69, NNZs: 3072, Bias: -11.755442, T: 8450000, Avg. loss: 513.931896\n",
      "Total training time: 43.84 seconds.\n",
      "-- Epoch 170\n",
      "Norm: 128.32, NNZs: 3072, Bias: -11.764477, T: 8500000, Avg. loss: 512.282900\n",
      "Total training time: 44.12 seconds.\n",
      "-- Epoch 171\n",
      "Norm: 127.80, NNZs: 3072, Bias: -11.769929, T: 8550000, Avg. loss: 508.671274\n",
      "Total training time: 44.39 seconds.\n",
      "-- Epoch 172\n",
      "Norm: 127.39, NNZs: 3072, Bias: -11.779006, T: 8600000, Avg. loss: 511.438857\n",
      "Total training time: 44.66 seconds.\n",
      "-- Epoch 173\n",
      "Norm: 127.07, NNZs: 3072, Bias: -11.789479, T: 8650000, Avg. loss: 503.304182\n",
      "Total training time: 44.96 seconds.\n",
      "-- Epoch 174\n",
      "Norm: 126.72, NNZs: 3072, Bias: -11.799277, T: 8700000, Avg. loss: 495.665361\n",
      "Total training time: 45.28 seconds.\n",
      "-- Epoch 175\n",
      "Norm: 126.40, NNZs: 3072, Bias: -11.806809, T: 8750000, Avg. loss: 495.528073\n",
      "Total training time: 45.56 seconds.\n",
      "-- Epoch 176\n",
      "Norm: 125.96, NNZs: 3072, Bias: -11.817136, T: 8800000, Avg. loss: 492.527597\n",
      "Total training time: 45.83 seconds.\n",
      "-- Epoch 177\n",
      "Norm: 125.67, NNZs: 3072, Bias: -11.824189, T: 8850000, Avg. loss: 487.152695\n",
      "Total training time: 46.13 seconds.\n",
      "-- Epoch 178\n",
      "Norm: 125.38, NNZs: 3072, Bias: -11.832996, T: 8900000, Avg. loss: 489.299935\n",
      "Total training time: 46.43 seconds.\n",
      "-- Epoch 179\n",
      "Norm: 124.98, NNZs: 3072, Bias: -11.843924, T: 8950000, Avg. loss: 479.285053\n",
      "Total training time: 46.70 seconds.\n",
      "-- Epoch 180\n",
      "Norm: 124.68, NNZs: 3072, Bias: -11.852974, T: 9000000, Avg. loss: 480.521405\n",
      "Total training time: 46.96 seconds.\n",
      "-- Epoch 181\n",
      "Norm: 124.24, NNZs: 3072, Bias: -11.862616, T: 9050000, Avg. loss: 476.742261\n",
      "Total training time: 47.23 seconds.\n",
      "-- Epoch 182\n",
      "Norm: 123.97, NNZs: 3072, Bias: -11.869989, T: 9100000, Avg. loss: 475.032085\n",
      "Total training time: 47.51 seconds.\n",
      "-- Epoch 183\n",
      "Norm: 123.70, NNZs: 3072, Bias: -11.880964, T: 9150000, Avg. loss: 476.229874\n",
      "Total training time: 47.78 seconds.\n",
      "-- Epoch 184\n",
      "Norm: 123.32, NNZs: 3072, Bias: -11.893139, T: 9200000, Avg. loss: 470.592192\n",
      "Total training time: 48.06 seconds.\n",
      "-- Epoch 185\n",
      "Norm: 122.96, NNZs: 3072, Bias: -11.902773, T: 9250000, Avg. loss: 464.286295\n",
      "Total training time: 48.33 seconds.\n",
      "-- Epoch 186\n",
      "Norm: 122.76, NNZs: 3072, Bias: -11.912986, T: 9300000, Avg. loss: 465.351399\n",
      "Total training time: 48.60 seconds.\n",
      "-- Epoch 187\n",
      "Norm: 122.44, NNZs: 3072, Bias: -11.920861, T: 9350000, Avg. loss: 459.116632\n",
      "Total training time: 48.89 seconds.\n",
      "-- Epoch 188\n",
      "Norm: 122.06, NNZs: 3072, Bias: -11.926690, T: 9400000, Avg. loss: 460.846903\n",
      "Total training time: 49.18 seconds.\n",
      "-- Epoch 189\n",
      "Norm: 121.70, NNZs: 3072, Bias: -11.936852, T: 9450000, Avg. loss: 458.708487\n",
      "Total training time: 49.44 seconds.\n",
      "-- Epoch 190\n",
      "Norm: 121.36, NNZs: 3072, Bias: -11.946077, T: 9500000, Avg. loss: 456.049446\n",
      "Total training time: 49.71 seconds.\n",
      "-- Epoch 191\n",
      "Norm: 121.06, NNZs: 3072, Bias: -11.952321, T: 9550000, Avg. loss: 451.216129\n",
      "Total training time: 50.02 seconds.\n",
      "-- Epoch 192\n",
      "Norm: 120.82, NNZs: 3072, Bias: -11.959545, T: 9600000, Avg. loss: 450.313013\n",
      "Total training time: 50.31 seconds.\n",
      "-- Epoch 193\n",
      "Norm: 120.50, NNZs: 3072, Bias: -11.966448, T: 9650000, Avg. loss: 447.558671\n",
      "Total training time: 50.58 seconds.\n",
      "-- Epoch 194\n",
      "Norm: 120.14, NNZs: 3072, Bias: -11.978244, T: 9700000, Avg. loss: 445.210360\n",
      "Total training time: 50.85 seconds.\n",
      "-- Epoch 195\n",
      "Norm: 119.81, NNZs: 3072, Bias: -11.987602, T: 9750000, Avg. loss: 441.332211\n",
      "Total training time: 51.13 seconds.\n",
      "-- Epoch 196\n",
      "Norm: 119.48, NNZs: 3072, Bias: -11.996146, T: 9800000, Avg. loss: 444.586526\n",
      "Total training time: 51.40 seconds.\n",
      "-- Epoch 197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 119.15, NNZs: 3072, Bias: -12.003405, T: 9850000, Avg. loss: 435.719642\n",
      "Total training time: 51.71 seconds.\n",
      "-- Epoch 198\n",
      "Norm: 118.81, NNZs: 3072, Bias: -12.011350, T: 9900000, Avg. loss: 434.149592\n",
      "Total training time: 51.98 seconds.\n",
      "-- Epoch 199\n",
      "Norm: 118.45, NNZs: 3072, Bias: -12.018133, T: 9950000, Avg. loss: 437.255524\n",
      "Total training time: 52.25 seconds.\n",
      "-- Epoch 200\n",
      "Norm: 118.18, NNZs: 3072, Bias: -12.027352, T: 10000000, Avg. loss: 432.485371\n",
      "Total training time: 52.54 seconds.\n",
      "-- Epoch 201\n",
      "Norm: 117.90, NNZs: 3072, Bias: -12.036581, T: 10050000, Avg. loss: 425.128855\n",
      "Total training time: 52.82 seconds.\n",
      "-- Epoch 202\n",
      "Norm: 117.68, NNZs: 3072, Bias: -12.046899, T: 10100000, Avg. loss: 426.930995\n",
      "Total training time: 53.15 seconds.\n",
      "-- Epoch 203\n",
      "Norm: 117.35, NNZs: 3072, Bias: -12.058093, T: 10150000, Avg. loss: 423.831190\n",
      "Total training time: 53.42 seconds.\n",
      "-- Epoch 204\n",
      "Norm: 117.01, NNZs: 3072, Bias: -12.063944, T: 10200000, Avg. loss: 420.156252\n",
      "Total training time: 53.70 seconds.\n",
      "-- Epoch 205\n",
      "Norm: 116.65, NNZs: 3072, Bias: -12.074048, T: 10250000, Avg. loss: 420.579557\n",
      "Total training time: 53.99 seconds.\n",
      "-- Epoch 206\n",
      "Norm: 116.32, NNZs: 3072, Bias: -12.082968, T: 10300000, Avg. loss: 417.725411\n",
      "Total training time: 54.27 seconds.\n",
      "-- Epoch 207\n",
      "Norm: 116.02, NNZs: 3072, Bias: -12.088503, T: 10350000, Avg. loss: 416.820941\n",
      "Total training time: 54.56 seconds.\n",
      "-- Epoch 208\n",
      "Norm: 115.62, NNZs: 3072, Bias: -12.098172, T: 10400000, Avg. loss: 414.751030\n",
      "Total training time: 54.84 seconds.\n",
      "-- Epoch 209\n",
      "Norm: 115.36, NNZs: 3072, Bias: -12.104057, T: 10450000, Avg. loss: 414.481480\n",
      "Total training time: 55.13 seconds.\n",
      "-- Epoch 210\n",
      "Norm: 115.10, NNZs: 3072, Bias: -12.111494, T: 10500000, Avg. loss: 410.751176\n",
      "Total training time: 55.40 seconds.\n",
      "-- Epoch 211\n",
      "Norm: 114.83, NNZs: 3072, Bias: -12.120904, T: 10550000, Avg. loss: 408.257580\n",
      "Total training time: 55.68 seconds.\n",
      "-- Epoch 212\n",
      "Norm: 114.59, NNZs: 3072, Bias: -12.127688, T: 10600000, Avg. loss: 405.847703\n",
      "Total training time: 55.96 seconds.\n",
      "-- Epoch 213\n",
      "Norm: 114.29, NNZs: 3072, Bias: -12.136664, T: 10650000, Avg. loss: 406.306447\n",
      "Total training time: 56.24 seconds.\n",
      "-- Epoch 214\n",
      "Norm: 114.04, NNZs: 3072, Bias: -12.145802, T: 10700000, Avg. loss: 406.335633\n",
      "Total training time: 56.52 seconds.\n",
      "-- Epoch 215\n",
      "Norm: 113.79, NNZs: 3072, Bias: -12.152553, T: 10750000, Avg. loss: 401.340314\n",
      "Total training time: 56.80 seconds.\n",
      "-- Epoch 216\n",
      "Norm: 113.50, NNZs: 3072, Bias: -12.159967, T: 10800000, Avg. loss: 399.243017\n",
      "Total training time: 57.09 seconds.\n",
      "-- Epoch 217\n",
      "Norm: 113.22, NNZs: 3072, Bias: -12.166930, T: 10850000, Avg. loss: 396.432294\n",
      "Total training time: 57.36 seconds.\n",
      "-- Epoch 218\n",
      "Norm: 112.94, NNZs: 3072, Bias: -12.173356, T: 10900000, Avg. loss: 396.916727\n",
      "Total training time: 57.64 seconds.\n",
      "-- Epoch 219\n",
      "Norm: 112.64, NNZs: 3072, Bias: -12.180062, T: 10950000, Avg. loss: 392.717018\n",
      "Total training time: 57.92 seconds.\n",
      "-- Epoch 220\n",
      "Norm: 112.39, NNZs: 3072, Bias: -12.185466, T: 11000000, Avg. loss: 392.368920\n",
      "Total training time: 58.20 seconds.\n",
      "-- Epoch 221\n",
      "Norm: 112.18, NNZs: 3072, Bias: -12.192506, T: 11050000, Avg. loss: 392.677982\n",
      "Total training time: 58.48 seconds.\n",
      "-- Epoch 222\n",
      "Norm: 112.02, NNZs: 3072, Bias: -12.199705, T: 11100000, Avg. loss: 385.546620\n",
      "Total training time: 58.76 seconds.\n",
      "-- Epoch 223\n",
      "Norm: 111.78, NNZs: 3072, Bias: -12.207675, T: 11150000, Avg. loss: 386.043411\n",
      "Total training time: 59.05 seconds.\n",
      "-- Epoch 224\n",
      "Norm: 111.52, NNZs: 3072, Bias: -12.212695, T: 11200000, Avg. loss: 385.944610\n",
      "Total training time: 59.34 seconds.\n",
      "-- Epoch 225\n",
      "Norm: 111.34, NNZs: 3072, Bias: -12.219543, T: 11250000, Avg. loss: 383.450745\n",
      "Total training time: 59.62 seconds.\n",
      "-- Epoch 226\n",
      "Norm: 111.07, NNZs: 3072, Bias: -12.228241, T: 11300000, Avg. loss: 377.242093\n",
      "Total training time: 59.92 seconds.\n",
      "-- Epoch 227\n",
      "Norm: 110.88, NNZs: 3072, Bias: -12.234813, T: 11350000, Avg. loss: 382.597881\n",
      "Total training time: 60.20 seconds.\n",
      "-- Epoch 228\n",
      "Norm: 110.59, NNZs: 3072, Bias: -12.242428, T: 11400000, Avg. loss: 380.554414\n",
      "Total training time: 60.48 seconds.\n",
      "-- Epoch 229\n",
      "Norm: 110.34, NNZs: 3072, Bias: -12.246931, T: 11450000, Avg. loss: 375.535280\n",
      "Total training time: 60.77 seconds.\n",
      "-- Epoch 230\n",
      "Norm: 110.13, NNZs: 3072, Bias: -12.253450, T: 11500000, Avg. loss: 373.474651\n",
      "Total training time: 61.06 seconds.\n",
      "-- Epoch 231\n",
      "Norm: 109.93, NNZs: 3072, Bias: -12.259841, T: 11550000, Avg. loss: 373.911165\n",
      "Total training time: 61.34 seconds.\n",
      "-- Epoch 232\n",
      "Norm: 109.64, NNZs: 3072, Bias: -12.267301, T: 11600000, Avg. loss: 371.489304\n",
      "Total training time: 61.62 seconds.\n",
      "-- Epoch 233\n",
      "Norm: 109.40, NNZs: 3072, Bias: -12.276661, T: 11650000, Avg. loss: 370.869986\n",
      "Total training time: 61.91 seconds.\n",
      "-- Epoch 234\n",
      "Norm: 109.13, NNZs: 3072, Bias: -12.284585, T: 11700000, Avg. loss: 366.997683\n",
      "Total training time: 62.19 seconds.\n",
      "-- Epoch 235\n",
      "Norm: 108.98, NNZs: 3072, Bias: -12.292866, T: 11750000, Avg. loss: 364.679709\n",
      "Total training time: 62.58 seconds.\n",
      "-- Epoch 236\n",
      "Norm: 108.75, NNZs: 3072, Bias: -12.297375, T: 11800000, Avg. loss: 367.044872\n",
      "Total training time: 62.90 seconds.\n",
      "-- Epoch 237\n",
      "Norm: 108.47, NNZs: 3072, Bias: -12.305106, T: 11850000, Avg. loss: 364.786135\n",
      "Total training time: 63.19 seconds.\n",
      "-- Epoch 238\n",
      "Norm: 108.32, NNZs: 3072, Bias: -12.311847, T: 11900000, Avg. loss: 361.060588\n",
      "Total training time: 63.47 seconds.\n",
      "-- Epoch 239\n",
      "Norm: 108.03, NNZs: 3072, Bias: -12.319339, T: 11950000, Avg. loss: 360.608339\n",
      "Total training time: 63.78 seconds.\n",
      "-- Epoch 240\n",
      "Norm: 107.85, NNZs: 3072, Bias: -12.325476, T: 12000000, Avg. loss: 358.388935\n",
      "Total training time: 64.09 seconds.\n",
      "-- Epoch 241\n",
      "Norm: 107.65, NNZs: 3072, Bias: -12.332305, T: 12050000, Avg. loss: 353.988163\n",
      "Total training time: 64.36 seconds.\n",
      "-- Epoch 242\n",
      "Norm: 107.34, NNZs: 3072, Bias: -12.337519, T: 12100000, Avg. loss: 355.769062\n",
      "Total training time: 64.63 seconds.\n",
      "-- Epoch 243\n",
      "Norm: 107.06, NNZs: 3072, Bias: -12.345056, T: 12150000, Avg. loss: 355.750700\n",
      "Total training time: 64.91 seconds.\n",
      "-- Epoch 244\n",
      "Norm: 106.82, NNZs: 3072, Bias: -12.352757, T: 12200000, Avg. loss: 355.656311\n",
      "Total training time: 65.20 seconds.\n",
      "-- Epoch 245\n",
      "Norm: 106.55, NNZs: 3072, Bias: -12.358497, T: 12250000, Avg. loss: 346.828106\n",
      "Total training time: 65.46 seconds.\n",
      "-- Epoch 246\n",
      "Norm: 106.38, NNZs: 3072, Bias: -12.366223, T: 12300000, Avg. loss: 350.733345\n",
      "Total training time: 65.76 seconds.\n",
      "-- Epoch 247\n",
      "Norm: 106.19, NNZs: 3072, Bias: -12.371225, T: 12350000, Avg. loss: 348.559096\n",
      "Total training time: 66.07 seconds.\n",
      "-- Epoch 248\n",
      "Norm: 105.92, NNZs: 3072, Bias: -12.379346, T: 12400000, Avg. loss: 346.702257\n",
      "Total training time: 66.37 seconds.\n",
      "-- Epoch 249\n",
      "Norm: 105.76, NNZs: 3072, Bias: -12.383314, T: 12450000, Avg. loss: 343.245175\n",
      "Total training time: 66.66 seconds.\n",
      "-- Epoch 250\n",
      "Norm: 105.54, NNZs: 3072, Bias: -12.389900, T: 12500000, Avg. loss: 343.598003\n",
      "Total training time: 66.96 seconds.\n",
      "-- Epoch 251\n",
      "Norm: 105.36, NNZs: 3072, Bias: -12.396691, T: 12550000, Avg. loss: 347.659585\n",
      "Total training time: 67.26 seconds.\n",
      "-- Epoch 252\n",
      "Norm: 105.11, NNZs: 3072, Bias: -12.403238, T: 12600000, Avg. loss: 341.884451\n",
      "Total training time: 67.56 seconds.\n",
      "-- Epoch 253\n",
      "Norm: 104.96, NNZs: 3072, Bias: -12.409958, T: 12650000, Avg. loss: 338.767195\n",
      "Total training time: 67.83 seconds.\n",
      "-- Epoch 254\n",
      "Norm: 104.71, NNZs: 3072, Bias: -12.414674, T: 12700000, Avg. loss: 338.815394\n",
      "Total training time: 68.11 seconds.\n",
      "-- Epoch 255\n",
      "Norm: 104.53, NNZs: 3072, Bias: -12.420940, T: 12750000, Avg. loss: 336.178690\n",
      "Total training time: 68.38 seconds.\n",
      "-- Epoch 256\n",
      "Norm: 104.30, NNZs: 3072, Bias: -12.426440, T: 12800000, Avg. loss: 337.801972\n",
      "Total training time: 68.65 seconds.\n",
      "-- Epoch 257\n",
      "Norm: 104.09, NNZs: 3072, Bias: -12.432022, T: 12850000, Avg. loss: 336.331321\n",
      "Total training time: 68.95 seconds.\n",
      "-- Epoch 258\n",
      "Norm: 103.95, NNZs: 3072, Bias: -12.438818, T: 12900000, Avg. loss: 331.408936\n",
      "Total training time: 69.25 seconds.\n",
      "-- Epoch 259\n",
      "Norm: 103.77, NNZs: 3072, Bias: -12.445526, T: 12950000, Avg. loss: 332.752366\n",
      "Total training time: 69.55 seconds.\n",
      "-- Epoch 260\n",
      "Norm: 103.56, NNZs: 3072, Bias: -12.451635, T: 13000000, Avg. loss: 331.998994\n",
      "Total training time: 69.84 seconds.\n",
      "-- Epoch 261\n",
      "Norm: 103.31, NNZs: 3072, Bias: -12.457489, T: 13050000, Avg. loss: 331.561798\n",
      "Total training time: 70.15 seconds.\n",
      "-- Epoch 262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 103.13, NNZs: 3072, Bias: -12.463212, T: 13100000, Avg. loss: 330.714512\n",
      "Total training time: 70.45 seconds.\n",
      "-- Epoch 263\n",
      "Norm: 102.96, NNZs: 3072, Bias: -12.470932, T: 13150000, Avg. loss: 325.505345\n",
      "Total training time: 70.75 seconds.\n",
      "-- Epoch 264\n",
      "Norm: 102.74, NNZs: 3072, Bias: -12.476696, T: 13200000, Avg. loss: 327.242547\n",
      "Total training time: 71.06 seconds.\n",
      "-- Epoch 265\n",
      "Norm: 102.55, NNZs: 3072, Bias: -12.481689, T: 13250000, Avg. loss: 321.787647\n",
      "Total training time: 71.36 seconds.\n",
      "-- Epoch 266\n",
      "Norm: 102.34, NNZs: 3072, Bias: -12.488339, T: 13300000, Avg. loss: 322.630400\n",
      "Total training time: 71.67 seconds.\n",
      "-- Epoch 267\n",
      "Norm: 102.19, NNZs: 3072, Bias: -12.493816, T: 13350000, Avg. loss: 316.485106\n",
      "Total training time: 71.97 seconds.\n",
      "-- Epoch 268\n",
      "Norm: 101.99, NNZs: 3072, Bias: -12.500010, T: 13400000, Avg. loss: 321.747659\n",
      "Total training time: 72.27 seconds.\n",
      "-- Epoch 269\n",
      "Norm: 101.77, NNZs: 3072, Bias: -12.506210, T: 13450000, Avg. loss: 321.843178\n",
      "Total training time: 72.58 seconds.\n",
      "-- Epoch 270\n",
      "Norm: 101.57, NNZs: 3072, Bias: -12.512718, T: 13500000, Avg. loss: 323.503380\n",
      "Total training time: 72.87 seconds.\n",
      "-- Epoch 271\n",
      "Norm: 101.43, NNZs: 3072, Bias: -12.518810, T: 13550000, Avg. loss: 317.353283\n",
      "Total training time: 73.18 seconds.\n",
      "-- Epoch 272\n",
      "Norm: 101.25, NNZs: 3072, Bias: -12.525300, T: 13600000, Avg. loss: 314.410920\n",
      "Total training time: 73.48 seconds.\n",
      "-- Epoch 273\n",
      "Norm: 101.08, NNZs: 3072, Bias: -12.531265, T: 13650000, Avg. loss: 314.983293\n",
      "Total training time: 73.81 seconds.\n",
      "-- Epoch 274\n",
      "Norm: 100.86, NNZs: 3072, Bias: -12.537127, T: 13700000, Avg. loss: 315.094435\n",
      "Total training time: 74.12 seconds.\n",
      "-- Epoch 275\n",
      "Norm: 100.76, NNZs: 3072, Bias: -12.541604, T: 13750000, Avg. loss: 310.923921\n",
      "Total training time: 74.41 seconds.\n",
      "-- Epoch 276\n",
      "Norm: 100.51, NNZs: 3072, Bias: -12.547175, T: 13800000, Avg. loss: 314.720762\n",
      "Total training time: 74.71 seconds.\n",
      "-- Epoch 277\n",
      "Norm: 100.34, NNZs: 3072, Bias: -12.553664, T: 13850000, Avg. loss: 313.012480\n",
      "Total training time: 75.00 seconds.\n",
      "-- Epoch 278\n",
      "Norm: 100.15, NNZs: 3072, Bias: -12.559196, T: 13900000, Avg. loss: 308.583002\n",
      "Total training time: 75.31 seconds.\n",
      "-- Epoch 279\n",
      "Norm: 99.97, NNZs: 3072, Bias: -12.565064, T: 13950000, Avg. loss: 305.030050\n",
      "Total training time: 75.61 seconds.\n",
      "-- Epoch 280\n",
      "Norm: 99.82, NNZs: 3072, Bias: -12.570871, T: 14000000, Avg. loss: 305.500398\n",
      "Total training time: 75.91 seconds.\n",
      "-- Epoch 281\n",
      "Norm: 99.64, NNZs: 3072, Bias: -12.576794, T: 14050000, Avg. loss: 306.381361\n",
      "Total training time: 76.19 seconds.\n",
      "-- Epoch 282\n",
      "Norm: 99.41, NNZs: 3072, Bias: -12.583197, T: 14100000, Avg. loss: 303.501794\n",
      "Total training time: 76.49 seconds.\n",
      "-- Epoch 283\n",
      "Norm: 99.30, NNZs: 3072, Bias: -12.588723, T: 14150000, Avg. loss: 301.771431\n",
      "Total training time: 76.78 seconds.\n",
      "-- Epoch 284\n",
      "Norm: 99.08, NNZs: 3072, Bias: -12.594334, T: 14200000, Avg. loss: 305.553351\n",
      "Total training time: 77.08 seconds.\n",
      "-- Epoch 285\n",
      "Norm: 98.90, NNZs: 3072, Bias: -12.599266, T: 14250000, Avg. loss: 302.652455\n",
      "Total training time: 77.38 seconds.\n",
      "-- Epoch 286\n",
      "Norm: 98.77, NNZs: 3072, Bias: -12.606426, T: 14300000, Avg. loss: 302.718032\n",
      "Total training time: 77.68 seconds.\n",
      "-- Epoch 287\n",
      "Norm: 98.59, NNZs: 3072, Bias: -12.611600, T: 14350000, Avg. loss: 297.753448\n",
      "Total training time: 77.99 seconds.\n",
      "-- Epoch 288\n",
      "Norm: 98.46, NNZs: 3072, Bias: -12.615759, T: 14400000, Avg. loss: 297.214312\n",
      "Total training time: 78.29 seconds.\n",
      "-- Epoch 289\n",
      "Norm: 98.30, NNZs: 3072, Bias: -12.621621, T: 14450000, Avg. loss: 297.287269\n",
      "Total training time: 78.59 seconds.\n",
      "-- Epoch 290\n",
      "Norm: 98.13, NNZs: 3072, Bias: -12.628344, T: 14500000, Avg. loss: 298.796564\n",
      "Total training time: 78.88 seconds.\n",
      "-- Epoch 291\n",
      "Norm: 97.92, NNZs: 3072, Bias: -12.635261, T: 14550000, Avg. loss: 297.905256\n",
      "Total training time: 79.18 seconds.\n",
      "-- Epoch 292\n",
      "Norm: 97.74, NNZs: 3072, Bias: -12.642374, T: 14600000, Avg. loss: 293.590571\n",
      "Total training time: 79.48 seconds.\n",
      "-- Epoch 293\n",
      "Norm: 97.58, NNZs: 3072, Bias: -12.646522, T: 14650000, Avg. loss: 291.620011\n",
      "Total training time: 79.77 seconds.\n",
      "-- Epoch 294\n",
      "Norm: 97.43, NNZs: 3072, Bias: -12.651219, T: 14700000, Avg. loss: 295.624450\n",
      "Total training time: 80.06 seconds.\n",
      "-- Epoch 295\n",
      "Norm: 97.27, NNZs: 3072, Bias: -12.658795, T: 14750000, Avg. loss: 291.195206\n",
      "Total training time: 80.37 seconds.\n",
      "-- Epoch 296\n",
      "Norm: 97.10, NNZs: 3072, Bias: -12.664518, T: 14800000, Avg. loss: 286.280338\n",
      "Total training time: 80.66 seconds.\n",
      "-- Epoch 297\n",
      "Norm: 96.93, NNZs: 3072, Bias: -12.670118, T: 14850000, Avg. loss: 292.458129\n",
      "Total training time: 80.97 seconds.\n",
      "-- Epoch 298\n",
      "Norm: 96.70, NNZs: 3072, Bias: -12.677005, T: 14900000, Avg. loss: 291.175171\n",
      "Total training time: 81.27 seconds.\n",
      "-- Epoch 299\n",
      "Norm: 96.54, NNZs: 3072, Bias: -12.682417, T: 14950000, Avg. loss: 288.543783\n",
      "Total training time: 81.58 seconds.\n",
      "-- Epoch 300\n",
      "Norm: 96.38, NNZs: 3072, Bias: -12.689516, T: 15000000, Avg. loss: 287.297320\n",
      "Total training time: 81.88 seconds.\n",
      "-- Epoch 301\n",
      "Norm: 96.21, NNZs: 3072, Bias: -12.695616, T: 15050000, Avg. loss: 283.121294\n",
      "Total training time: 82.19 seconds.\n",
      "-- Epoch 302\n",
      "Norm: 96.06, NNZs: 3072, Bias: -12.701597, T: 15100000, Avg. loss: 286.661881\n",
      "Total training time: 82.47 seconds.\n",
      "-- Epoch 303\n",
      "Norm: 95.87, NNZs: 3072, Bias: -12.706715, T: 15150000, Avg. loss: 285.291270\n",
      "Total training time: 82.78 seconds.\n",
      "-- Epoch 304\n",
      "Norm: 95.66, NNZs: 3072, Bias: -12.710799, T: 15200000, Avg. loss: 286.241098\n",
      "Total training time: 83.07 seconds.\n",
      "-- Epoch 305\n",
      "Norm: 95.49, NNZs: 3072, Bias: -12.717850, T: 15250000, Avg. loss: 281.167860\n",
      "Total training time: 83.37 seconds.\n",
      "-- Epoch 306\n",
      "Norm: 95.31, NNZs: 3072, Bias: -12.722815, T: 15300000, Avg. loss: 278.025940\n",
      "Total training time: 83.67 seconds.\n",
      "-- Epoch 307\n",
      "Norm: 95.19, NNZs: 3072, Bias: -12.726827, T: 15350000, Avg. loss: 280.764876\n",
      "Total training time: 83.97 seconds.\n",
      "-- Epoch 308\n",
      "Norm: 95.04, NNZs: 3072, Bias: -12.731464, T: 15400000, Avg. loss: 279.642128\n",
      "Total training time: 84.26 seconds.\n",
      "-- Epoch 309\n",
      "Norm: 94.92, NNZs: 3072, Bias: -12.737631, T: 15450000, Avg. loss: 276.827622\n",
      "Total training time: 84.53 seconds.\n",
      "-- Epoch 310\n",
      "Norm: 94.74, NNZs: 3072, Bias: -12.743014, T: 15500000, Avg. loss: 277.581147\n",
      "Total training time: 84.82 seconds.\n",
      "-- Epoch 311\n",
      "Norm: 94.63, NNZs: 3072, Bias: -12.750151, T: 15550000, Avg. loss: 276.354402\n",
      "Total training time: 85.11 seconds.\n",
      "-- Epoch 312\n",
      "Norm: 94.45, NNZs: 3072, Bias: -12.754978, T: 15600000, Avg. loss: 276.420861\n",
      "Total training time: 85.41 seconds.\n",
      "-- Epoch 313\n",
      "Norm: 94.28, NNZs: 3072, Bias: -12.759641, T: 15650000, Avg. loss: 274.457103\n",
      "Total training time: 85.71 seconds.\n",
      "-- Epoch 314\n",
      "Norm: 94.14, NNZs: 3072, Bias: -12.765408, T: 15700000, Avg. loss: 274.723577\n",
      "Total training time: 86.00 seconds.\n",
      "-- Epoch 315\n",
      "Norm: 93.98, NNZs: 3072, Bias: -12.771124, T: 15750000, Avg. loss: 275.486386\n",
      "Total training time: 86.30 seconds.\n",
      "-- Epoch 316\n",
      "Norm: 93.86, NNZs: 3072, Bias: -12.776129, T: 15800000, Avg. loss: 274.014709\n",
      "Total training time: 86.59 seconds.\n",
      "-- Epoch 317\n",
      "Norm: 93.72, NNZs: 3072, Bias: -12.780176, T: 15850000, Avg. loss: 271.587945\n",
      "Total training time: 86.88 seconds.\n",
      "-- Epoch 318\n",
      "Norm: 93.60, NNZs: 3072, Bias: -12.786096, T: 15900000, Avg. loss: 270.456038\n",
      "Total training time: 87.18 seconds.\n",
      "-- Epoch 319\n",
      "Norm: 93.45, NNZs: 3072, Bias: -12.791270, T: 15950000, Avg. loss: 267.233527\n",
      "Total training time: 87.49 seconds.\n",
      "-- Epoch 320\n",
      "Norm: 93.32, NNZs: 3072, Bias: -12.797589, T: 16000000, Avg. loss: 267.157511\n",
      "Total training time: 87.79 seconds.\n",
      "-- Epoch 321\n",
      "Norm: 93.19, NNZs: 3072, Bias: -12.803829, T: 16050000, Avg. loss: 264.865406\n",
      "Total training time: 88.09 seconds.\n",
      "-- Epoch 322\n",
      "Norm: 93.04, NNZs: 3072, Bias: -12.808312, T: 16100000, Avg. loss: 269.488977\n",
      "Total training time: 88.39 seconds.\n",
      "-- Epoch 323\n",
      "Norm: 92.91, NNZs: 3072, Bias: -12.812937, T: 16150000, Avg. loss: 262.244823\n",
      "Total training time: 88.69 seconds.\n",
      "-- Epoch 324\n",
      "Norm: 92.73, NNZs: 3072, Bias: -12.818128, T: 16200000, Avg. loss: 265.966860\n",
      "Total training time: 88.99 seconds.\n",
      "-- Epoch 325\n",
      "Norm: 92.59, NNZs: 3072, Bias: -12.823094, T: 16250000, Avg. loss: 265.888876\n",
      "Total training time: 89.29 seconds.\n",
      "-- Epoch 326\n",
      "Norm: 92.44, NNZs: 3072, Bias: -12.827569, T: 16300000, Avg. loss: 262.924391\n",
      "Total training time: 89.60 seconds.\n",
      "-- Epoch 327\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 92.26, NNZs: 3072, Bias: -12.833483, T: 16350000, Avg. loss: 260.900779\n",
      "Total training time: 89.89 seconds.\n",
      "-- Epoch 328\n",
      "Norm: 92.12, NNZs: 3072, Bias: -12.837681, T: 16400000, Avg. loss: 259.962958\n",
      "Total training time: 90.20 seconds.\n",
      "-- Epoch 329\n",
      "Norm: 91.96, NNZs: 3072, Bias: -12.840806, T: 16450000, Avg. loss: 259.729585\n",
      "Total training time: 90.49 seconds.\n",
      "-- Epoch 330\n",
      "Norm: 91.85, NNZs: 3072, Bias: -12.845972, T: 16500000, Avg. loss: 260.251374\n",
      "Total training time: 90.79 seconds.\n",
      "-- Epoch 331\n",
      "Norm: 91.73, NNZs: 3072, Bias: -12.851431, T: 16550000, Avg. loss: 258.171258\n",
      "Total training time: 91.10 seconds.\n",
      "-- Epoch 332\n",
      "Norm: 91.55, NNZs: 3072, Bias: -12.857623, T: 16600000, Avg. loss: 257.955710\n",
      "Total training time: 91.39 seconds.\n",
      "-- Epoch 333\n",
      "Norm: 91.42, NNZs: 3072, Bias: -12.863427, T: 16650000, Avg. loss: 257.535860\n",
      "Total training time: 91.69 seconds.\n",
      "-- Epoch 334\n",
      "Norm: 91.29, NNZs: 3072, Bias: -12.868267, T: 16700000, Avg. loss: 253.299587\n",
      "Total training time: 91.99 seconds.\n",
      "-- Epoch 335\n",
      "Norm: 91.14, NNZs: 3072, Bias: -12.874105, T: 16750000, Avg. loss: 255.864409\n",
      "Total training time: 92.30 seconds.\n",
      "-- Epoch 336\n",
      "Norm: 91.00, NNZs: 3072, Bias: -12.881012, T: 16800000, Avg. loss: 255.422718\n",
      "Total training time: 92.60 seconds.\n",
      "-- Epoch 337\n",
      "Norm: 90.85, NNZs: 3072, Bias: -12.886472, T: 16850000, Avg. loss: 256.575140\n",
      "Total training time: 92.90 seconds.\n",
      "-- Epoch 338\n",
      "Norm: 90.74, NNZs: 3072, Bias: -12.892979, T: 16900000, Avg. loss: 257.983058\n",
      "Total training time: 93.21 seconds.\n",
      "-- Epoch 339\n",
      "Norm: 90.60, NNZs: 3072, Bias: -12.898391, T: 16950000, Avg. loss: 252.925501\n",
      "Total training time: 93.51 seconds.\n",
      "-- Epoch 340\n",
      "Norm: 90.51, NNZs: 3072, Bias: -12.902063, T: 17000000, Avg. loss: 253.181386\n",
      "Total training time: 93.82 seconds.\n",
      "-- Epoch 341\n",
      "Norm: 90.42, NNZs: 3072, Bias: -12.906879, T: 17050000, Avg. loss: 252.644409\n",
      "Total training time: 94.14 seconds.\n",
      "-- Epoch 342\n",
      "Norm: 90.26, NNZs: 3072, Bias: -12.911007, T: 17100000, Avg. loss: 253.049222\n",
      "Total training time: 94.44 seconds.\n",
      "-- Epoch 343\n",
      "Norm: 90.14, NNZs: 3072, Bias: -12.915304, T: 17150000, Avg. loss: 248.683681\n",
      "Total training time: 94.74 seconds.\n",
      "-- Epoch 344\n",
      "Norm: 90.01, NNZs: 3072, Bias: -12.919633, T: 17200000, Avg. loss: 248.636228\n",
      "Total training time: 95.06 seconds.\n",
      "-- Epoch 345\n",
      "Norm: 89.88, NNZs: 3072, Bias: -12.924540, T: 17250000, Avg. loss: 248.660584\n",
      "Total training time: 95.36 seconds.\n",
      "-- Epoch 346\n",
      "Norm: 89.77, NNZs: 3072, Bias: -12.928480, T: 17300000, Avg. loss: 251.029266\n",
      "Total training time: 95.65 seconds.\n",
      "-- Epoch 347\n",
      "Norm: 89.63, NNZs: 3072, Bias: -12.931906, T: 17350000, Avg. loss: 246.486525\n",
      "Total training time: 95.96 seconds.\n",
      "-- Epoch 348\n",
      "Norm: 89.50, NNZs: 3072, Bias: -12.936094, T: 17400000, Avg. loss: 247.947560\n",
      "Total training time: 96.27 seconds.\n",
      "-- Epoch 349\n",
      "Norm: 89.37, NNZs: 3072, Bias: -12.940654, T: 17450000, Avg. loss: 247.872820\n",
      "Total training time: 96.58 seconds.\n",
      "-- Epoch 350\n",
      "Norm: 89.21, NNZs: 3072, Bias: -12.943932, T: 17500000, Avg. loss: 246.386178\n",
      "Total training time: 96.89 seconds.\n",
      "-- Epoch 351\n",
      "Norm: 89.12, NNZs: 3072, Bias: -12.947935, T: 17550000, Avg. loss: 244.101539\n",
      "Total training time: 97.19 seconds.\n",
      "-- Epoch 352\n",
      "Norm: 89.01, NNZs: 3072, Bias: -12.952746, T: 17600000, Avg. loss: 240.960466\n",
      "Total training time: 97.49 seconds.\n",
      "-- Epoch 353\n",
      "Norm: 88.88, NNZs: 3072, Bias: -12.956391, T: 17650000, Avg. loss: 243.242460\n",
      "Total training time: 97.79 seconds.\n",
      "-- Epoch 354\n",
      "Norm: 88.78, NNZs: 3072, Bias: -12.960663, T: 17700000, Avg. loss: 244.993464\n",
      "Total training time: 98.10 seconds.\n",
      "-- Epoch 355\n",
      "Norm: 88.64, NNZs: 3072, Bias: -12.964490, T: 17750000, Avg. loss: 239.239991\n",
      "Total training time: 98.40 seconds.\n",
      "-- Epoch 356\n",
      "Norm: 88.48, NNZs: 3072, Bias: -12.969360, T: 17800000, Avg. loss: 241.704682\n",
      "Total training time: 98.70 seconds.\n",
      "-- Epoch 357\n",
      "Norm: 88.36, NNZs: 3072, Bias: -12.973225, T: 17850000, Avg. loss: 239.129359\n",
      "Total training time: 99.00 seconds.\n",
      "-- Epoch 358\n",
      "Norm: 88.20, NNZs: 3072, Bias: -12.977205, T: 17900000, Avg. loss: 241.184479\n",
      "Total training time: 99.31 seconds.\n",
      "-- Epoch 359\n",
      "Norm: 88.06, NNZs: 3072, Bias: -12.983166, T: 17950000, Avg. loss: 237.842820\n",
      "Total training time: 99.61 seconds.\n",
      "-- Epoch 360\n",
      "Norm: 87.92, NNZs: 3072, Bias: -12.987904, T: 18000000, Avg. loss: 239.649603\n",
      "Total training time: 99.92 seconds.\n",
      "-- Epoch 361\n",
      "Norm: 87.80, NNZs: 3072, Bias: -12.993738, T: 18050000, Avg. loss: 236.590398\n",
      "Total training time: 100.23 seconds.\n",
      "-- Epoch 362\n",
      "Norm: 87.69, NNZs: 3072, Bias: -12.997803, T: 18100000, Avg. loss: 234.443400\n",
      "Total training time: 100.52 seconds.\n",
      "-- Epoch 363\n",
      "Norm: 87.55, NNZs: 3072, Bias: -13.001026, T: 18150000, Avg. loss: 239.564134\n",
      "Total training time: 100.83 seconds.\n",
      "-- Epoch 364\n",
      "Norm: 87.47, NNZs: 3072, Bias: -13.005277, T: 18200000, Avg. loss: 238.362414\n",
      "Total training time: 101.14 seconds.\n",
      "-- Epoch 365\n",
      "Norm: 87.40, NNZs: 3072, Bias: -13.010073, T: 18250000, Avg. loss: 231.700382\n",
      "Total training time: 101.44 seconds.\n",
      "-- Epoch 366\n",
      "Norm: 87.27, NNZs: 3072, Bias: -13.015395, T: 18300000, Avg. loss: 232.104493\n",
      "Total training time: 101.75 seconds.\n",
      "-- Epoch 367\n",
      "Norm: 87.15, NNZs: 3072, Bias: -13.019273, T: 18350000, Avg. loss: 234.677677\n",
      "Total training time: 102.05 seconds.\n",
      "-- Epoch 368\n",
      "Norm: 87.05, NNZs: 3072, Bias: -13.023717, T: 18400000, Avg. loss: 233.672034\n",
      "Total training time: 102.36 seconds.\n",
      "-- Epoch 369\n",
      "Norm: 86.93, NNZs: 3072, Bias: -13.026906, T: 18450000, Avg. loss: 233.527604\n",
      "Total training time: 102.67 seconds.\n",
      "-- Epoch 370\n",
      "Norm: 86.82, NNZs: 3072, Bias: -13.031270, T: 18500000, Avg. loss: 231.324484\n",
      "Total training time: 102.97 seconds.\n",
      "-- Epoch 371\n",
      "Norm: 86.70, NNZs: 3072, Bias: -13.035150, T: 18550000, Avg. loss: 231.165506\n",
      "Total training time: 103.28 seconds.\n",
      "-- Epoch 372\n",
      "Norm: 86.59, NNZs: 3072, Bias: -13.039834, T: 18600000, Avg. loss: 231.542313\n",
      "Total training time: 103.59 seconds.\n",
      "-- Epoch 373\n",
      "Norm: 86.46, NNZs: 3072, Bias: -13.045289, T: 18650000, Avg. loss: 226.455873\n",
      "Total training time: 103.91 seconds.\n",
      "-- Epoch 374\n",
      "Norm: 86.33, NNZs: 3072, Bias: -13.050098, T: 18700000, Avg. loss: 227.488619\n",
      "Total training time: 104.21 seconds.\n",
      "-- Epoch 375\n",
      "Norm: 86.22, NNZs: 3072, Bias: -13.053305, T: 18750000, Avg. loss: 229.016875\n",
      "Total training time: 104.53 seconds.\n",
      "-- Epoch 376\n",
      "Norm: 86.12, NNZs: 3072, Bias: -13.056906, T: 18800000, Avg. loss: 230.107979\n",
      "Total training time: 104.83 seconds.\n",
      "-- Epoch 377\n",
      "Norm: 86.00, NNZs: 3072, Bias: -13.061912, T: 18850000, Avg. loss: 227.832687\n",
      "Total training time: 105.14 seconds.\n",
      "-- Epoch 378\n",
      "Norm: 85.87, NNZs: 3072, Bias: -13.067167, T: 18900000, Avg. loss: 226.314832\n",
      "Total training time: 105.45 seconds.\n",
      "-- Epoch 379\n",
      "Norm: 85.72, NNZs: 3072, Bias: -13.071105, T: 18950000, Avg. loss: 226.434877\n",
      "Total training time: 105.75 seconds.\n",
      "-- Epoch 380\n",
      "Norm: 85.61, NNZs: 3072, Bias: -13.075462, T: 19000000, Avg. loss: 227.205316\n",
      "Total training time: 106.07 seconds.\n",
      "-- Epoch 381\n",
      "Norm: 85.46, NNZs: 3072, Bias: -13.081485, T: 19050000, Avg. loss: 226.636182\n",
      "Total training time: 106.39 seconds.\n",
      "-- Epoch 382\n",
      "Norm: 85.29, NNZs: 3072, Bias: -13.086718, T: 19100000, Avg. loss: 224.646138\n",
      "Total training time: 106.69 seconds.\n",
      "-- Epoch 383\n",
      "Norm: 85.20, NNZs: 3072, Bias: -13.091943, T: 19150000, Avg. loss: 225.006467\n",
      "Total training time: 107.00 seconds.\n",
      "-- Epoch 384\n",
      "Norm: 85.10, NNZs: 3072, Bias: -13.095085, T: 19200000, Avg. loss: 221.523829\n",
      "Total training time: 107.31 seconds.\n",
      "-- Epoch 385\n",
      "Norm: 85.00, NNZs: 3072, Bias: -13.099717, T: 19250000, Avg. loss: 221.583080\n",
      "Total training time: 107.62 seconds.\n",
      "-- Epoch 386\n",
      "Norm: 84.88, NNZs: 3072, Bias: -13.104647, T: 19300000, Avg. loss: 223.460957\n",
      "Total training time: 107.92 seconds.\n",
      "-- Epoch 387\n",
      "Norm: 84.74, NNZs: 3072, Bias: -13.109678, T: 19350000, Avg. loss: 222.006073\n",
      "Total training time: 108.23 seconds.\n",
      "-- Epoch 388\n",
      "Norm: 84.59, NNZs: 3072, Bias: -13.112770, T: 19400000, Avg. loss: 222.075147\n",
      "Total training time: 108.54 seconds.\n",
      "-- Epoch 389\n",
      "Norm: 84.47, NNZs: 3072, Bias: -13.115565, T: 19450000, Avg. loss: 220.404212\n",
      "Total training time: 108.85 seconds.\n",
      "-- Epoch 390\n",
      "Norm: 84.35, NNZs: 3072, Bias: -13.120492, T: 19500000, Avg. loss: 217.733646\n",
      "Total training time: 109.16 seconds.\n",
      "-- Epoch 391\n",
      "Norm: 84.27, NNZs: 3072, Bias: -13.125250, T: 19550000, Avg. loss: 219.354727\n",
      "Total training time: 109.47 seconds.\n",
      "-- Epoch 392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 84.15, NNZs: 3072, Bias: -13.129581, T: 19600000, Avg. loss: 218.999028\n",
      "Total training time: 109.78 seconds.\n",
      "-- Epoch 393\n",
      "Norm: 84.04, NNZs: 3072, Bias: -13.134511, T: 19650000, Avg. loss: 219.124086\n",
      "Total training time: 110.09 seconds.\n",
      "-- Epoch 394\n",
      "Norm: 83.93, NNZs: 3072, Bias: -13.137495, T: 19700000, Avg. loss: 217.725395\n",
      "Total training time: 110.39 seconds.\n",
      "-- Epoch 395\n",
      "Norm: 83.84, NNZs: 3072, Bias: -13.140737, T: 19750000, Avg. loss: 215.617067\n",
      "Total training time: 110.70 seconds.\n",
      "-- Epoch 396\n",
      "Norm: 83.75, NNZs: 3072, Bias: -13.145862, T: 19800000, Avg. loss: 216.086028\n",
      "Total training time: 111.00 seconds.\n",
      "-- Epoch 397\n",
      "Norm: 83.65, NNZs: 3072, Bias: -13.149546, T: 19850000, Avg. loss: 214.503950\n",
      "Total training time: 111.32 seconds.\n",
      "-- Epoch 398\n",
      "Norm: 83.50, NNZs: 3072, Bias: -13.154516, T: 19900000, Avg. loss: 216.388896\n",
      "Total training time: 111.63 seconds.\n",
      "-- Epoch 399\n",
      "Norm: 83.41, NNZs: 3072, Bias: -13.158061, T: 19950000, Avg. loss: 215.363614\n",
      "Total training time: 111.94 seconds.\n",
      "-- Epoch 400\n",
      "Norm: 83.32, NNZs: 3072, Bias: -13.161885, T: 20000000, Avg. loss: 213.019898\n",
      "Total training time: 112.26 seconds.\n",
      "-- Epoch 401\n",
      "Norm: 83.18, NNZs: 3072, Bias: -13.165751, T: 20050000, Avg. loss: 214.160786\n",
      "Total training time: 112.57 seconds.\n",
      "-- Epoch 402\n",
      "Norm: 83.06, NNZs: 3072, Bias: -13.170163, T: 20100000, Avg. loss: 213.402659\n",
      "Total training time: 112.88 seconds.\n",
      "-- Epoch 403\n",
      "Norm: 82.96, NNZs: 3072, Bias: -13.174857, T: 20150000, Avg. loss: 212.023347\n",
      "Total training time: 113.19 seconds.\n",
      "-- Epoch 404\n",
      "Norm: 82.85, NNZs: 3072, Bias: -13.177916, T: 20200000, Avg. loss: 214.463427\n",
      "Total training time: 113.52 seconds.\n",
      "-- Epoch 405\n",
      "Norm: 82.74, NNZs: 3072, Bias: -13.184159, T: 20250000, Avg. loss: 211.720224\n",
      "Total training time: 113.83 seconds.\n",
      "-- Epoch 406\n",
      "Norm: 82.65, NNZs: 3072, Bias: -13.188409, T: 20300000, Avg. loss: 209.669106\n",
      "Total training time: 114.13 seconds.\n",
      "-- Epoch 407\n",
      "Norm: 82.60, NNZs: 3072, Bias: -13.192245, T: 20350000, Avg. loss: 210.225928\n",
      "Total training time: 114.45 seconds.\n",
      "-- Epoch 408\n",
      "Norm: 82.50, NNZs: 3072, Bias: -13.196075, T: 20400000, Avg. loss: 208.126313\n",
      "Total training time: 114.76 seconds.\n",
      "-- Epoch 409\n",
      "Norm: 82.40, NNZs: 3072, Bias: -13.200680, T: 20450000, Avg. loss: 207.138499\n",
      "Total training time: 115.08 seconds.\n",
      "-- Epoch 410\n",
      "Norm: 82.29, NNZs: 3072, Bias: -13.203465, T: 20500000, Avg. loss: 207.734552\n",
      "Total training time: 115.40 seconds.\n",
      "-- Epoch 411\n",
      "Norm: 82.21, NNZs: 3072, Bias: -13.207888, T: 20550000, Avg. loss: 207.094277\n",
      "Total training time: 115.72 seconds.\n",
      "-- Epoch 412\n",
      "Norm: 82.10, NNZs: 3072, Bias: -13.211401, T: 20600000, Avg. loss: 207.531287\n",
      "Total training time: 116.04 seconds.\n",
      "-- Epoch 413\n",
      "Norm: 82.00, NNZs: 3072, Bias: -13.215969, T: 20650000, Avg. loss: 206.226881\n",
      "Total training time: 116.34 seconds.\n",
      "-- Epoch 414\n",
      "Norm: 81.89, NNZs: 3072, Bias: -13.220851, T: 20700000, Avg. loss: 206.476087\n",
      "Total training time: 116.66 seconds.\n",
      "-- Epoch 415\n",
      "Norm: 81.80, NNZs: 3072, Bias: -13.224339, T: 20750000, Avg. loss: 206.172409\n",
      "Total training time: 116.97 seconds.\n",
      "-- Epoch 416\n",
      "Norm: 81.69, NNZs: 3072, Bias: -13.227590, T: 20800000, Avg. loss: 205.842555\n",
      "Total training time: 117.29 seconds.\n",
      "-- Epoch 417\n",
      "Norm: 81.57, NNZs: 3072, Bias: -13.231284, T: 20850000, Avg. loss: 204.013547\n",
      "Total training time: 117.60 seconds.\n",
      "-- Epoch 418\n",
      "Norm: 81.47, NNZs: 3072, Bias: -13.234655, T: 20900000, Avg. loss: 205.954249\n",
      "Total training time: 117.91 seconds.\n",
      "-- Epoch 419\n",
      "Norm: 81.37, NNZs: 3072, Bias: -13.239325, T: 20950000, Avg. loss: 204.437748\n",
      "Total training time: 118.22 seconds.\n",
      "-- Epoch 420\n",
      "Norm: 81.23, NNZs: 3072, Bias: -13.243022, T: 21000000, Avg. loss: 202.923176\n",
      "Total training time: 118.54 seconds.\n",
      "-- Epoch 421\n",
      "Norm: 81.18, NNZs: 3072, Bias: -13.247190, T: 21050000, Avg. loss: 201.434418\n",
      "Total training time: 118.85 seconds.\n",
      "-- Epoch 422\n",
      "Norm: 81.09, NNZs: 3072, Bias: -13.250736, T: 21100000, Avg. loss: 202.649720\n",
      "Total training time: 119.16 seconds.\n",
      "-- Epoch 423\n",
      "Norm: 80.99, NNZs: 3072, Bias: -13.254161, T: 21150000, Avg. loss: 203.335750\n",
      "Total training time: 119.49 seconds.\n",
      "-- Epoch 424\n",
      "Norm: 80.89, NNZs: 3072, Bias: -13.257784, T: 21200000, Avg. loss: 204.148497\n",
      "Total training time: 119.80 seconds.\n",
      "-- Epoch 425\n",
      "Norm: 80.77, NNZs: 3072, Bias: -13.260827, T: 21250000, Avg. loss: 201.814553\n",
      "Total training time: 120.12 seconds.\n",
      "-- Epoch 426\n",
      "Norm: 80.68, NNZs: 3072, Bias: -13.264655, T: 21300000, Avg. loss: 199.193679\n",
      "Total training time: 120.44 seconds.\n",
      "-- Epoch 427\n",
      "Norm: 80.57, NNZs: 3072, Bias: -13.268174, T: 21350000, Avg. loss: 200.842916\n",
      "Total training time: 120.76 seconds.\n",
      "-- Epoch 428\n",
      "Norm: 80.50, NNZs: 3072, Bias: -13.272739, T: 21400000, Avg. loss: 201.301498\n",
      "Total training time: 121.08 seconds.\n",
      "-- Epoch 429\n",
      "Norm: 80.38, NNZs: 3072, Bias: -13.275674, T: 21450000, Avg. loss: 199.234352\n",
      "Total training time: 121.40 seconds.\n",
      "-- Epoch 430\n",
      "Norm: 80.28, NNZs: 3072, Bias: -13.279198, T: 21500000, Avg. loss: 198.511754\n",
      "Total training time: 121.72 seconds.\n",
      "-- Epoch 431\n",
      "Norm: 80.20, NNZs: 3072, Bias: -13.282560, T: 21550000, Avg. loss: 200.800326\n",
      "Total training time: 122.06 seconds.\n",
      "-- Epoch 432\n",
      "Norm: 80.12, NNZs: 3072, Bias: -13.286551, T: 21600000, Avg. loss: 198.173364\n",
      "Total training time: 122.40 seconds.\n",
      "-- Epoch 433\n",
      "Norm: 80.04, NNZs: 3072, Bias: -13.289747, T: 21650000, Avg. loss: 197.372839\n",
      "Total training time: 122.72 seconds.\n",
      "-- Epoch 434\n",
      "Norm: 79.92, NNZs: 3072, Bias: -13.292830, T: 21700000, Avg. loss: 198.408556\n",
      "Total training time: 123.03 seconds.\n",
      "-- Epoch 435\n",
      "Norm: 79.83, NNZs: 3072, Bias: -13.296159, T: 21750000, Avg. loss: 196.815296\n",
      "Total training time: 123.38 seconds.\n",
      "-- Epoch 436\n",
      "Norm: 79.74, NNZs: 3072, Bias: -13.299193, T: 21800000, Avg. loss: 195.761383\n",
      "Total training time: 123.71 seconds.\n",
      "-- Epoch 437\n",
      "Norm: 79.66, NNZs: 3072, Bias: -13.302730, T: 21850000, Avg. loss: 193.373185\n",
      "Total training time: 124.05 seconds.\n",
      "-- Epoch 438\n",
      "Norm: 79.53, NNZs: 3072, Bias: -13.306556, T: 21900000, Avg. loss: 195.916815\n",
      "Total training time: 124.37 seconds.\n",
      "-- Epoch 439\n",
      "Norm: 79.43, NNZs: 3072, Bias: -13.309797, T: 21950000, Avg. loss: 195.046774\n",
      "Total training time: 124.69 seconds.\n",
      "-- Epoch 440\n",
      "Norm: 79.30, NNZs: 3072, Bias: -13.313654, T: 22000000, Avg. loss: 194.830990\n",
      "Total training time: 125.00 seconds.\n",
      "-- Epoch 441\n",
      "Norm: 79.20, NNZs: 3072, Bias: -13.318090, T: 22050000, Avg. loss: 194.708586\n",
      "Total training time: 125.32 seconds.\n",
      "-- Epoch 442\n",
      "Norm: 79.15, NNZs: 3072, Bias: -13.321731, T: 22100000, Avg. loss: 191.253893\n",
      "Total training time: 125.64 seconds.\n",
      "-- Epoch 443\n",
      "Norm: 79.04, NNZs: 3072, Bias: -13.325087, T: 22150000, Avg. loss: 192.494867\n",
      "Total training time: 125.97 seconds.\n",
      "-- Epoch 444\n",
      "Norm: 78.94, NNZs: 3072, Bias: -13.329081, T: 22200000, Avg. loss: 193.320561\n",
      "Total training time: 126.29 seconds.\n",
      "-- Epoch 445\n",
      "Norm: 78.87, NNZs: 3072, Bias: -13.332174, T: 22250000, Avg. loss: 190.881985\n",
      "Total training time: 126.60 seconds.\n",
      "-- Epoch 446\n",
      "Norm: 78.77, NNZs: 3072, Bias: -13.336349, T: 22300000, Avg. loss: 193.246168\n",
      "Total training time: 126.92 seconds.\n",
      "-- Epoch 447\n",
      "Norm: 78.68, NNZs: 3072, Bias: -13.340646, T: 22350000, Avg. loss: 194.068486\n",
      "Total training time: 127.24 seconds.\n",
      "-- Epoch 448\n",
      "Norm: 78.57, NNZs: 3072, Bias: -13.343769, T: 22400000, Avg. loss: 191.037730\n",
      "Total training time: 127.56 seconds.\n",
      "-- Epoch 449\n",
      "Norm: 78.47, NNZs: 3072, Bias: -13.346744, T: 22450000, Avg. loss: 190.742911\n",
      "Total training time: 127.88 seconds.\n",
      "-- Epoch 450\n",
      "Norm: 78.38, NNZs: 3072, Bias: -13.350638, T: 22500000, Avg. loss: 189.301331\n",
      "Total training time: 128.20 seconds.\n",
      "-- Epoch 451\n",
      "Norm: 78.30, NNZs: 3072, Bias: -13.353397, T: 22550000, Avg. loss: 190.416798\n",
      "Total training time: 128.51 seconds.\n",
      "-- Epoch 452\n",
      "Norm: 78.22, NNZs: 3072, Bias: -13.355895, T: 22600000, Avg. loss: 188.237556\n",
      "Total training time: 128.84 seconds.\n",
      "-- Epoch 453\n",
      "Norm: 78.12, NNZs: 3072, Bias: -13.359504, T: 22650000, Avg. loss: 187.713228\n",
      "Total training time: 129.16 seconds.\n",
      "-- Epoch 454\n",
      "Norm: 78.02, NNZs: 3072, Bias: -13.362983, T: 22700000, Avg. loss: 189.930782\n",
      "Total training time: 129.49 seconds.\n",
      "-- Epoch 455\n",
      "Norm: 77.91, NNZs: 3072, Bias: -13.365865, T: 22750000, Avg. loss: 184.744526\n",
      "Total training time: 129.80 seconds.\n",
      "-- Epoch 456\n",
      "Norm: 77.82, NNZs: 3072, Bias: -13.369820, T: 22800000, Avg. loss: 187.796056\n",
      "Total training time: 130.12 seconds.\n",
      "-- Epoch 457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 77.73, NNZs: 3072, Bias: -13.372071, T: 22850000, Avg. loss: 188.141978\n",
      "Total training time: 130.44 seconds.\n",
      "-- Epoch 458\n",
      "Norm: 77.67, NNZs: 3072, Bias: -13.375894, T: 22900000, Avg. loss: 186.955947\n",
      "Total training time: 130.76 seconds.\n",
      "-- Epoch 459\n",
      "Norm: 77.58, NNZs: 3072, Bias: -13.379280, T: 22950000, Avg. loss: 185.209031\n",
      "Total training time: 131.07 seconds.\n",
      "-- Epoch 460\n",
      "Norm: 77.47, NNZs: 3072, Bias: -13.383218, T: 23000000, Avg. loss: 185.613481\n",
      "Total training time: 131.41 seconds.\n",
      "Convergence after 460 epochs took 131.41 seconds\n",
      "Training complete.  Time elapsed =  132.66126155853271\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "sgdlr.fit(X,catY)\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "print(\"Training complete.  Time elapsed = \",elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy:  0.7776\n"
     ]
    }
   ],
   "source": [
    "sgdlrscore = sgdlr.score(test_X,cat_test_Y)\n",
    "print(\"Logistic Regression Accuracy: \",sgdlrscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OLD- MAY DELETE -Model Comparison\n",
    "\n",
    "The two models were trained with similar hyperparameters to simplify the A/B comparison between the two models.  The only parameter that varied between the two models was the *loss* parameter. \n",
    "\n",
    "The loss function defaults to hinge, which gives a linear SVM. The log loss gives logistic regression, a probabilistic classifier. \n",
    "\n",
    "The two models we cerated in identifying cats peformed similarly in terms of accuracy, 85% to 89% accuaracy over several test runs.\n",
    "\n",
    "Training times varied depending on the underlying hardware and computer workload; however, the two models were comparable in overall training times on this dataset 69-72 seconds on Intel Core i7-6700 @ 3.4GHz, 24Gb of RAM, 4 Cores, and and NVIDIA 1060. \n",
    "\n",
    "Both models also seemed to mislabel similar images as evident in the plots above.  Both models seemed to struggle with cats in odd, off center poses, or curled up into a ball.  The one car image that was mislabaled as a cat kind of resemebles a cat with the openned hatchback of the car resembling a tail.\n",
    "\n",
    "Overall both models performed similarily in this classification task with the chosen model parameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
